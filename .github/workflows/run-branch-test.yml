name: Run Branch Tests with Regression Detection

on:
  workflow_call:
    inputs:
      target_branch:
        description: "Target branch to compare against (e.g., main)."
        required: true
        type: string
      python-version:
        description: "Python version for pytest."
        required: false
        type: string
        default: "3.10"
      runs_on:
        description: "Runner label."
        required: false
        type: string
        default: '["self-hosted", "multithreaded"]'
      parallel_workers:
        description: "Number of parallel pytest workers. Use 'auto' for CPU count."
        required: false
        type: string
        default: "auto"
    secrets:
      DISCORD_WEBHOOK_URL:
        required: false
      DISCORD_USER_MAP:
        required: false
    outputs:
      has_regressions:
        description: "Whether regressions were detected"
        value: ${{ jobs.compare.outputs.has_regressions }}
      regression_count:
        description: "Number of regressions"
        value: ${{ jobs.compare.outputs.regression_count }}

jobs:
  # Detect which test frameworks are present
  detect-frameworks:
    runs-on: ${{ fromJSON(inputs.runs_on) }}
    outputs:
      has_pytest: ${{ steps.detect.outputs.has_pytest }}
      # Future: has_jest, has_xunit, etc.
    steps:
      - uses: actions/checkout@v4.2.2
      - name: Detect test frameworks
        id: detect
        run: |
          # Detect pytest
          if [ -f "pyproject.toml" ] || [ -f "setup.py" ] || [ -f "requirements.txt" ] || find . -name "test_*.py" -o -name "*_test.py" | head -1 | grep -q .; then
            echo "has_pytest=true" >> $GITHUB_OUTPUT
            echo "âœ… Detected: pytest"
          else
            echo "has_pytest=false" >> $GITHUB_OUTPUT
          fi
          # Future: Add jest, xunit, etc. detection

  # Test source branch (always fresh, no caching)
  test-source:
    needs: detect-frameworks
    if: needs.detect-frameworks.outputs.has_pytest == 'true'
    uses: ./.github/workflows/test-py-pytest.yml
    with:
      ref: ""  # Default checkout = PR branch
      python-version: ${{ inputs.python-version }}
      runs_on: ${{ inputs.runs_on }}
      artifact_name: pytest_source_${{ github.event.pull_request.number || github.run_id }}
      parallel_workers: ${{ inputs.parallel_workers }}

  # Test target branch with smart caching
  # Cache is shared across PRs targeting same branch+SHA
  # First PR to run populates cache, subsequent PRs use it if available
  test-target:
    needs: detect-frameworks
    if: needs.detect-frameworks.outputs.has_pytest == 'true'
    runs-on: ${{ fromJSON(inputs.runs_on) }}
    outputs:
      total: ${{ steps.results.outputs.total }}
      passed: ${{ steps.results.outputs.passed }}
      percentage: ${{ steps.results.outputs.percentage }}
      collection_errors: ${{ steps.results.outputs.collection_errors }}
      no_tests_found: ${{ steps.results.outputs.no_tests_found }}
      has_errors: ${{ steps.results.outputs.has_errors }}
      error_type: ${{ steps.results.outputs.error_type }}
      failing_count: ${{ steps.results.outputs.failing_count }}
      error_count: ${{ steps.results.outputs.error_count }}
      skipped_count: ${{ steps.results.outputs.skipped_count }}
      xfailed_count: ${{ steps.results.outputs.xfailed_count }}
      xpassed_count: ${{ steps.results.outputs.xpassed_count }}

    steps:
      # Define cache keys
      - name: Set cache keys
        id: cache-keys
        run: |
          # Version bump forces cache invalidation when extraction logic changes
          CACHE_VERSION="v4"
          BASE_KEY="pytest-${CACHE_VERSION}-${{ inputs.target_branch }}-${{ github.event.pull_request.base.sha || github.sha }}"
          echo "base_key=$BASE_KEY" >> $GITHUB_OUTPUT
          echo "pending_key=${BASE_KEY}-pending-${{ github.run_id }}" >> $GITHUB_OUTPUT
          echo "ðŸ” Cache base key: $BASE_KEY"

      # Try to restore complete results first
      - name: Check for complete cache
        id: cache-complete
        uses: actions/cache/restore@v4
        with:
          path: cached_target
          key: ${{ steps.cache-keys.outputs.base_key }}

      # If no complete cache, check for any pending cache (someone else is running)
      - name: Check for pending cache
        id: cache-pending
        if: steps.cache-complete.outputs.cache-hit != 'true'
        uses: actions/cache/restore@v4
        with:
          path: cached_pending
          key: ${{ steps.cache-keys.outputs.base_key }}-pending-impossible-match
          restore-keys: |
            ${{ steps.cache-keys.outputs.base_key }}-pending-

      - name: Determine initial status
        id: initial-status
        run: |
          if [ "${{ steps.cache-complete.outputs.cache-hit }}" == "true" ]; then
            echo "status=complete" >> $GITHUB_OUTPUT
            echo "âœ… Found complete cache - will use it"
          elif [ "${{ steps.cache-pending.outputs.cache-hit }}" == "true" ]; then
            echo "status=pending" >> $GITHUB_OUTPUT
            echo "â³ Found pending cache - another job is running, will poll"
          else
            echo "status=miss" >> $GITHUB_OUTPUT
            echo "ðŸ“­ No cache found - will run tests"
          fi

      # If cache miss, immediately save a pending marker so others know to wait
      - name: Create pending marker
        if: steps.initial-status.outputs.status == 'miss'
        run: |
          mkdir -p cached_pending_marker
          echo "pending" > cached_pending_marker/status
          echo "started=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> cached_pending_marker/status
          echo "run_id=${{ github.run_id }}" >> cached_pending_marker/status

      - name: Save pending marker
        if: steps.initial-status.outputs.status == 'miss'
        uses: actions/cache/save@v4
        with:
          path: cached_pending_marker
          key: ${{ steps.cache-keys.outputs.pending_key }}

      # If pending found, poll for complete cache with exponential backoff
      - name: Poll for complete cache
        id: poll-cache
        if: steps.initial-status.outputs.status == 'pending'
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          echo "â³ Another job is running tests, polling for results..."
          TOTAL_WAIT=0
          MAX_WAIT=1200  # 20 minutes
          DELAY=5
          CACHE_KEY="${{ steps.cache-keys.outputs.base_key }}"

          while [ $TOTAL_WAIT -lt $MAX_WAIT ]; do
            echo "â³ Waiting ${DELAY}s... (${TOTAL_WAIT}s / ${MAX_WAIT}s elapsed)"
            sleep $DELAY
            TOTAL_WAIT=$((TOTAL_WAIT + DELAY))

            # Check if complete cache exists now using GitHub API
            CACHE_CHECK=$(gh cache list --key "$CACHE_KEY" --limit 1 2>/dev/null || echo "")
            if echo "$CACHE_CHECK" | grep -q "$CACHE_KEY"; then
              echo "âœ… Complete cache is now available!"
              echo "found=true" >> $GITHUB_OUTPUT
              break
            fi

            # Exponential backoff: 5, 10, 20, 40, 60, 60...
            DELAY=$((DELAY * 2))
            if [ $DELAY -gt 60 ]; then
              DELAY=60
            fi
          done

          if [ $TOTAL_WAIT -ge $MAX_WAIT ]; then
            echo "â° Timeout after ${MAX_WAIT}s - will run tests ourselves"
            echo "found=false" >> $GITHUB_OUTPUT
          fi

      # Restore complete cache after polling found it
      - name: Restore cache after poll
        id: cache-after-poll
        if: steps.poll-cache.outputs.found == 'true'
        uses: actions/cache/restore@v4
        with:
          path: cached_target
          key: ${{ steps.cache-keys.outputs.base_key }}

      - name: Determine final status
        id: final-status
        run: |
          if [ "${{ steps.cache-complete.outputs.cache-hit }}" == "true" ]; then
            echo "cache_hit=true" >> $GITHUB_OUTPUT
            echo "âœ… Using complete cache (found immediately)"
          elif [ "${{ steps.cache-after-poll.outputs.cache-hit }}" == "true" ]; then
            echo "cache_hit=true" >> $GITHUB_OUTPUT
            echo "âœ… Using complete cache (found after polling)"
          else
            echo "cache_hit=false" >> $GITHUB_OUTPUT
            echo "ðŸ§ª Will run tests"
          fi

      - name: Load cached results
        id: load-cache
        if: steps.final-status.outputs.cache_hit == 'true'
        run: |
          echo "âœ… Loading cached target results (skipping test run)"
          if [ -f cached_target/outputs.env ]; then
            cat cached_target/outputs.env >> $GITHUB_OUTPUT
          fi

      - name: Upload cached artifact
        if: steps.final-status.outputs.cache_hit == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: pytest_target_${{ github.event.pull_request.number || github.run_id }}
          path: cached_target/test_data.json
          if-no-files-found: ignore

      # === Only run tests if no usable cache ===
      - name: Checkout
        if: steps.final-status.outputs.cache_hit != 'true'
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"
          ref: ${{ inputs.target_branch }}

      - name: Set up Python
        if: steps.final-status.outputs.cache_hit != 'true'
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Install dependencies
        if: steps.final-status.outputs.cache_hit != 'true'
        run: |
          echo "ðŸ“¦ Installing dependencies..."
          python -m pip install --upgrade pip
          pip install pytest pytest-json-report pytest-asyncio pytest-xdist
          PYPROJECT=$(find . -name "pyproject.toml" -type f | head -n 1)
          if [ -n "$PYPROJECT" ]; then
            pip install -e "$(dirname "$PYPROJECT")[dev]"
          elif [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

      - name: Check for test collection errors
        id: check-collection
        if: steps.final-status.outputs.cache_hit != 'true'
        run: |
          echo "ðŸ” Running pytest collection check..."
          python -m pytest --collect-only -v > collection_output.txt 2>&1 || true

          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"

          if grep -q "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt; then
            echo "::error::Test discovery errors detected"
            if grep -q "ImportError" collection_output.txt; then
              ERROR_TYPE="ImportError"
            elif grep -q "ModuleNotFoundError" collection_output.txt; then
              ERROR_TYPE="ModuleNotFoundError"
            elif grep -q "SyntaxError" collection_output.txt; then
              ERROR_TYPE="SyntaxError"
            elif grep -q "ERROR collecting" collection_output.txt; then
              ERROR_TYPE="CollectionError"
            elif grep -q "Interrupted:" collection_output.txt; then
              ERROR_TYPE="Interrupted"
            else
              ERROR_TYPE="UnknownError"
            fi
            HAS_COLLECTION_ERRORS="true"
          else
            TEST_COUNT=$(grep -o "collected [0-9]* item" collection_output.txt | grep -o "[0-9]*" || echo "0")
            if [[ "$TEST_COUNT" == "0" ]]; then
              echo "::warning::No tests were found"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
            else
              echo "âœ… Found $TEST_COUNT tests"
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          if [[ "$HAS_COLLECTION_ERRORS" == "true" || "$NO_TESTS_FOUND" == "true" ]]; then
            echo "has_errors=true" >> $GITHUB_OUTPUT
          else
            echo "has_errors=false" >> $GITHUB_OUTPUT
          fi

      - name: Run tests
        id: run-tests
        continue-on-error: true
        if: |
          steps.final-status.outputs.cache_hit != 'true' &&
          steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "ðŸ§ª Running tests with ${{ inputs.parallel_workers }} workers..."
          PARALLEL_FLAG=""
          if [ "${{ inputs.parallel_workers }}" != "1" ]; then
            PARALLEL_FLAG="-n ${{ inputs.parallel_workers }}"
          fi

          # Run pytest and capture exit code
          # Use quiet mode for console (-q) - full details in JSON report
          set +e
          python -m pytest -q $PARALLEL_FLAG --json-report --json-report-file=results.json --tb=line 2>&1 | tee test_output.txt
          PYTEST_EXIT=$?
          set -e

          echo "pytest_exit_code=$PYTEST_EXIT" >> $GITHUB_OUTPUT

          if [ $PYTEST_EXIT -eq 137 ]; then
            echo "::warning::Tests were killed (exit 137) - likely OOM. Partial results may be available."
          fi

          if [ -f results.json ]; then
            echo "âœ… Test execution completed (exit code: $PYTEST_EXIT)"
          else
            echo "âŒ No results.json - creating empty results file"
            echo '{"exitcode": '$PYTEST_EXIT', "summary": {"total": 0, "passed": 0}, "tests": []}' > results.json
          fi

      - name: Extract test results
        id: extract-results
        if: steps.final-status.outputs.cache_hit != 'true'
        run: |
          python3 -c "
          import json
          import os

          total = passed = 0
          percentage = 0.0
          passing_tests = []
          failing_tests = []
          error_tests = []
          skipped_tests = []
          xfailed_tests = []
          xpassed_tests = []
          all_tests = []
          skipped_with_reasons = {}
          xfailed_with_reasons = {}
          warnings_list = []

          try:
              with open('results.json') as f:
                  results = json.load(f)

              # Extract results regardless of exit code - we want to capture
              # whatever tests ran, even if pytest had errors
              if 'summary' in results:
                  summary = results['summary']
                  total = summary.get('total', 0)
                  passed = summary.get('passed', 0)

              for test in results.get('tests', []):
                  outcome = test.get('outcome')
                  nodeid = test.get('nodeid', '')
                  if not nodeid:
                      continue
                  all_tests.append(nodeid)
                  if outcome == 'passed':
                      passing_tests.append(nodeid)
                  elif outcome == 'failed':
                      failing_tests.append(nodeid)
                  elif outcome == 'error':
                      error_tests.append(nodeid)
                  elif outcome == 'skipped':
                      skipped_tests.append(nodeid)
                      reason = test.get('longrepr', 'No reason')
                      if isinstance(reason, list):
                          reason = reason[0] if reason else 'No reason'
                      skipped_with_reasons[nodeid] = str(reason).strip()
                  elif outcome == 'xfailed':
                      xfailed_tests.append(nodeid)
                      reason = test.get('longrepr', 'No reason')
                      if isinstance(reason, list):
                          reason = reason[0] if reason else 'No reason'
                      xfailed_with_reasons[nodeid] = str(reason).strip()
                  elif outcome == 'xpassed':
                      xpassed_tests.append(nodeid)

              percentage = (passed / total * 100) if total > 0 else 0
          except FileNotFoundError:
              print('Results file not found')
          except Exception as e:
              print(f'Error: {e}')

          # Extract warnings
          try:
              with open('test_output.txt') as f:
                  content = f.read()
              if 'warnings summary' in content:
                  section = content.split('warnings summary')[1].split('-- Docs:')[0] if '-- Docs:' in content else content.split('warnings summary')[1]
                  current = []
                  for line in section.split('\n'):
                      line = line.rstrip()
                      if not line or line.startswith('='):
                          continue
                      if not line.startswith(' ') and ('.py:' in line or 'warning' in line.lower()):
                          if current:
                              warnings_list.append('\n'.join(current))
                          current = [line]
                      elif line.startswith(' ') and current:
                          current.append(line)
                  if current:
                      warnings_list.append('\n'.join(current))
          except:
              pass

          # Save artifact data
          with open('test_data.json', 'w') as f:
              json.dump({
                  'passing_tests': passing_tests,
                  'failing_tests': failing_tests,
                  'error_tests': error_tests,
                  'skipped_tests': skipped_tests,
                  'xfailed_tests': xfailed_tests,
                  'xpassed_tests': xpassed_tests,
                  'all_tests': all_tests,
                  'skipped_tests_with_reasons': skipped_with_reasons,
                  'xfailed_tests_with_reasons': xfailed_with_reasons,
                  'warnings': warnings_list
              }, f, indent=2)

          print(f'Results: {passed}/{total} ({percentage:.1f}%)')

          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={total}\n')
              f.write(f'passed={passed}\n')
              f.write(f'percentage={percentage:.2f}\n')
              f.write(f'failing_count={len(failing_tests)}\n')
              f.write(f'error_count={len(error_tests)}\n')
              f.write(f'skipped_count={len(skipped_tests)}\n')
              f.write(f'xfailed_count={len(xfailed_tests)}\n')
              f.write(f'xpassed_count={len(xpassed_tests)}\n')
          "

      - name: Save results to cache
        if: steps.final-status.outputs.cache_hit != 'true'
        run: |
          echo "ðŸ’¾ Saving results to cache..."
          mkdir -p cached_target

          # Copy test artifacts to cache directory
          [ -f test_data.json ] && cp test_data.json cached_target/
          [ -f test_output.txt ] && cp test_output.txt cached_target/
          [ -f results.json ] && cp results.json cached_target/

          # Mark cache as complete (not pending)
          echo "complete" > cached_target/status
          echo "completed=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> cached_target/status
          echo "run_id=${{ github.run_id }}" >> cached_target/status

          # Save outputs for future cache loads (no leading spaces!)
          cat > cached_target/outputs.env << 'EOF'
          total=${{ steps.extract-results.outputs.total || '0' }}
          passed=${{ steps.extract-results.outputs.passed || '0' }}
          percentage=${{ steps.extract-results.outputs.percentage || '0.00' }}
          collection_errors=${{ steps.check-collection.outputs.has_collection_errors || 'false' }}
          no_tests_found=${{ steps.check-collection.outputs.no_tests_found || 'false' }}
          has_errors=${{ steps.check-collection.outputs.has_errors || 'false' }}
          error_type=${{ steps.check-collection.outputs.error_type || 'none' }}
          failing_count=${{ steps.extract-results.outputs.failing_count || '0' }}
          error_count=${{ steps.extract-results.outputs.error_count || '0' }}
          skipped_count=${{ steps.extract-results.outputs.skipped_count || '0' }}
          xfailed_count=${{ steps.extract-results.outputs.xfailed_count || '0' }}
          xpassed_count=${{ steps.extract-results.outputs.xpassed_count || '0' }}
          EOF
          # Remove leading whitespace from the env file
          sed -i 's/^[[:space:]]*//' cached_target/outputs.env

      # Save complete results so other PRs can find it
      - name: Upload to cache
        if: steps.final-status.outputs.cache_hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: cached_target
          key: ${{ steps.cache-keys.outputs.base_key }}

      - name: Upload test artifacts
        if: steps.final-status.outputs.cache_hit != 'true'
        uses: actions/upload-artifact@v4
        with:
          name: pytest_target_${{ github.event.pull_request.number || github.run_id }}
          path: |
            test_data.json
            test_output.txt
            results.json
          retention-days: 3
          if-no-files-found: ignore

      # Consolidate outputs from cache or fresh run
      - name: Set final outputs
        id: results
        run: |
          if [ "${{ steps.final-status.outputs.cache_hit }}" == "true" ]; then
            echo "ðŸ“‹ Using cached results"
            # Outputs already set by load-cache step, copy them
            echo "total=${{ steps.load-cache.outputs.total || '0' }}" >> $GITHUB_OUTPUT
            echo "passed=${{ steps.load-cache.outputs.passed || '0' }}" >> $GITHUB_OUTPUT
            echo "percentage=${{ steps.load-cache.outputs.percentage || '0.00' }}" >> $GITHUB_OUTPUT
            echo "collection_errors=${{ steps.load-cache.outputs.collection_errors || 'false' }}" >> $GITHUB_OUTPUT
            echo "no_tests_found=${{ steps.load-cache.outputs.no_tests_found || 'false' }}" >> $GITHUB_OUTPUT
            echo "has_errors=${{ steps.load-cache.outputs.has_errors || 'false' }}" >> $GITHUB_OUTPUT
            echo "error_type=${{ steps.load-cache.outputs.error_type || 'none' }}" >> $GITHUB_OUTPUT
            echo "failing_count=${{ steps.load-cache.outputs.failing_count || '0' }}" >> $GITHUB_OUTPUT
            echo "error_count=${{ steps.load-cache.outputs.error_count || '0' }}" >> $GITHUB_OUTPUT
            echo "skipped_count=${{ steps.load-cache.outputs.skipped_count || '0' }}" >> $GITHUB_OUTPUT
            echo "xfailed_count=${{ steps.load-cache.outputs.xfailed_count || '0' }}" >> $GITHUB_OUTPUT
            echo "xpassed_count=${{ steps.load-cache.outputs.xpassed_count || '0' }}" >> $GITHUB_OUTPUT
          else
            echo "ðŸ“‹ Using fresh results"
            echo "total=${{ steps.extract-results.outputs.total || '0' }}" >> $GITHUB_OUTPUT
            echo "passed=${{ steps.extract-results.outputs.passed || '0' }}" >> $GITHUB_OUTPUT
            echo "percentage=${{ steps.extract-results.outputs.percentage || '0.00' }}" >> $GITHUB_OUTPUT
            echo "collection_errors=${{ steps.check-collection.outputs.has_collection_errors || 'false' }}" >> $GITHUB_OUTPUT
            echo "no_tests_found=${{ steps.check-collection.outputs.no_tests_found || 'false' }}" >> $GITHUB_OUTPUT
            echo "has_errors=${{ steps.check-collection.outputs.has_errors || 'false' }}" >> $GITHUB_OUTPUT
            echo "error_type=${{ steps.check-collection.outputs.error_type || 'none' }}" >> $GITHUB_OUTPUT
            echo "failing_count=${{ steps.extract-results.outputs.failing_count || '0' }}" >> $GITHUB_OUTPUT
            echo "error_count=${{ steps.extract-results.outputs.error_count || '0' }}" >> $GITHUB_OUTPUT
            echo "skipped_count=${{ steps.extract-results.outputs.skipped_count || '0' }}" >> $GITHUB_OUTPUT
            echo "xfailed_count=${{ steps.extract-results.outputs.xfailed_count || '0' }}" >> $GITHUB_OUTPUT
            echo "xpassed_count=${{ steps.extract-results.outputs.xpassed_count || '0' }}" >> $GITHUB_OUTPUT
          fi

  # Compare results
  compare:
    needs: [test-source, test-target]
    if: always() && needs.test-source.result == 'success'
    uses: ./.github/workflows/regression-test.yml
    with:
      runs_on: ${{ inputs.runs_on }}
      baseline_label: ${{ inputs.target_branch }}
      baseline_results_artifact: pytest_target_${{ github.event.pull_request.number || github.run_id }}
      baseline_results_filename: test_data.json
      current_label: ${{ github.head_ref || github.ref_name }}
      current_results_artifact: pytest_source_${{ github.event.pull_request.number || github.run_id }}
      current_results_filename: test_data.json
      baseline_passed: ${{ needs.test-target.outputs.passed }}
      baseline_total: ${{ needs.test-target.outputs.total }}
      baseline_percentage: ${{ needs.test-target.outputs.percentage }}
      current_passed: ${{ needs.test-source.outputs.passed }}
      current_total: ${{ needs.test-source.outputs.total }}
      current_percentage: ${{ needs.test-source.outputs.percentage }}
      baseline_collection_errors: ${{ needs.test-target.outputs.collection_errors }}
      baseline_no_tests_found: ${{ needs.test-target.outputs.no_tests_found }}
      current_collection_errors: ${{ needs.test-source.outputs.collection_errors }}
      current_no_tests_found: ${{ needs.test-source.outputs.no_tests_found }}
      artifact_name: regression_pytest_${{ github.event.pull_request.number || github.run_id }}

  # Notify on regressions
  notify:
    needs: [test-source, test-target, compare]
    if: |
      always() &&
      (needs.compare.outputs.has_regressions == 'true' || needs.compare.result == 'failure')
    runs-on: ${{ fromJSON(inputs.runs_on) }}
    steps:
      - name: Send notification
        env:
          WEBHOOK: ${{ secrets.DISCORD_WEBHOOK_URL }}
        run: |
          if [ -z "$WEBHOOK" ]; then
            echo "No Discord webhook configured, skipping notification"
            exit 0
          fi

          MSG="**Pytest Regression Alert**\n"
          MSG+="PR #${{ github.event.pull_request.number }}: ${{ github.event.pull_request.title }}\n"
          MSG+="\`${{ github.head_ref }}\` â†’ \`${{ inputs.target_branch }}\`\n\n"
          MSG+="Source: ${{ needs.test-source.outputs.passed }}/${{ needs.test-source.outputs.total }}\n"
          MSG+="Target: ${{ needs.test-target.outputs.passed }}/${{ needs.test-target.outputs.total }}\n"
          MSG+="Regressions: ${{ needs.compare.outputs.regression_count || '?' }}\n\n"
          MSG+="[View Run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})"

          curl -s -H "Content-Type: application/json" \
            -d "{\"content\": \"$(echo -e "$MSG")\"}" \
            "$WEBHOOK" || true
