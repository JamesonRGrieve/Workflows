name: Reusable Compare Pytest Results

on:
  workflow_call:
    inputs:
      target_branch_to_compare:
        description: "The target branch to compare against (e.g., main, refs/heads/main)."
        required: true
        type: string
      python-version:
        description: "Python version to use for testing."
        required: false
        type: string
        default: "3.10"
      ping_latest_committer:
        description: "If true, the latest committer on the PR will be added to the ping list."
        required: false
        type: boolean
        default: false
      runs_on:
        required: false
        type: string
        default: "ubuntu-latest"
    secrets:
      DISCORD_WEBHOOK_URL:
        description: "Discord Webhook URL for failure notifications. If not provided, notifications are skipped."
        required: false
      DISCORD_USER_MAP:
        description: 'JSON string mapping GitHub usernames to Discord User IDs (e.g., {"user1":"id1"}). If not provided, users won''t be pinged.'
        required: false
    outputs:
      pr_total:
        description: "Total tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.total }}
      pr_passed:
        description: "Passed tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.passed }}
      pr_percentage:
        description: "Pass percentage in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.percentage }}
      pr_collection_errors:
        description: "PR branch has collection errors"
        value: ${{ jobs.test-source-branch.outputs.collection_errors }}
      pr_no_tests_found:
        description: "PR branch has no tests found"
        value: ${{ jobs.test-source-branch.outputs.no_tests_found }}
      target_total:
        description: "Total tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.total }}
      target_passed:
        description: "Passed tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.passed }}
      target_percentage:
        description: "Pass percentage in target branch"
        value: ${{ jobs.test-target-branch.outputs.percentage }}
      has_regressions:
        description: "Boolean indicating if regressions were found"
        value: ${{ jobs.compare-results.outputs.has_regressions }}
      regression_count:
        description: "Number of test regressions found"
        value: ${{ jobs.compare-results.outputs.regression_count }}

jobs:
  test-source-branch:
    runs-on: ${{ inputs.runs_on }}
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}
      failing_count: ${{ steps.extract-results.outputs.failing_count }}
      skipped_count: ${{ steps.extract-results.outputs.skipped_count }}
      xfailed_count: ${{ steps.extract-results.outputs.xfailed_count }}

    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip --quiet
          pip install pytest pytest-json-report pytest-asyncio --quiet
          if [ -f requirements.txt ]; then 
            pip install -r requirements.txt --quiet
          fi

      - name: Silent pytest execution and collection check
        id: check-collection
        run: |
          # Use Python for completely silent execution
          python3 << 'EOF'
          import subprocess
          import sys
          import os

          def run_silent_command(cmd, capture_output=False):
              try:
                  if capture_output:
                      return subprocess.run(cmd, capture_output=True, text=True, timeout=60)
                  else:
                      with open(os.devnull, 'w') as devnull:
                          return subprocess.run(cmd, stdout=devnull, stderr=devnull, timeout=1800)
              except:
                  return subprocess.CompletedProcess(cmd, 1)

          # Step 1: Collection check
          collection_cmd = [sys.executable, '-m', 'pytest', '--collect-only', '--quiet', '--tb=no']
          collection_result = run_silent_command(collection_cmd, capture_output=True)
          
          has_collection_errors = "false"
          no_tests_found = "false"
          error_type = "none"
          error_details = "none"
          
          if collection_result.returncode != 0:
              if any(err in collection_result.stderr for err in ['ImportError', 'ModuleNotFoundError', 'SyntaxError', 'ERROR collecting']):
                  has_collection_errors = "true"
                  if 'ImportError' in collection_result.stderr:
                      error_type = "ImportError"
                  elif 'ModuleNotFoundError' in collection_result.stderr:
                      error_type = "ModuleNotFoundError"
                  elif 'SyntaxError' in collection_result.stderr:
                      error_type = "SyntaxError"
                  else:
                      error_type = "CollectionError"
                  error_details = collection_result.stderr[:200] if collection_result.stderr else "Collection failed"
          else:
              if 'collected 0 items' in collection_result.stdout or not collection_result.stdout.strip():
                  no_tests_found = "true"
                  error_type = "NoTestsFound"
                  error_details = "No test files discovered"

          # Write outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'has_collection_errors={has_collection_errors}\n')
              f.write(f'no_tests_found={no_tests_found}\n')
              f.write(f'error_type={error_type}\n')
              f.write(f'error_details={error_details}\n')
              f.write(f'has_errors={str(has_collection_errors == "true" or no_tests_found == "true").lower()}\n')

          # Step 2: Run tests if collection succeeded
          if has_collection_errors == "false":
              test_cmd = [
                  sys.executable, '-m', 'pytest',
                  '--json-report',
                  '--json-report-file=pr_results.json',
                  '--tb=no',
                  '--no-header',
                  '--no-summary',
                  '--quiet',
                  '--disable-warnings',
                  '--log-level=CRITICAL',
                  '--capture=sys',
                  '--maxfail=999999'
              ]
              
              test_result = run_silent_command(test_cmd)
              
              if os.path.exists('pr_results.json'):
                  print("Test results generated successfully")
              else:
                  print("::error::Failed to create test results file")
          else:
              print(f"::error::Skipping tests due to collection errors: {error_type}")
          EOF

      - name: Extract test results and create artifacts
        id: extract-results
        run: |
          python -c "
          import json
          import sys
          import os

          pr_total = 0
          pr_passed = 0
          pr_percentage = 0
          failing_tests = []
          skipped_tests = []
          xfailed_tests = []
          all_tests = []
          skipped_tests_with_reasons = {}
          xfailed_tests_with_reasons = {}

          try:
              with open('pr_results.json') as f:
                  pr_results = json.load(f)
              
              if pr_results.get('exitcode', 0) > 1:
                  pr_total = 0
                  pr_passed = 0
              elif 'summary' in pr_results and isinstance(pr_results['summary'], dict):
                  summary = pr_results['summary']
                  pr_total = summary.get('total', 0)
                  pr_passed = summary.get('passed', 0)
                  
                  if 'tests' in pr_results:
                      for test in pr_results['tests']:
                          outcome = test.get('outcome')
                          nodeid = test.get('nodeid', '')
                          if nodeid:
                              all_tests.append(nodeid)
                              if outcome in ['failed', 'error']:
                                  failing_tests.append(nodeid)
                              elif outcome == 'skipped':
                                  skipped_tests.append(nodeid)
                                  skip_reason = 'No reason provided'
                                  if 'longrepr' in test and test['longrepr']:
                                      longrepr = test['longrepr']
                                      if isinstance(longrepr, list) and longrepr:
                                          skip_reason = str(longrepr[0]) if longrepr[0] else 'No reason provided'
                                      elif isinstance(longrepr, str):
                                          skip_reason = longrepr
                                  skipped_tests_with_reasons[nodeid] = skip_reason.strip()
                              elif outcome == 'xfailed':
                                  xfailed_tests.append(nodeid)
                                  xfail_reason = 'No reason provided'
                                  if 'longrepr' in test and test['longrepr']:
                                      longrepr = test['longrepr']
                                      if isinstance(longrepr, list) and longrepr:
                                          xfail_reason = str(longrepr[0]) if longrepr[0] else 'No reason provided'
                                      elif isinstance(longrepr, str):
                                          xfail_reason = longrepr
                                  xfailed_tests_with_reasons[nodeid] = xfail_reason.strip()
              
              pr_percentage = (pr_passed / pr_total * 100) if pr_total > 0 else 0
              
          except Exception as e:
              pass

          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={pr_total}\\n')
              f.write(f'passed={pr_passed}\\n')
              f.write(f'percentage={pr_percentage:.2f}\\n')
              f.write(f'failing_count={len(failing_tests)}\\n')
              f.write(f'skipped_count={len(skipped_tests)}\\n')
              f.write(f'xfailed_count={len(xfailed_tests)}\\n')

          test_data = {
              'failing_tests': failing_tests,
              'skipped_tests': skipped_tests,
              'xfailed_tests': xfailed_tests,
              'all_tests': all_tests,
              'skipped_tests_with_reasons': skipped_tests_with_reasons,
              'xfailed_tests_with_reasons': xfailed_tests_with_reasons
          }

          with open('pr_test_data.json', 'w') as f:
              json.dump(test_data, f, indent=2)
          "

      - name: Upload PR branch artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pr_branch_data_${{ github.event.pull_request.number || github.run_id }}
          path: |
            pr_test_data.json
            pr_results.json
            collection_check.txt
          retention-days: 3
          if-no-files-found: ignore

  test-target-branch:
    runs-on: ${{ inputs.runs_on }}
    outputs:
      total: ${{ steps.check-collection.outputs.has_collection_errors == 'true' && steps.set-error-outputs.outputs.total || steps.extract-results.outputs.total }}
      passed: ${{ steps.check-collection.outputs.has_collection_errors == 'true' && steps.set-error-outputs.outputs.passed || steps.extract-results.outputs.passed }}
      percentage: ${{ steps.check-collection.outputs.has_collection_errors == 'true' && steps.set-error-outputs.outputs.percentage || steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}
      passing_count: ${{ steps.check-collection.outputs.has_collection_errors == 'true' && steps.set-error-outputs.outputs.passing_count || steps.extract-results.outputs.passing_count }}

    steps:
      - name: Checkout target branch
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"
          ref: ${{ inputs.target_branch_to_compare }}

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip --quiet
          pip install pytest pytest-json-report pytest-asyncio --quiet
          if [ -f requirements.txt ]; then 
            pip install -r requirements.txt --quiet
          fi

      - name: Check for test collection errors
        id: check-collection
        run: |
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          python -m pytest --collect-only --quiet --tb=no >/dev/null 2>collection_check.txt
          COLLECTION_EXIT_CODE=$?

          if [ $COLLECTION_EXIT_CODE -ne 0 ]; then
            if grep -q "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting" collection_check.txt; then
              echo "::warning::Test discovery errors detected in target branch"
              HAS_COLLECTION_ERRORS="true"
              
              if grep -q "ImportError" collection_check.txt; then
                ERROR_TYPE="ImportError"
              elif grep -q "ModuleNotFoundError" collection_check.txt; then
                ERROR_TYPE="ModuleNotFoundError"
              elif grep -q "SyntaxError" collection_check.txt; then
                ERROR_TYPE="SyntaxError"
              else
                ERROR_TYPE="CollectionError"
              fi
              
              ERROR_DETAILS=$(head -1 collection_check.txt | tr '\n' ' ' | cut -c1-200)
            fi
          else
            TEST_COUNT=$(python -m pytest --collect-only --quiet 2>/dev/null | grep -o "collected [0-9]* item" | grep -o "[0-9]*" || echo "0")
            if [[ "$TEST_COUNT" == "0" ]]; then
              echo "::warning::No tests were found in the target branch"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              ERROR_DETAILS="No test files discovered"
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT
          
          if [[ "$HAS_COLLECTION_ERRORS" == "true" || "$NO_TESTS_FOUND" == "true" ]]; then
            echo "has_errors=true" >> $GITHUB_OUTPUT
          else
            echo "has_errors=false" >> $GITHUB_OUTPUT
          fi

      - name: Run tests on target branch
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        env:
          PYTHONUNBUFFERED: "0"
          PYTHONWARNINGS: "ignore"
          PYTEST_DISABLE_PLUGIN_AUTOLOAD: "1"
          PYTEST_CURRENT_TEST: ""
          COLUMNS: "80"
        run: |
          # Create completely isolated execution to prevent any output leakage
          (
            # Close all file descriptors and redirect to /dev/null
            exec 0</dev/null
            exec 1>/dev/null
            exec 2>/dev/null
            
            # Run pytest in completely silent mode
            python -m pytest \
              --json-report \
              --json-report-file=target_results.json \
              --tb=no \
              --no-header \
              --no-summary \
              --quiet \
              --disable-warnings \
              --log-level=CRITICAL \
              --log-cli-level=CRITICAL \
              --capture=sys \
              --maxfail=999999 \
              --tb=no
          ) &
          
          # Wait for background process to complete
          wait
          
          # Check results outside the silent execution
          if [ -f target_results.json ]; then
            echo "Test results generated successfully"
          else
            echo "::warning::Failed to create test results file"
          fi

      - name: Extract test results and create artifacts
        id: extract-results
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          python -c "
          import json
          import sys
          import os

          target_total = 0
          target_passed = 0
          target_percentage = 0
          passing_tests = []
          all_tests = []

          try:
              with open('target_results.json') as f:
                  target_results = json.load(f)
              
              if target_results.get('exitcode', 0) > 1:
                  target_total = 0
                  target_passed = 0
              elif 'summary' in target_results and isinstance(target_results['summary'], dict):
                  summary = target_results['summary']
                  target_total = summary.get('total', 0)
                  target_passed = summary.get('passed', 0)
                  
                  if 'tests' in target_results:
                      for test in target_results['tests']:
                          outcome = test.get('outcome')
                          nodeid = test.get('nodeid', '')
                          if nodeid:
                              all_tests.append(nodeid)
                              if outcome == 'passed':
                                  passing_tests.append(nodeid)
              
              target_percentage = (target_passed / target_total * 100) if target_total > 0 else 0
              
          except Exception as e:
              pass

          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={target_total}\\n')
              f.write(f'passed={target_passed}\\n')
              f.write(f'percentage={target_percentage:.2f}\\n')
              f.write(f'passing_count={len(passing_tests)}\\n')

          test_data = {
              'passing_tests': passing_tests,
              'all_tests': all_tests
          }

          with open('target_test_data.json', 'w') as f:
              json.dump(test_data, f, indent=2)
          "

      - name: Upload target branch artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: target_branch_data_${{ github.event.pull_request.number || github.run_id }}
          path: |
            target_test_data.json
            target_results.json
            collection_check.txt
          retention-days: 3
          if-no-files-found: ignore

      - name: Set collection error outputs
        id: set-error-outputs
        if: steps.check-collection.outputs.has_collection_errors == 'true'
        run: |
          echo "total=0" >> $GITHUB_OUTPUT
          echo "passed=0" >> $GITHUB_OUTPUT
          echo "percentage=0.00" >> $GITHUB_OUTPUT
          echo "passing_count=0" >> $GITHUB_OUTPUT

  compare-results:
    needs: [test-source-branch, test-target-branch]
    runs-on: ${{ inputs.runs_on }}
    outputs:
      has_regressions: ${{ needs.perform-regression-analysis.outputs.has_regressions }}
      regression_count: ${{ needs.perform-regression-analysis.outputs.regression_count }}

    steps:
      - name: Install bc
        run: |
          sudo apt-get update -y
          sudo apt-get install -y bc

      - name: Download test data artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: "*_branch_data_${{ github.event.pull_request.number || github.run_id }}"
          path: ./artifacts
          merge-multiple: false

      - name: Check for collection errors
        run: |
          PR_COLLECTION_ERRORS="${{ needs.test-source-branch.outputs.collection_errors }}"
          PR_NO_TESTS="${{ needs.test-source-branch.outputs.no_tests_found }}"
          PR_ERROR_TYPE="${{ needs.test-source-branch.outputs.error_type }}"
          TARGET_COLLECTION_ERRORS="${{ needs.test-target-branch.outputs.collection_errors }}"

          if [[ "$PR_COLLECTION_ERRORS" == "true" ]]; then
            echo "::error::Test discovery errors in PR branch: $PR_ERROR_TYPE"
            exit 1
          fi

          if [[ "$PR_NO_TESTS" == "true" ]]; then
            echo "::error::No tests were found in the PR branch"
            exit 1
          fi

          if [[ "$TARGET_COLLECTION_ERRORS" == "true" ]]; then
            echo "⚠️ Target branch has test discovery errors."
          fi

      - name: Run regression analysis from artifacts
        run: |
          python3 - << 'EOF'
          import json
          import os
          import glob

          try:
              target_data = {}
              pr_data = {}
              
              target_files = glob.glob('./artifacts/target_branch_data_*/target_test_data.json')
              if target_files:
                  with open(target_files[0], 'r') as f:
                      target_data = json.load(f)
              
              pr_files = glob.glob('./artifacts/pr_branch_data_*/pr_test_data.json')
              if pr_files:
                  with open(pr_files[0], 'r') as f:
                      pr_data = json.load(f)
              
              target_passing = target_data.get('passing_tests', [])
              pr_failing = pr_data.get('failing_tests', [])
              
              target_passing_set = set(target_passing)
              pr_failing_set = set(pr_failing)
              regression_tests = list(target_passing_set.intersection(pr_failing_set))
              
              if regression_tests:
                  with open("regression_details.txt", "w") as f:
                      f.write(f"Found {len(regression_tests)} tests that were passing in target branch but now failing in PR branch:\\n\\n")
                      for idx, test in enumerate(sorted(regression_tests), 1):
                          f.write(f"{idx}. {test}\\n")
          except Exception as e:
              pass
          EOF

      - name: Check for regression details file
        id: check-regressions
        run: |
          _has_regressions="false"
          _regression_count="0"

          if [ -f "regression_details.txt" ]; then
            _current_count=$(grep -c "^[0-9]\+\." regression_details.txt || echo "0")
            if [ "$_current_count" -gt 0 ]; then
              _has_regressions="true"
              _regression_count="$_current_count"
              echo "::error::Test Regressions Found: $_regression_count test(s) that were passing in target branch are now failing in PR branch."
            fi
          fi

          echo "HAS_REGRESSIONS=$_has_regressions" >> $GITHUB_OUTPUT
          echo "REGRESSION_COUNT=$_regression_count" >> $GITHUB_OUTPUT

      - name: Upload regression details artifact
        if: steps.check-regressions.outputs.HAS_REGRESSIONS == 'true' && steps.check-regressions.outputs.REGRESSION_COUNT > 0
        uses: actions/upload-artifact@v4
        with:
          name: regression_details_pr_${{ github.event.pull_request.number || github.run_id }}_tests
          path: regression_details.txt
          retention-days: 1

      - name: Compare test results
        run: |
          echo "Test Results Summary:"
          echo "Target branch (${{ inputs.target_branch_to_compare }}): ${{ needs.test-target-branch.outputs.passed }}/${{ needs.test-target-branch.outputs.total }} tests passed (${{ needs.test-target-branch.outputs.percentage }}%)"
          echo "PR branch: ${{ needs.test-source-branch.outputs.passed }}/${{ needs.test-source-branch.outputs.total }} tests passed (${{ needs.test-source-branch.outputs.percentage }}%)"

          if [[ "${{ needs.test-source-branch.outputs.total }}" == "0" ]]; then
            echo "::error::No tests were found in the PR branch"
            exit 1
          fi

          PR_PASSED=${{ needs.test-source-branch.outputs.passed }}
          TARGET_PASSED=${{ needs.test-target-branch.outputs.passed }}
          PR_PERCENTAGE=${{ needs.test-source-branch.outputs.percentage }}
          TARGET_PERCENTAGE=${{ needs.test-target-branch.outputs.percentage }}
          PR_TOTAL=${{ needs.test-source-branch.outputs.total }}
          TARGET_TOTAL=${{ needs.test-target-branch.outputs.total }}

          if [[ "$TARGET_TOTAL" == "0" ]]; then
            if [[ "$PR_PASSED" -gt 0 ]]; then
              echo "✅ PR branch has tests and some are passing (target branch has no tests)"
              exit 0
            else
              echo "❌ PR branch has no passing tests"
              exit 1
            fi
          fi

          if [[ "${{ needs.perform-regression-analysis.outputs.has_regressions }}" == "true" ]]; then
            echo "❌ PR branch has test regressions from target branch"
            exit 1
          fi

          if (( $(echo "$PR_PASSED >= $TARGET_PASSED" | bc -l) )) && (( $(echo "$PR_PERCENTAGE >= $TARGET_PERCENTAGE" | bc -l) )); then
            echo "✅ PR branch has equal or better test results than target branch"
            exit 0
          else
            echo "❌ PR branch has worse test results than target branch"
            exit 1
          fi

  perform-regression-analysis:
    needs: [test-source-branch, test-target-branch]
    uses: ./.github/workflows/meta-regression-analysis.yml
    with:
      item_type_singular: "test"
      item_type_plural: "tests"
      pr_number: ${{ github.event.pull_request.number }}
      run_id: ${{ github.run_id }}
      target_branch_artifact_name: target_branch_data_${{ github.event.pull_request.number || github.run_id }}
      pr_branch_artifact_name: pr_branch_data_${{ github.event.pull_request.number || github.run_id }}

  prepare-notification:
    name: Prepare Notification Data
    needs:
      [
        lint,
        test-source-branch,
        test-target-branch,
        compare-results,
        perform-regression-analysis,
      ]
    if: |
      always() &&
      (
        needs.test-source-branch.outputs.collection_errors == 'true' ||
        needs.test-source-branch.outputs.no_tests_found == 'true' ||
        needs.compare-results.result == 'failure' ||
        needs.perform-regression-analysis.outputs.has_regressions == 'true'
      )
    runs-on: ${{ inputs.runs_on }}
    outputs:
      message_body: ${{ steps.construct_notification.outputs.message_body_out }}
      ping_user_ids: ${{ steps.construct_notification.outputs.ping_user_ids_out }}
      artifact_path: ${{ steps.construct_notification.outputs.artifact_path_out }}
      should_notify: "true"
      webhook_available_for_alert: ${{ steps.check_webhook_availability.outputs.webhook_available }}

    steps:
      - name: Check for Discord Webhook URL
        id: check_webhook_availability
        run: |
          if [ -z "${{ secrets.DISCORD_WEBHOOK_URL }}" ]; then
            echo "webhook_available=false" >> $GITHUB_OUTPUT
          else
            echo "webhook_available=true" >> $GITHUB_OUTPUT
          fi

      - name: Download regression details (if any)
        id: download_regressions
        if: needs.perform-regression-analysis.outputs.has_regressions == 'true' && needs.perform-regression-analysis.outputs.regression_count > 0
        uses: actions/download-artifact@v4
        with:
          name: regression_details_pr_${{ github.event.pull_request.number || github.run_id }}_tests
          path: .
        continue-on-error: true

      - name: Construct Discord Notification
        id: construct_notification
        env:
          LINT_RESULT: ${{ needs.lint.result }}
          SOURCE_TEST_RESULT: ${{ needs.test-source-branch.result }}
          TARGET_TEST_RESULT: ${{ needs.test-target-branch.result }}
          COMPARE_RESULT: ${{ needs.compare-results.result }}
          PR_COLLECTION_ERRORS: ${{ needs.test-source-branch.outputs.collection_errors }}
          PR_NO_TESTS_FOUND: ${{ needs.test-source-branch.outputs.no_tests_found }}
          PR_ERROR_TYPE: ${{ needs.test-source-branch.outputs.error_type }}
          PR_ERROR_DETAILS_TRUNCATED: ${{ needs.test-source-branch.outputs.error_details }}
          HAS_REGRESSIONS: ${{ needs.perform-regression-analysis.outputs.has_regressions }}
          REGRESSION_COUNT: ${{ needs.perform-regression-analysis.outputs.regression_count }}
          PR_TOTAL_TESTS: ${{ needs.test-source-branch.outputs.total }}
          PR_PASSED_TESTS: ${{ needs.test-source-branch.outputs.passed }}
          PR_PERCENTAGE: ${{ needs.test-source-branch.outputs.percentage }}
          TARGET_TOTAL_TESTS: ${{ needs.test-target-branch.outputs.total }}
          TARGET_PASSED_TESTS: ${{ needs.test-target-branch.outputs.passed }}
          TARGET_PERCENTAGE: ${{ needs.test-target-branch.outputs.percentage }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          PR_TITLE: ${{ github.event.pull_request.title }}
          PR_URL: ${{ github.event.pull_request.html_url }}
          TARGET_BRANCH_NAME: ${{ inputs.target_branch_to_compare }}
          PR_BRANCH_NAME: ${{ github.head_ref }}
          REPO_URL: ${{ github.server_url }}/${{ github.repository }}
          ACTION_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          GH_ASSIGNEES_JSON: ${{ toJson(github.event.pull_request.assignees) }}
          USER_MAP_JSON: ${{ secrets.DISCORD_USER_MAP }}
          REGRESSION_FILE_PATH: "regression_details.txt"
          DOWNLOAD_REGRESSIONS_OUTCOME: ${{ steps.download_regressions.outcome }}
          INPUT_PING_LATEST_COMMITTER: ${{ inputs.ping_latest_committer }}
        run: |
          MESSAGE_LINES=()
          PING_KEYS_OUTPUT=""
          ARTIFACT_PATH_OUTPUT=""

          PING_KEYS_OUTPUT=""

          if [ -n "$USER_MAP_JSON" ] && [ "$USER_MAP_JSON" != "{}" ] && command -v jq &> /dev/null; then
            ASSIGNEE_LOGINS_ARRAY=($(echo "$GH_ASSIGNEES_JSON" | jq -r '.[].login // empty'))
            MAPPED_ASSIGNEE_COUNT=0
            TEMP_PING_KEYS=()

            for assignee_login in "${ASSIGNEE_LOGINS_ARRAY[@]}"; do
              if [ -z "$assignee_login" ]; then
                continue
              fi
              if echo "$USER_MAP_JSON" | jq -e --arg K "$assignee_login" '.[$K]' > /dev/null; then
                TEMP_PING_KEYS+=("$assignee_login")
                MAPPED_ASSIGNEE_COUNT=$((MAPPED_ASSIGNEE_COUNT + 1))
              fi
            done

            if [ ${#TEMP_PING_KEYS[@]} -gt 0 ]; then
              PING_KEYS_OUTPUT=$(IFS=,; echo "${TEMP_PING_KEYS[*]}")
            fi
          fi

          if [[ "$INPUT_PING_LATEST_COMMITTER" == "true" ]]; then
            if command -v gh &> /dev/null && [ -n "$PR_NUMBER" ]; then
              LATEST_COMMITTER_LOGIN_RAW=$(gh pr view "$PR_NUMBER" --json commits --jq '.commits[-1].author.login' 2>/dev/null || echo "")
              
              if [ -n "$LATEST_COMMITTER_LOGIN_RAW" ] && [ "$LATEST_COMMITTER_LOGIN_RAW" != "null" ]; then
                LATEST_COMMITTER_LOGIN=$(echo "$LATEST_COMMITTER_LOGIN_RAW" | grep -v -E -i '(\[bot\]$|-bot$)' || echo "")
                
                if [ -n "$LATEST_COMMITTER_LOGIN" ]; then
                  ALREADY_IN_LIST=0
                  if [ -n "$PING_KEYS_OUTPUT" ]; then
                    IFS=',' read -ra PING_ARRAY <<< "$PING_KEYS_OUTPUT"
                    for key in "${PING_ARRAY[@]}"; do
                      if [[ "$key" == "$LATEST_COMMITTER_LOGIN" ]]; then
                        ALREADY_IN_LIST=1
                        break
                      fi
                    done
                  fi
                  
                  if [[ "$ALREADY_IN_LIST" -eq 0 ]]; then
                    if [ -z "$PING_KEYS_OUTPUT" ]; then
                      PING_KEYS_OUTPUT="$LATEST_COMMITTER_LOGIN"
                    else
                      PING_KEYS_OUTPUT="$PING_KEYS_OUTPUT,$LATEST_COMMITTER_LOGIN"
                    fi
                  fi
                fi
              fi
            fi
          fi

          echo "ping_user_ids_out=$PING_KEYS_OUTPUT" >> $GITHUB_OUTPUT

          PR_BRANCH="${PR_BRANCH_NAME:-unknown}"
          TARGET_BRANCH="${TARGET_BRANCH_NAME:-unknown}"

          MESSAGE_LINES+=("**Pytest Comparison & Regression Analysis for PR [#${PR_NUMBER}: ${PR_TITLE}](${PR_URL})**")
          MESSAGE_LINES+=("Branch: [\`${PR_BRANCH}\`](${REPO_URL}/tree/${PR_BRANCH}) against [\`${TARGET_BRANCH}\`](${REPO_URL}/tree/${TARGET_BRANCH})")
          MESSAGE_LINES+=("---")

          MESSAGE_LINES+=("**Job Status:**")
          LINT_STATUS="Success"
          if [[ "$LINT_RESULT" == "failure" ]]; then LINT_STATUS="Failed"; elif [[ "$LINT_RESULT" == "skipped" ]]; then LINT_STATUS="Skipped"; fi
          MESSAGE_LINES+=("- Linting: $LINT_STATUS")

          SOURCE_TEST_STATUS="Success"
          if [[ "$SOURCE_TEST_RESULT" == "failure" ]]; then SOURCE_TEST_STATUS="Failed"; elif [[ "$SOURCE_TEST_RESULT" == "skipped" ]]; then SOURCE_TEST_STATUS="Skipped"; fi
          MESSAGE_LINES+=("- PR Branch Tests (\`${PR_BRANCH}\`): $SOURCE_TEST_STATUS")

          TARGET_TEST_STATUS="Success"
          if [[ "$TARGET_TEST_RESULT" == "failure" ]]; then TARGET_TEST_STATUS="Failed"; elif [[ "$TARGET_TEST_RESULT" == "skipped" ]]; then TARGET_TEST_STATUS="Skipped"; fi
          MESSAGE_LINES+=("- Target Branch Tests (\`${TARGET_BRANCH}\`): $TARGET_TEST_STATUS")

          COMPARE_STATUS="Success"
          if [[ "$COMPARE_RESULT" == "failure" ]]; then COMPARE_STATUS="Failed"; elif [[ "$COMPARE_RESULT" == "skipped" ]]; then COMPARE_STATUS="Skipped"; fi
          MESSAGE_LINES+=("- Comparison & Regression: $COMPARE_STATUS")
          MESSAGE_LINES+=("---")

          if [[ "$PR_COLLECTION_ERRORS" == "true" ]]; then
            MESSAGE_LINES+=("**:red_circle: ERROR: Test Discovery Failed in PR Branch (\`${PR_BRANCH}\`)**")
            MESSAGE_LINES+=("  - Type: \`${PR_ERROR_TYPE}\`")
            MESSAGE_LINES+=("  - Details: \`\`\`${PR_ERROR_DETAILS_TRUNCATED}\`\`\`")
          elif [[ "$PR_NO_TESTS_FOUND" == "true" ]]; then
            MESSAGE_LINES+=("**:warning: WARNING: No Tests Found in PR Branch (\`${PR_BRANCH}\`)**")
          fi

          if [[ "$HAS_REGRESSIONS" == "true" ]]; then
            MESSAGE_LINES+=("**:red_circle: REGRESSIONS DETECTED**")
            MESSAGE_LINES+=("  - **${REGRESSION_COUNT} test(s)** that were passing in \`${TARGET_BRANCH}\` are now **failing** in \`${PR_BRANCH}\`.")
            
            CURRENT_MESSAGE=$(printf "%s\\n" "${MESSAGE_LINES[@]}")
            CURRENT_LENGTH=${#CURRENT_MESSAGE}
            
            if [ -f "$REGRESSION_FILE_PATH" ] && [[ "$DOWNLOAD_REGRESSIONS_OUTCOME" == "success" ]]; then
              REGRESSION_LIST=$(awk '/^[0-9]+\./ {sub(/^[0-9]+\. /, "- "); print}' "$REGRESSION_FILE_PATH")
              
              TEMP_MESSAGE="$CURRENT_MESSAGE"
              TEMP_MESSAGE+="\`\`\`"
              TEMP_MESSAGE+="$REGRESSION_LIST"
              TEMP_MESSAGE+="\`\`\`"
              TEMP_LENGTH=${#TEMP_MESSAGE}
              
              if [ $TEMP_LENGTH -le 2000 ]; then
                MESSAGE_LINES+=("  - **Failed Tests (Regressions):**")
                MESSAGE_LINES+=("\`\`\`")
                MESSAGE_LINES+=("$REGRESSION_LIST")
                MESSAGE_LINES+=("\`\`\`")
                ARTIFACT_PATH_OUTPUT=""
              else
                MESSAGE_LINES+=("  - Details for the ${REGRESSION_COUNT} regressions are in the attached \`regression_details.txt\` file.")
                ARTIFACT_PATH_OUTPUT="$REGRESSION_FILE_PATH"
              fi
            else
              ARTIFACT_PATH_OUTPUT=""
            fi
          elif [[ "$COMPARE_RESULT" == "failure" ]] && [[ "$HAS_REGRESSIONS" != "true" ]]; then
            MESSAGE_LINES+=("**:warning: TEST RESULTS DECLINED**")
            MESSAGE_LINES+=("  - PR Branch (\`${PR_BRANCH}\`): **${PR_PASSED_TESTS}/${PR_TOTAL_TESTS} passed (${PR_PERCENTAGE}%)**")
            MESSAGE_LINES+=("  - Target Branch (\`${TARGET_BRANCH}\`): **${TARGET_PASSED_TESTS}/${TARGET_TOTAL_TESTS} passed (${TARGET_PERCENTAGE}%)**")
          elif [[ "$COMPARE_RESULT" == "success" ]] && [[ "$HAS_REGRESSIONS" != "true" ]]; then
             MESSAGE_LINES+=("**:white_check_mark: NO REGRESSIONS DETECTED**")
             MESSAGE_LINES+=("  - PR Branch (\`${PR_BRANCH}\`): **${PR_PASSED_TESTS}/${PR_TOTAL_TESTS} passed (${PR_PERCENTAGE}%)**")
             MESSAGE_LINES+=("  - Target Branch (\`${TARGET_BRANCH}\`): **${TARGET_PASSED_TESTS}/${TARGET_TOTAL_TESTS} passed (${TARGET_PERCENTAGE}%)**")
          fi

          MESSAGE_LINES+=("---")
          MESSAGE_LINES+=("[View Workflow Run](${ACTION_RUN_URL})")

          FINAL_MESSAGE_BODY=$(printf "%s\\n" "${MESSAGE_LINES[@]}")
          if [ ${#MESSAGE_LINES[@]} -gt 0 ]; then
            FINAL_MESSAGE_BODY="${FINAL_MESSAGE_BODY%\\n}"
          fi

          echo "message_body_out<<EOF" >> $GITHUB_OUTPUT
          echo "$FINAL_MESSAGE_BODY" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          echo "artifact_path_out=$ARTIFACT_PATH_OUTPUT" >> $GITHUB_OUTPUT

  notify-discord:
    name: Send Discord Notification
    needs: [prepare-notification]
    if: |
      always() &&
      needs.prepare-notification.outputs.should_notify == 'true' &&
      needs.prepare-notification.outputs.webhook_available_for_alert == 'true'
    uses: ./.github/workflows/alert-discord.yml
    with:
      message_body: ${{ needs.prepare-notification.outputs.message_body }}
      ping_user_ids: ${{ needs.prepare-notification.outputs.ping_user_ids }}
      artifact_paths: ${{ needs.prepare-notification.outputs.artifact_path }}
      should_notify: ${{ needs.prepare-notification.outputs.should_notify }}
      runs_on: ${{ inputs.runs_on }}
    secrets:
      DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      DISCORD_USER_MAP: ${{ secrets.DISCORD_USER_MAP }}
