name: Reusable Compare Pytest Results

on:
  workflow_call:
    inputs:
      target_branch_to_compare:
        description: "The target branch to compare against (e.g., main, refs/heads/main)."
        required: true
        type: string
      python-version:
        description: "Python version to use for testing."
        required: false
        type: string
        default: "3.10"
    secrets:
      DISCORD_WEBHOOK_URL:
        description: "Discord Webhook URL for failure notifications. If not provided, notifications are skipped."
        required: false
      GH_TO_DISCORD_USER_MAP:
        description: "JSON string mapping GitHub usernames to Discord User IDs (e.g., {\\\"user1\\\":\\\"id1\\\"}). If not provided, users won't be pinged."
        required: false
    outputs:
      pr_total:
        description: "Total tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.total }}
      pr_passed:
        description: "Passed tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.passed }}
      pr_percentage:
        description: "Pass percentage in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.percentage }}
      pr_failing_tests:
        description: "List of failing tests in PR/source branch (JSON string)"
        value: ${{ jobs.test-source-branch.outputs.failing_tests }}
      pr_collection_errors:
        description: "PR branch has collection errors"
        value: ${{ jobs.test-source-branch.outputs.collection_errors }}
      pr_no_tests_found:
        description: "PR branch has no tests found"
        value: ${{ jobs.test-source-branch.outputs.no_tests_found }}
      target_total:
        description: "Total tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.total }}
      target_passed:
        description: "Passed tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.passed }}
      target_percentage:
        description: "Pass percentage in target branch"
        value: ${{ jobs.test-target-branch.outputs.percentage }}
      target_passing_tests:
        description: "List of passing tests in target branch (JSON string)"
        value: ${{ jobs.test-target-branch.outputs.passing_tests }}
      has_regressions:
        description: "Boolean indicating if regressions were found"
        value: ${{ jobs.compare-results.outputs.has_regressions }}
      regression_count:
        description: "Number of test regressions found"
        value: ${{ jobs.compare-results.outputs.regression_count }}

jobs:
  lint:
    uses: ./.github/workflows/test-lint-py.yml
    permissions:
      contents: write # Lint job might push changes

  test-source-branch:
    needs: lint
    runs-on: ubuntu-latest
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}
      failing_tests: ${{ steps.extract-results.outputs.failing_tests }}

    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Install dependencies
        run: |
          set -x
          echo "::debug::Upgrading pip with verbose output"
          python -m pip install --upgrade pip -v
          echo "::debug::Installing pytest packages with verbose output"
          pip install -v pytest pytest-json-report pytest-asyncio
          echo "::debug::Checking for requirements.txt file"
          if [ -f requirements.txt ]; then 
            echo "::debug::Installing dependencies from requirements.txt with verbose output"
            pip install -v -r requirements.txt
          else 
            echo "::debug::No requirements.txt file found"
          fi

      - name: Check for test collection errors
        id: check-collection
        run: |
          set -x
          echo "::debug::Current working directory: $(pwd)"
          echo "::debug::Listing python files:"
          find . -name "*.py" | sort
          echo "::debug::Python version: $(python --version)"
          echo "::debug::Installed packages:"
          pip list

          echo "::debug::Running pytest in collection mode"
          python -m pytest --collect-only -v > collection_output.txt 2>&1 || true

          echo "::debug::Collection completed, checking output file"
          echo "::debug::Collection output file size: $(wc -l collection_output.txt)"
          echo "::debug::First 50 lines of collection output:"
          head -n 50 collection_output.txt

          # Set default values
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          echo "::debug::Checking for collection errors"
          if grep -q "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt; then
            echo "::debug::Collection errors detected, extracting details"
            echo "::error::Test discovery errors detected - Python modules could not be imported correctly"
            
            # Attempt to identify specific error type
            if grep -q "ImportError" collection_output.txt; then
              ERROR_TYPE="ImportError"
            elif grep -q "ModuleNotFoundError" collection_output.txt; then
              ERROR_TYPE="ModuleNotFoundError"
            elif grep -q "SyntaxError" collection_output.txt; then
              ERROR_TYPE="SyntaxError"
            elif grep -q "ERROR collecting" collection_output.txt; then
              ERROR_TYPE="CollectionError"
            elif grep -q "Interrupted:" collection_output.txt; then
              ERROR_TYPE="Interrupted"
            else 
              ERROR_TYPE="UnknownError"
            fi
            
            echo "::debug::Error type identified: $ERROR_TYPE"
            echo "Discovery error type: $ERROR_TYPE"
            
            echo "::debug::Attempting to find specific error file"
            ERROR_FILE=$(grep -o "ERROR collecting.*\.py" collection_output.txt | grep -o "[a-zA-Z0-9_/]*\.py" || echo "Unknown file")
            echo "::debug::Error file identification result: $ERROR_FILE"
            
            if [[ "$ERROR_FILE" != "Unknown file" ]]; then
              echo "Error in file $ERROR_FILE"
              echo "::debug::Extracting detailed error information for $ERROR_FILE"
              grep -A 15 "$ERROR_FILE" collection_output.txt > error_details.txt
              ERROR_DETAILS=$(cat error_details.txt | tr '\n' ' ' | sed 's/"/\\"/g')
            fi
            
            echo "::error::Discovery error details: ${ERROR_DETAILS:0:200}..."
            echo "::debug::Setting has_collection_errors=true in GitHub output"
            HAS_COLLECTION_ERRORS="true"
          else
            echo "::debug::No collection errors detected, checking test count"
            echo "No discovery errors detected in PR branch"
            
            echo "::debug::Extracting test count from collection output"
            TEST_COUNT=$(grep -o "collected [0-9]* item" collection_output.txt | grep -o "[0-9]*" || echo "0")
            echo "::debug::Test count extraction result: $TEST_COUNT"
            
            if [[ "$TEST_COUNT" == "0" ]]; then
              echo "::warning::No tests were found in the PR branch"
              echo "::debug::Setting no_tests_found=true"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              ERROR_DETAILS="No test files were discovered that match pytest's test discovery pattern"
            else  
              echo "Found $TEST_COUNT tests in PR branch"
            fi
          fi

          # Set all the outputs
          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT

          # For backward compatibility
          if [[ "$HAS_COLLECTION_ERRORS" == "true" || "$NO_TESTS_FOUND" == "true" ]]; then
            echo "has_errors=true" >> $GITHUB_OUTPUT
          else
            echo "has_errors=false" >> $GITHUB_OUTPUT
          fi

          echo "::debug::Full collection output for debugging:"
          cat collection_output.txt

      - name: Run tests on PR Branch
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          set -x
          echo "::debug::Current working directory for test run: $(pwd)"
          echo "::debug::Listing test files before running:"
          find . -name "test_*.py" -o -name "*_test.py" | sort

          echo "::debug::Running pytest with json report"
          echo "Running tests on PR branch..."
          python -m pytest -vv --json-report --json-report-file=pr_results.json || true

          echo "::debug::Test run completed, checking if json results file exists"
          if [ -f pr_results.json ]; then
            echo "::debug::JSON results file size: $(wc -c pr_results.json)"
            echo "::debug::JSON results file first 200 characters:"
            head -c 200 pr_results.json
            echo "Test results file successfully created for PR branch"
          else
            echo "::debug::WARNING: pr_results.json file was not created"
            echo "::error::Failed to create test results file for PR branch"
          fi

      - name: Extract test results
        id: extract-results
        run: |
          set -x
          echo "::debug::Current branch: $(git rev-parse --abbrev-ref HEAD)"
          echo "PR_BRANCH=$(git rev-parse --abbrev-ref HEAD)" >> $GITHUB_ENV
          echo "Processing test results for PR branch: $PR_BRANCH"

          echo "::debug::Running Python script to extract test results"
          python -c "
          import json
          import sys
          import os

          print('Debug: Starting test results extraction script for PR branch')

          # Default values in case file doesn't exist or is invalid
          pr_total = 0
          pr_passed = 0
          pr_percentage = 0
          failing_tests = []

          try:
              print('Debug: Attempting to open pr_results.json')
              with open('pr_results.json') as f:
                  print('Debug: File opened successfully')
                  pr_results = json.load(f)
                  print(f'Debug: JSON loaded successfully, keys: {list(pr_results.keys())}')
                  print(f'Debug: Summary key structure: {pr_results[\"summary\"] if \"summary\" in pr_results else \"No summary key\"}')
              
              # Check for collection errors by looking at exitcode or error patterns
              if pr_results.get('exitcode', 0) > 1:
                  print('Debug: Detected non-zero exitcode, likely a collection error')
                  # Let's see if we can extract the error message
                  if 'collectors' in pr_results and pr_results['collectors']:
                      print(f'Debug: Collection errors found: {pr_results[\"collectors\"]}')
                  pr_total = 0  # Explicitly set to 0 - no tests run when collection fails
                  pr_passed = 0
              elif 'summary' in pr_results and isinstance(pr_results['summary'], dict):
                  # Normal case - extract data from summary
                  summary = pr_results['summary']
                  pr_total = summary.get('total', 0)
                  pr_passed = summary.get('passed', 0)
                  print(f'Debug: Results extracted from summary - Total: {pr_total}, Passed: {pr_passed}')
                  
                  # Extract failing tests
                  if 'tests' in pr_results:
                      print('Debug: Extracting failing tests')
                      for test in pr_results['tests']:
                          if test.get('outcome') in ['failed', 'error']:
                              nodeid = test.get('nodeid', '')
                              if nodeid:
                                  failing_tests.append(nodeid)
                      
                      print(f'Debug: Found {len(failing_tests)} failing tests')
              else:
                  print('Debug: No valid summary structure found')
              
              # Calculate percentage safely
              pr_percentage = (pr_passed / pr_total * 100) if pr_total > 0 else 0
              print(f'Debug: Pass percentage calculated: {pr_percentage:.2f}%')
              
          except FileNotFoundError as e:
              print(f'Debug: File not found error: {e}')
          except KeyError as e:
              print(f'Debug: Missing key in results file: {e}')
              if 'pr_results' in locals():
                  print(f'Debug: Available keys: {list(pr_results.keys())}')
                  if 'summary' in pr_results:
                      print(f'Debug: Summary structure: {pr_results[\"summary\"]}')
          except Exception as e:
              print(f'Debug: Error processing results: {e}')
              import traceback
              print(f'Debug: Full exception: {traceback.format_exc()}')

          print(f'Total tests: {pr_total}')
          print(f'Passed tests: {pr_passed}')
          print(f'Pass percentage: {pr_percentage:.2f}%')
          print(f'Failing tests: {len(failing_tests)}')

          # Set outputs for GitHub Actions
          print('Debug: Writing results to GITHUB_OUTPUT')
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={pr_total}\\n')
              f.write(f'passed={pr_passed}\\n')
              f.write(f'percentage={pr_percentage:.2f}\\n')
              # Write failing tests as a compact JSON string to avoid issues with large outputs
              if failing_tests:
                  f.write(f'failing_tests={json.dumps(failing_tests)}\\n')
              else:
                  f.write('failing_tests=[]\\n')

          print('Debug: Results extraction completed')
          "

          echo "::debug::GITHUB_OUTPUT file content:"
          cat $GITHUB_OUTPUT
          echo "PR branch test results processed: ${{ steps.extract-results.outputs.passed }}/${{ steps.extract-results.outputs.total }} tests passed (${{ steps.extract-results.outputs.percentage }}%)"

  test-target-branch:
    needs: lint
    runs-on: ubuntu-latest
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}
      passing_tests: ${{ steps.extract-results.outputs.passing_tests }}

    steps:
      - name: Checkout target branch
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"
          ref: ${{ inputs.target_branch_to_compare }}

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Install dependencies
        run: |
          set -x
          echo "::debug::Upgrading pip with verbose output"
          python -m pip install --upgrade pip -v
          echo "::debug::Installing pytest packages with verbose output"
          pip install -v pytest pytest-json-report pytest-asyncio
          echo "::debug::Checking for requirements.txt file"
          if [ -f requirements.txt ]; then 
            echo "::debug::Installing dependencies from requirements.txt with verbose output"
            pip install -v -r requirements.txt
          else 
            echo "::debug::No requirements.txt file found"
          fi

      - name: Check for test collection errors
        id: check-collection
        run: |
          set -x
          echo "::debug::Current working directory: $(pwd)"
          echo "::debug::Listing python files:"
          find . -name "*.py" | sort
          echo "::debug::Python version: $(python --version)"
          echo "::debug::Installed packages:"
          pip list

          echo "::debug::Running pytest in collection mode"
          python -m pytest --collect-only -v > collection_output.txt 2>&1 || true

          echo "::debug::Collection completed, checking output file"
          echo "::debug::Collection output file size: $(wc -l collection_output.txt)"
          echo "::debug::First 50 lines of collection output:"
          head -n 50 collection_output.txt

          # Set default values
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          echo "::debug::Checking for collection errors"
          if grep -q "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt; then
            echo "::debug::Collection errors detected, extracting details"
            echo "::warning::Test discovery errors detected in target branch - Python modules could not be imported correctly"
            
            # Attempt to identify specific error type
            if grep -q "ImportError" collection_output.txt; then
              ERROR_TYPE="ImportError"
            elif grep -q "ModuleNotFoundError" collection_output.txt; then
              ERROR_TYPE="ModuleNotFoundError"
            elif grep -q "SyntaxError" collection_output.txt; then
              ERROR_TYPE="SyntaxError"
            elif grep -q "ERROR collecting" collection_output.txt; then
              ERROR_TYPE="CollectionError"
            elif grep -q "Interrupted:" collection_output.txt; then
              ERROR_TYPE="Interrupted"
            else 
              ERROR_TYPE="UnknownError"
            fi
            
            echo "::debug::Error type identified: $ERROR_TYPE"
            echo "Target branch discovery error type: $ERROR_TYPE"
            
            echo "::debug::Attempting to find specific error file"
            ERROR_FILE=$(grep -o "ERROR collecting.*\.py" collection_output.txt | grep -o "[a-zA-Z0-9_/]*\.py" || echo "Unknown file")
            echo "::debug::Error file identification result: $ERROR_FILE"
            
            if [[ "$ERROR_FILE" != "Unknown file" ]]; then
              echo "Error in file $ERROR_FILE"
              echo "::debug::Extracting detailed error information for $ERROR_FILE"
              grep -A 15 "$ERROR_FILE" collection_output.txt > error_details.txt
              ERROR_DETAILS=$(cat error_details.txt | tr '\n' ' ' | sed 's/"/\\"/g')
              echo "::debug::Error details extracted (truncated): ${ERROR_DETAILS:0:100}..."
            else
              # If we couldn't find a specific file, get general error info
              grep -A 15 "$ERROR_TYPE" collection_output.txt > error_details.txt
              ERROR_DETAILS=$(cat error_details.txt | tr '\n' ' ' | sed 's/"/\\"/g')
            fi
            
            echo "::warning::Target branch discovery error details: ${ERROR_DETAILS:0:200}..."
            echo "::debug::Setting has_collection_errors=true in GitHub output"
            HAS_COLLECTION_ERRORS="true"
          else
            echo "::debug::No collection errors detected, checking test count"
            echo "No discovery errors detected in target branch"
            
            echo "::debug::Extracting test count from collection output"
            TEST_COUNT=$(grep -o "collected [0-9]* item" collection_output.txt | grep -o "[0-9]*" || echo "0")
            echo "::debug::Test count extraction result: $TEST_COUNT"
            
            if [[ "$TEST_COUNT" == "0" ]]; then
              echo "::warning::No tests were found in the target branch"
              echo "::debug::Setting no_tests_found=true"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              ERROR_DETAILS="No test files were discovered in target branch that match pytest's test discovery pattern"
            else  
              echo "Found $TEST_COUNT tests in target branch"
            fi
          fi

          # Set all the outputs
          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT

          # For backward compatibility
          if [[ "$HAS_COLLECTION_ERRORS" == "true" || "$NO_TESTS_FOUND" == "true" ]]; then
            echo "has_errors=true" >> $GITHUB_OUTPUT
          else
            echo "has_errors=false" >> $GITHUB_OUTPUT
          fi

          echo "::debug::Full collection output for debugging:"
          cat collection_output.txt

      - name: Run tests on target branch
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          set -x
          echo "::debug::Current working directory for test run: $(pwd)"
          echo "::debug::Listing test files before running:"
          find . -name "test_*.py" -o -name "*_test.py" | sort

          echo "::debug::Running pytest with json report"
          echo "Running tests on target branch..."
          python -m pytest -vv --json-report --json-report-file=target_results.json || true

          echo "::debug::Test run completed, checking if json results file exists"
          if [ -f target_results.json ]; then
            echo "::debug::JSON results file size: $(wc -c target_results.json)"
            echo "::debug::JSON results file first 200 characters:"
            head -c 200 target_results.json
            echo "Test results file successfully created for target branch"
          else
            echo "::debug::WARNING: target_results.json file was not created"
            echo "::warning::Failed to create test results file for target branch"
          fi

      - name: Extract test results
        id: extract-results
        run: |
          set -x
          echo "::debug::Running Python script to extract test results"
          echo "Processing test results for target branch: ${{ inputs.target_branch_to_compare }}"

          python -c "
          import json
          import sys
          import os

          print('Debug: Starting test results extraction script for target branch')

          # Default values in case file doesn't exist or is invalid
          target_total = 0
          target_passed = 0
          target_percentage = 0
          passing_tests = []

          try:
              print('Debug: Attempting to open target_results.json')
              with open('target_results.json') as f:
                  print('Debug: File opened successfully')
                  target_results = json.load(f)
                  print(f'Debug: JSON loaded successfully, keys: {list(target_results.keys())}')
                  print(f'Debug: Summary key structure: {target_results[\"summary\"] if \"summary\" in target_results else \"No summary key\"}')
              
              # Check for collection errors by looking at exitcode or error patterns
              if target_results.get('exitcode', 0) > 1:
                  print('Debug: Detected non-zero exitcode, likely a collection error')
                  # Let's see if we can extract the error message
                  if 'collectors' in target_results and target_results['collectors']:
                      print(f'Debug: Collection errors found: {target_results[\"collectors\"]}')
                  target_total = 0  # Explicitly set to 0 - no tests run when collection fails
                  target_passed = 0
              elif 'summary' in target_results and isinstance(target_results['summary'], dict):
                  # Normal case - extract data from summary
                  summary = target_results['summary']
                  target_total = summary.get('total', 0)
                  target_passed = summary.get('passed', 0)
                  print(f'Debug: Results extracted from summary - Total: {target_total}, Passed: {target_passed}')
                  
                  # Extract passing tests
                  if 'tests' in target_results:
                      print('Debug: Extracting passing tests')
                      for test in target_results['tests']:
                          if test.get('outcome') == 'passed':
                              nodeid = test.get('nodeid', '')
                              if nodeid:
                                  passing_tests.append(nodeid)
                      
                      print(f'Debug: Found {len(passing_tests)} passing tests')
              else:
                  print('Debug: No valid summary structure found')
              
              # Calculate percentage safely
              target_percentage = (target_passed / target_total * 100) if target_total > 0 else 0
              print(f'Debug: Pass percentage calculated: {target_percentage:.2f}%')
              
          except FileNotFoundError as e:
              print(f'Debug: File not found error: {e}')
          except KeyError as e:
              print(f'Debug: Missing key in results file: {e}')
              if 'target_results' in locals():
                  print(f'Debug: Available keys: {list(target_results.keys())}')
                  if 'summary' in target_results:
                      print(f'Debug: Summary structure: {target_results[\"summary\"]}')
          except Exception as e:
              print(f'Debug: Error processing results: {e}')
              import traceback
              print(f'Debug: Full exception: {traceback.format_exc()}')

          print(f'Total tests: {target_total}')
          print(f'Passed tests: {target_passed}')
          print(f'Pass percentage: {target_percentage:.2f}%')
          print(f'Passing tests: {len(passing_tests)}')

          # Set outputs for GitHub Actions
          print('Debug: Writing results to GITHUB_OUTPUT')
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={target_total}\\n')
              f.write(f'passed={target_passed}\\n')
              f.write(f'percentage={target_percentage:.2f}\\n')
              # Write passing tests as a compact JSON string to avoid issues with large outputs
              if passing_tests:
                  f.write(f'passing_tests={json.dumps(passing_tests)}\\n')
              else:
                  f.write('passing_tests=[]\\n')

          print('Debug: Results extraction completed')
          "

          echo "::debug::GITHUB_OUTPUT file content:"
          cat $GITHUB_OUTPUT
          echo "Target branch test results processed: ${{ steps.extract-results.outputs.passed }}/${{ steps.extract-results.outputs.total }} tests passed (${{ steps.extract-results.outputs.percentage }}%)"

  compare-results:
    needs: [test-source-branch, test-target-branch]
    runs-on: ubuntu-latest
    outputs:
      has_regressions: ${{ needs.perform-regression-analysis.outputs.has_regressions }}
      regression_count: ${{ needs.perform-regression-analysis.outputs.regression_count }}

    steps:
      - name: Install bc
        run: |
          set -x
          echo "::debug::Installing bc package for numeric comparison"
          sudo apt-get update -y
          sudo apt-get install -y bc

      - name: Check for collection errors
        run: |
          set -x
          echo "::debug::Retrieving collection error status information"
          PR_COLLECTION_ERRORS="${{ needs.test-source-branch.outputs.collection_errors }}"
          PR_NO_TESTS="${{ needs.test-source-branch.outputs.no_tests_found }}"
          PR_ERROR_TYPE="${{ needs.test-source-branch.outputs.error_type }}"
          PR_ERROR_DETAILS="${{ needs.test-source-branch.outputs.error_details }}"
          TARGET_COLLECTION_ERRORS="${{ needs.test-target-branch.outputs.collection_errors }}"

          echo "::debug::PR branch collection errors: $PR_COLLECTION_ERRORS"
          echo "::debug::PR branch no tests found: $PR_NO_TESTS"
          echo "::debug::PR branch error type: $PR_ERROR_TYPE"
          echo "::debug::Target branch collection errors: $TARGET_COLLECTION_ERRORS"

          # Distinct error handling for PR branch
          if [[ "$PR_COLLECTION_ERRORS" == "true" ]]; then
            echo "::debug::PR branch has discovery errors, failing workflow"
            echo "::error::Test discovery errors in PR branch: $PR_ERROR_TYPE"
            echo "::error::$PR_ERROR_DETAILS"
            echo "❌ PR branch has test discovery errors. Python modules could not be imported correctly."
            exit 1
          fi

          if [[ "$PR_NO_TESTS" == "true" ]]; then
            echo "::debug::PR branch has no tests, failing workflow"
            echo "::error::No tests were found in the PR branch"
            echo "❌ PR branch has no tests detected. Please add test files that match pytest's discovery pattern."
            exit 1
          fi

          # Warning for target branch issues (not a failure)
          if [[ "$TARGET_COLLECTION_ERRORS" == "true" ]]; then
            echo "::debug::Target branch has discovery errors, displaying warning"
            echo "⚠️ Target branch has test discovery errors. Tests will still be compared but results may not be accurate."
          fi

          if [[ "${{ needs.test-target-branch.outputs.no_tests_found }}" == "true" ]]; then
            echo "::debug::Target branch has no tests, displaying warning"
            echo "⚠️ Target branch has no tests detected. PR branch tests will still be evaluated."
          fi

      # Split the regression check into separate steps for better control
      - name: Run regression analysis
        run: |
          set -x
          echo "Running regression analysis..."

          python3 - << 'EOF'
          import json
          import os

          try:
              # Parse the inputs
              target_passing_str = '''${{ needs.test-target-branch.outputs.passing_tests }}'''
              pr_failing_str = '''${{ needs.test-source-branch.outputs.failing_tests }}'''
              
              # Parse JSON 
              target_passing = json.loads(target_passing_str) if target_passing_str and target_passing_str != '[]' else []
              pr_failing = json.loads(pr_failing_str) if pr_failing_str and pr_failing_str != '[]' else []
              
              print(f"Parsed {len(target_passing)} passing tests from target branch")
              print(f"Parsed {len(pr_failing)} failing tests from PR branch")
              
              # Find regressions using set operations
              target_passing_set = set(target_passing)
              pr_failing_set = set(pr_failing)
              regression_tests = list(target_passing_set.intersection(pr_failing_set))
              
              # Write results to file if there are regressions
              if regression_tests:
                  print(f"Found {len(regression_tests)} regression(s)!")
                  
                  with open("regression_details.txt", "w") as f:
                      f.write(f"Found {len(regression_tests)} tests that were passing in target branch but now failing in PR branch:\\n\\n")
                      for idx, test in enumerate(sorted(regression_tests), 1):
                          f.write(f"{idx}. {test}\\n")
                  print("Regression details written to file")
              else:
                  print("No regressions found")
          except Exception as e:
              print(f"Error in regression analysis: {e}")
              import traceback
              print(traceback.format_exc())
          EOF

          echo "Regression analysis completed"

      - name: Check for regression details file
        id: check-regressions
        run: |
          set -x
          _has_regressions="false"
          _regression_count="0"

          if [ -f "regression_details.txt" ]; then
            echo "Regression details file exists"
            # Count regression lines (lines starting with a number and period)
            _current_count=$(grep -c "^[0-9]\+\." regression_details.txt || echo "0")
            echo "Found $_current_count regression items in file"

            if [ "$_current_count" -gt 0 ]; then
              _has_regressions="true"
              _regression_count="$_current_count"
            else
              # File exists but no regressions counted (e.g. empty or malformed)
              _has_regressions="false"
              _regression_count="0"
            fi
            
            echo "Regression details:"
            cat regression_details.txt
          else
            echo "No regression details file found - no regressions detected"
            _has_regressions="false"
            _regression_count="0"
          fi

          echo "HAS_REGRESSIONS=$_has_regressions" >> $GITHUB_OUTPUT
          echo "REGRESSION_COUNT=$_regression_count" >> $GITHUB_OUTPUT

          if [[ "$_has_regressions" == "true" ]]; then
            echo "::error::Test Regressions Found: $_regression_count test(s) that were passing in target branch are now failing."
            # The regression_details.txt is already cat-ed above if it exists
          else
            # This case handles if _has_regressions is false.
            # Check if file exists unexpectedly (e.g., created but empty of actual regression lines)
            if [ -f regression_details.txt ] && [ "$_has_regressions" == "false" ]; then
               # This condition means regression_details.txt was found, but _current_count was 0 or it wasn't found.
               # The initial cat of regression_details.txt would have shown its content.
               echo "::notice::Regression details file (regression_details.txt) was found but no valid regression entries were counted by this step, or the file was empty."
            else
               # This means regression_details.txt was not found at all.
               echo "No test regressions detected by this step."
            fi
          fi

      - name: Upload regression details artifact
        if: steps.check-regressions.outputs.HAS_REGRESSIONS == 'true' && steps.check-regressions.outputs.REGRESSION_COUNT > 0
        uses: actions/upload-artifact@v4
        with:
          name: regression_details_pr_${{ github.event.pull_request.number || github.run_id }}_tests
          path: regression_details.txt
          retention-days: 1

      - name: Compare test results
        run: |
          set -x
          echo "::debug::Starting detailed test results comparison"

          # Print all input values for debugging
          echo "::debug::Target branch total: ${{ needs.test-target-branch.outputs.total }}"
          echo "::debug::Target branch passed: ${{ needs.test-target-branch.outputs.passed }}"
          echo "::debug::Target branch percentage: ${{ needs.test-target-branch.outputs.percentage }}"
          echo "::debug::PR branch total: ${{ needs.test-source-branch.outputs.total }}"
          echo "::debug::PR branch passed: ${{ needs.test-source-branch.outputs.passed }}"
          echo "::debug::PR branch percentage: ${{ needs.test-source-branch.outputs.percentage }}"

          echo "Test Results Summary:"
          echo "Target branch (${{ inputs.target_branch_to_compare }}): ${{ needs.test-target-branch.outputs.passed }}/${{ needs.test-target-branch.outputs.total }} tests passed (${{ needs.test-target-branch.outputs.percentage }}%)"
          echo "PR branch: ${{ needs.test-source-branch.outputs.passed }}/${{ needs.test-source-branch.outputs.total }} tests passed (${{ needs.test-source-branch.outputs.percentage }}%)"

          echo "::debug::Checking if PR branch has tests"
          if [[ "${{ needs.test-source-branch.outputs.total }}" == "0" ]]; then
            echo "::debug::PR branch has no tests, failing workflow"
            echo "::error::No tests were found in the PR branch"
            echo "❌ PR branch has no tests detected. Please add test files that match pytest's discovery pattern."
            exit 1
          fi

          echo "::debug::Converting string outputs to variables for comparison"
          PR_PASSED=${{ needs.test-source-branch.outputs.passed }}
          TARGET_PASSED=${{ needs.test-target-branch.outputs.passed }}
          PR_PERCENTAGE=${{ needs.test-source-branch.outputs.percentage }}
          TARGET_PERCENTAGE=${{ needs.test-target-branch.outputs.percentage }}
          PR_TOTAL=${{ needs.test-source-branch.outputs.total }}
          TARGET_TOTAL=${{ needs.test-target-branch.outputs.total }}

          echo "::debug::PR_PASSED=$PR_PASSED"
          echo "::debug::TARGET_PASSED=$TARGET_PASSED"
          echo "::debug::PR_PERCENTAGE=$PR_PERCENTAGE"
          echo "::debug::TARGET_PERCENTAGE=$TARGET_PERCENTAGE"
          echo "::debug::PR_TOTAL=$PR_TOTAL"
          echo "::debug::TARGET_TOTAL=$TARGET_TOTAL"
          echo "::debug::HAS_REGRESSIONS=${{ needs.perform-regression-analysis.outputs.has_regressions }}"
          echo "::debug::REGRESSION_COUNT=${{ needs.perform-regression-analysis.outputs.regression_count }}"

          # Handle case where target has no tests
          if [[ "$TARGET_TOTAL" == "0" ]]; then
            echo "::debug::Target branch has no tests, checking PR branch tests only"
            if [[ "$PR_PASSED" -gt 0 ]]; then
              echo "✅ PR branch has tests and some are passing (target branch has no tests)"
              exit 0
            else
              echo "❌ PR branch has no passing tests"
              echo "  - Pass percentage: $PR_PERCENTAGE%"
              exit 1
            fi
          fi

          # Fail if any tests passed in target branch but now fail in PR branch
          if [[ "${{ needs.perform-regression-analysis.outputs.has_regressions }}" == "true" ]]; then
            echo "❌ PR branch has test regressions from target branch"
            REGRESSION_COUNT_VAL=${{ needs.perform-regression-analysis.outputs.regression_count }}
            echo "  - $REGRESSION_COUNT_VAL tests that were passing in target branch are now failing"

            echo "### :x: Test Regressions Detected!" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**$REGRESSION_COUNT_VAL test(s) that were passing in the target branch are now failing in the PR branch.**" >> $GITHUB_STEP_SUMMARY
            echo "This job (\`compare-results\`) has been marked as failed due to these regressions." >> $GITHUB_STEP_SUMMARY

            if [ -f regression_details.txt ]; then # Check if file exists (it should from previous step)
                echo "Failing tests (regressions) list:"
                cat regression_details.txt
            else
                echo "::warning::Regression details file (regression_details.txt) not found in Compare test results step. It might be available as an artifact from the 'perform-regression-analysis' job."
            fi
            exit 1
          fi

          echo "::debug::Performing comparison with bc"
          echo "::debug::Comparing passed tests: $(echo "$PR_PASSED >= $TARGET_PASSED" | bc -l)"
          echo "::debug::Comparing pass percentage: $(echo "$PR_PERCENTAGE >= $TARGET_PERCENTAGE" | bc -l)"

          # Continue with the original comparison if no regressions
          if (( $(echo "$PR_PASSED >= $TARGET_PASSED" | bc -l) )) && (( $(echo "$PR_PERCENTAGE >= $TARGET_PERCENTAGE" | bc -l) )); then
            echo "::debug::PR branch has equal or better results"
            echo "✅ PR branch has equal or better test results than target branch"
            
            # Additional verbose information about improvement
            if (( $(echo "$PR_PASSED > $TARGET_PASSED" | bc -l) )); then
              IMPROVEMENT=$(( $PR_PASSED - $TARGET_PASSED ))
              echo "  - Improvement: $IMPROVEMENT more passing tests than target branch"
            fi
            
            if (( $(echo "$PR_PERCENTAGE > $TARGET_PERCENTAGE" | bc -l) )); then
              PERCENTAGE_IMPROVEMENT=$(echo "$PR_PERCENTAGE - $TARGET_PERCENTAGE" | bc -l)
              echo "  - Percentage improvement: +${PERCENTAGE_IMPROVEMENT}% compared to target branch"
            fi
            
            exit 0
          else
            echo "::debug::PR branch has worse results"
            echo "❌ PR branch has worse test results than target branch"
            echo "  - Passed tests: $PR_PASSED vs $TARGET_PASSED on target branch"
            echo "  - Pass percentage: $PR_PERCENTAGE% vs $TARGET_PERCENTAGE% on target branch"
            
            # Add to job summary for general comparison failure
            echo "### :x: Test Comparison Failed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The PR branch has worse test results than the target branch:" >> $GITHUB_STEP_SUMMARY
            echo "- Passed tests: $PR_PASSED (PR) vs $TARGET_PASSED (Target)" >> $GITHUB_STEP_SUMMARY
            echo "- Pass percentage: $PR_PERCENTAGE% (PR) vs $TARGET_PERCENTAGE% (Target)" >> $GITHUB_STEP_SUMMARY
            
            # Calculate regression metrics
            if (( $(echo "$PR_PASSED < $TARGET_PASSED" | bc -l) )); then
              REGRESSION=$(( $TARGET_PASSED - $PR_PASSED ))
              echo "  - Regression: $REGRESSION fewer passing tests than target branch"
            fi
            
            if (( $(echo "$PR_PERCENTAGE < $TARGET_PERCENTAGE" | bc -l) )); then
              PERCENTAGE_REGRESSION=$(echo "$TARGET_PERCENTAGE - $PR_PERCENTAGE" | bc -l)
              echo "  - Percentage regression: -${PERCENTAGE_REGRESSION}% compared to target branch"
            fi
            
            exit 1
          fi

  perform-regression-analysis:
    needs: [test-source-branch, test-target-branch]
    uses: ./.github/workflows/meta-regression-analysis.yml
    with:
      target_branch_passing_items_json: ${{ needs.test-target-branch.outputs.passing_tests }}
      pr_branch_failing_items_json: ${{ needs.test-source-branch.outputs.failing_tests }}
      item_type_singular: "test"
      item_type_plural: "tests"
      pr_number: ${{ github.event.pull_request.number }}
      run_id: ${{ github.run_id }}
    # Secrets are not needed for this reusable workflow currently
    # secrets: inherit

  # Conditionally run notification job only if needed
  prepare-notification:
    name: Prepare Notification Data
    needs:
      [
        lint,
        test-source-branch,
        test-target-branch,
        compare-results,
        perform-regression-analysis,
      ]
    # Notify on collection errors, no tests found, compare result failure, or if regressions are detected
    if: |
      always() &&
      (
        needs.test-source-branch.outputs.collection_errors == 'true' ||
        needs.test-source-branch.outputs.no_tests_found == 'true' ||
        needs.compare-results.result == 'failure' ||
        needs.perform-regression-analysis.outputs.has_regressions == 'true'
      )
    runs-on: ubuntu-latest
    outputs:
      message_body: ${{ steps.construct_notification.outputs.message_body_out }}
      ping_user_ids: ${{ steps.construct_notification.outputs.ping_user_ids_out }}
      artifact_path: ${{ steps.construct_notification.outputs.artifact_path_out }}
      should_notify: "true"
      webhook_available_for_alert: ${{ steps.check_webhook_availability.outputs.webhook_available }}

    steps:
      - name: Check for Discord Webhook URL
        id: check_webhook_availability
        run: |
          if [ -z "${{ secrets.DISCORD_WEBHOOK_URL }}" ]; then
            echo "::notice::DISCORD_WEBHOOK_URL secret is not set. Discord notifications will likely be skipped by the alert workflow if it relies on this secret."
            echo "webhook_available=false" >> $GITHUB_OUTPUT
          else
            echo "::debug::DISCORD_WEBHOOK_URL secret is present."
            echo "webhook_available=true" >> $GITHUB_OUTPUT
          fi
      - name: Download regression details (if any)
        id: download_regressions
        if: needs.perform-regression-analysis.outputs.has_regressions == 'true' && needs.perform-regression-analysis.outputs.regression_count > 0
        uses: actions/download-artifact@v4
        with:
          # The artifact name from reusable-regression-analyzer.yml
          name: regression_details_pr_${{ github.event.pull_request.number || github.run_id }}_tests
          path: . # Download to current directory
        continue-on-error: true

      - name: Construct Discord Notification
        id: construct_notification
        env:
          LINT_RESULT: ${{ needs.lint.result }}
          SOURCE_TEST_RESULT: ${{ needs.test-source-branch.result }}
          TARGET_TEST_RESULT: ${{ needs.test-target-branch.result }}
          COMPARE_RESULT: ${{ needs.compare-results.result }}
          PR_COLLECTION_ERRORS: ${{ needs.test-source-branch.outputs.collection_errors }}
          PR_NO_TESTS_FOUND: ${{ needs.test-source-branch.outputs.no_tests_found }}
          PR_ERROR_TYPE: ${{ needs.test-source-branch.outputs.error_type }}
          PR_ERROR_DETAILS_TRUNCATED: ${{ needs.test-source-branch.outputs.error_details }}
          HAS_REGRESSIONS: ${{ needs.perform-regression-analysis.outputs.has_regressions }}
          REGRESSION_COUNT: ${{ needs.perform-regression-analysis.outputs.regression_count }}
          PR_TOTAL_TESTS: ${{ needs.test-source-branch.outputs.total }}
          PR_PASSED_TESTS: ${{ needs.test-source-branch.outputs.passed }}
          PR_PERCENTAGE: ${{ needs.test-source-branch.outputs.percentage }}
          TARGET_TOTAL_TESTS: ${{ needs.test-target-branch.outputs.total }}
          TARGET_PASSED_TESTS: ${{ needs.test-target-branch.outputs.passed }}
          TARGET_PERCENTAGE: ${{ needs.test-target-branch.outputs.percentage }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          PR_TITLE: ${{ github.event.pull_request.title }}
          PR_URL: ${{ github.event.pull_request.html_url }}
          TARGET_BRANCH_NAME: ${{ inputs.target_branch_to_compare }}
          PR_BRANCH_NAME: ${{ github.head_ref }}
          REPO_URL: ${{ github.server_url }}/${{ github.repository }}
          ACTION_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          GH_ASSIGNEES_JSON: ${{ toJson(github.event.pull_request.assignees) }}
          USER_MAP_JSON: ${{ secrets.GH_TO_DISCORD_USER_MAP || '{}' }}
          REGRESSION_FILE_PATH: "regression_details.txt"
          DOWNLOAD_REGRESSIONS_OUTCOME: ${{ steps.download_regressions.outcome }}
        run: |
          MESSAGE_LINES=() # Use an array to build message lines
          PING_KEYS_OUTPUT="" # Will be comma-separated GitHub logins
          ARTIFACT_PATH_OUTPUT=""

          # 1. Determine Pings - Collect GitHub Logins to pass to alert-discord.yml
          if [ -n "$USER_MAP_JSON" ] && [ "$USER_MAP_JSON" != "{}" ] && command -v jq &> /dev/null; then
            ASSIGNEE_LOGINS_ARRAY=($(echo "$GH_ASSIGNEES_JSON" | jq -r '.[].login // empty'))
            if [ ${#ASSIGNEE_LOGINS_ARRAY[@]} -gt 0 ]; then
              PING_KEYS_OUTPUT=$(IFS=,; echo "${ASSIGNEE_LOGINS_ARRAY[*]}")
            fi
            # Make this a standard echo for better visibility
            echo "Ping Keys Output (GitHub Logins from test-pytest.yml): [$PING_KEYS_OUTPUT]"
          elif [ -n "$USER_MAP_JSON" ] && [ "$USER_MAP_JSON" != "{}" ] && ! command -v jq &> /dev/null; then
            echo "::warning::jq is not available. Cannot determine GitHub users for pings."
          else
            echo "Debug: No user map JSON or jq not found, or no assignees. PING_KEYS_OUTPUT will be empty."
          fi
          echo "ping_user_ids_out=$PING_KEYS_OUTPUT" >> $GITHUB_OUTPUT

          # Store branch names in variables with proper quoting
          PR_BRANCH="${PR_BRANCH_NAME:-unknown}"
          TARGET_BRANCH="${TARGET_BRANCH_NAME:-unknown}"

          # 2. Construct Message Body - TEMPORARILY SIMPLIFIED FOR NEWLINE DEBUGGING
          MESSAGE_LINES+=("Simple Line 1 from test-pytest")
          MESSAGE_LINES+=("Simple Line 2 from test-pytest")
          MESSAGE_LINES+=("Simple Line 3 with PING_KEYS_OUTPUT: [$PING_KEYS_OUTPUT]") # Include for checking

          # Original complex message construction commented out for now
          # MESSAGE_LINES+=("**Pytest Failure on [\`${PR_BRANCH}\`](${REPO_URL}/tree/${PR_BRANCH}) for PR [#${PR_NUMBER}: ${PR_TITLE}](${PR_URL})**")
          # ... (rest of original message lines) ...

          FINAL_MESSAGE_BODY=$(printf "%s\n" "${MESSAGE_LINES[@]}")
          if [ ${#MESSAGE_LINES[@]} -gt 0 ]; then
            FINAL_MESSAGE_BODY="${FINAL_MESSAGE_BODY%\n}"
          fi

          echo "::debug::Final message body prepared in test-pytest.yml:"
          echo "$FINAL_MESSAGE_BODY"
          echo "$FINAL_MESSAGE_BODY" | cat -A # Show with non-printables

          echo "message_body_out<<EOF" >> $GITHUB_OUTPUT
          echo "$FINAL_MESSAGE_BODY" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          echo "artifact_path_out=$ARTIFACT_PATH_OUTPUT" >> $GITHUB_OUTPUT

  # Even if webhook checks are handled inside the alert workflow,
  # we still need to pass the secret to satisfy GitHub's workflow validation
  notify-discord:
    name: Send Discord Notification
    needs: [prepare-notification]
    if: |
      always() &&
      needs.prepare-notification.outputs.should_notify == 'true' &&
      needs.prepare-notification.outputs.webhook_available_for_alert == 'true'
    uses: ./.github/workflows/alert-discord.yml
    with:
      message_body: ${{ needs.prepare-notification.outputs.message_body }}
      ping_user_ids: ${{ needs.prepare-notification.outputs.ping_user_ids }}
      artifact_paths: ${{ needs.prepare-notification.outputs.artifact_path }}
      should_notify: ${{ needs.prepare-notification.outputs.should_notify }}
    secrets:
      DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      DISCORD_USER_MAP: ${{ secrets.GH_TO_DISCORD_USER_MAP }}
