name: Reusable Mocha Runner

on:
  workflow_call:
    inputs:
      ref:
        description: "Git ref to checkout and test. Leave empty for default checkout."
        required: false
        type: string
        default: ""
      node-version:
        description: "Node.js version to use for testing."
        required: false
        type: string
        default: "18"
      runs_on:
        description: "Runner label for the test job."
        required: false
        type: string
        default: '["self-hosted", "multithreaded"]'
      artifact_name:
        description: "Name for the test results artifact."
        required: true
        type: string
      parallel_workers:
        description: "Number of parallel workers for Mocha. Leave empty for runner default (6 for multithreaded, 1 for singlethreaded). Use 'auto' for cgroup-aware CPU count, or a number."
        required: false
        type: string
        default: ""
      install-command:
        description: "Optional command to install dependencies (defaults to npm/pnpm/yarn auto-detection)."
        required: false
        type: string
        default: ""
      mocha-command:
        description: "Base command used to invoke Mocha (e.g., npx mocha, pnpm exec mocha)."
        required: false
        type: string
        default: "npx mocha"
      mocha-extra-args:
        description: "Additional arguments to pass to the Mocha command (applied before workflow-managed flags)."
        required: false
        type: string
        default: ""
      working-directory:
        description: "Directory where install and test commands should be executed."
        required: false
        type: string
        default: "."
    outputs:
      total:
        description: "Total number of tests"
        value: ${{ jobs.test.outputs.total }}
      passed:
        description: "Number of passing tests"
        value: ${{ jobs.test.outputs.passed }}
      percentage:
        description: "Pass percentage"
        value: ${{ jobs.test.outputs.percentage }}
      collection_errors:
        description: "Whether collection errors occurred"
        value: ${{ jobs.test.outputs.collection_errors }}
      no_tests_found:
        description: "Whether no tests were found"
        value: ${{ jobs.test.outputs.no_tests_found }}
      has_errors:
        description: "Whether any errors occurred"
        value: ${{ jobs.test.outputs.has_errors }}
      error_type:
        description: "Type of error if any"
        value: ${{ jobs.test.outputs.error_type }}
      failing_count:
        description: "Number of failing tests"
        value: ${{ jobs.test.outputs.failing_count }}
      error_count:
        description: "Number of errored tests"
        value: ${{ jobs.test.outputs.error_count }}
      skipped_count:
        description: "Number of skipped tests"
        value: ${{ jobs.test.outputs.skipped_count }}
      xfailed_count:
        description: "Number of xfailed tests (pending in Mocha)"
        value: ${{ jobs.test.outputs.xfailed_count }}

jobs:
  test:
    runs-on: ${{ fromJSON(inputs.runs_on) }}
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      failing_count: ${{ steps.extract-results.outputs.failing_count }}
      error_count: ${{ steps.extract-results.outputs.error_count }}
      skipped_count: ${{ steps.extract-results.outputs.skipped_count }}
      xfailed_count: ${{ steps.extract-results.outputs.xfailed_count }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"
          ref: ${{ inputs.ref || github.ref }}

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "${{ inputs['node-version'] }}"

      - name: Install dependencies
        run: |
          set -e
          if command -v corepack >/dev/null 2>&1; then
            corepack enable >/dev/null 2>&1 || true
          fi

          INSTALL_COMMAND="${{ inputs['install-command'] }}"
          if [ -n "$INSTALL_COMMAND" ]; then
            echo "Running custom install command: $INSTALL_COMMAND"
            eval "$INSTALL_COMMAND"
          elif [ -f package-lock.json ]; then
            echo "Detected package-lock.json; running npm ci"
            npm ci
          elif [ -f yarn.lock ]; then
            if command -v yarn >/dev/null 2>&1; then
              echo "Detected yarn.lock; running yarn install --frozen-lockfile"
              yarn install --frozen-lockfile
            else
              echo "::warning::yarn.lock detected but yarn is unavailable. Falling back to npm install."
              npm install
            fi
          elif [ -f pnpm-lock.yaml ]; then
            if command -v pnpm >/dev/null 2>&1; then
              echo "Detected pnpm-lock.yaml; running pnpm install --frozen-lockfile"
              pnpm install --frozen-lockfile
            else
              echo "::warning::pnpm-lock.yaml detected but pnpm is unavailable. Falling back to npm install."
              npm install
            fi
          else
            echo "No lockfile detected; running npm install"
            npm install
          fi
        working-directory: ${{ inputs['working-directory'] }}

      - name: Check for test collection errors
        id: check-collection
        run: |
          echo "Running Mocha collection check..."
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"

          MOCHA_COMMAND="${{ inputs['mocha-command'] }}"
          MOCHA_EXTRA_ARGS="${{ inputs['mocha-extra-args'] }}"
          COLLECTION_COMMAND="$MOCHA_COMMAND $MOCHA_EXTRA_ARGS --reporter json --dry-run"

          echo "Executing: $COLLECTION_COMMAND"

          set +e
          eval "$COLLECTION_COMMAND" > collection_output.json 2> collection_output.txt
          EXIT_CODE=$?
          set -e

          if [ "$EXIT_CODE" -ne 0 ]; then
            HAS_COLLECTION_ERRORS="true"
            if grep -qi "Cannot find module" collection_output.txt; then
              ERROR_TYPE="ModuleNotFoundError"
            elif grep -qi "SyntaxError" collection_output.txt; then
              ERROR_TYPE="SyntaxError"
            elif grep -qi "TypeError" collection_output.txt; then
              ERROR_TYPE="TypeError"
            elif grep -qi "ReferenceError" collection_output.txt; then
              ERROR_TYPE="ReferenceError"
            else
              ERROR_TYPE="ExecutionError"
            fi
            echo "::error::Test discovery errors detected via Mocha ($ERROR_TYPE)"
          else
            # Parse JSON output to count tests
            TEST_COUNT=0
            if [ -s collection_output.json ]; then
              TEST_COUNT=$(node -e "
                try {
                  const data = JSON.parse(require('fs').readFileSync('collection_output.json', 'utf-8'));
                  console.log(data.stats?.tests || (data.tests?.length || 0));
                } catch (e) {
                  console.log(0);
                }
              " 2>/dev/null || echo "0")
            fi

            if [ "$TEST_COUNT" = "0" ]; then
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              echo "::warning::No tests were found"
            else
              echo "Found $TEST_COUNT test(s)"
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> "$GITHUB_OUTPUT"
          echo "no_tests_found=$NO_TESTS_FOUND" >> "$GITHUB_OUTPUT"
          echo "error_type=$ERROR_TYPE" >> "$GITHUB_OUTPUT"
          if [[ "$HAS_COLLECTION_ERRORS" == "true" || "$NO_TESTS_FOUND" == "true" ]]; then
            echo "has_errors=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_errors=false" >> "$GITHUB_OUTPUT"
          fi
        working-directory: ${{ inputs['working-directory'] }}

      - name: Run tests
        id: run-tests
        continue-on-error: true
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          set -euo pipefail

          # Source shared worker calculation script
          source "$GITHUB_WORKSPACE/.github/scripts/cgroup_workers.sh"
          WORKERS=$(determine_workers "${{ inputs.parallel_workers }}" '${{ inputs.runs_on }}')

          echo "Running tests with $WORKERS workers..."

          # Build parallel flag for Mocha
          PARALLEL_FLAG=""
          if [ "$WORKERS" != "1" ]; then
            PARALLEL_FLAG="--parallel --jobs $WORKERS"
          fi

          MOCHA_COMMAND="${{ inputs['mocha-command'] }}"
          MOCHA_EXTRA_ARGS="${{ inputs['mocha-extra-args'] }}"

          set +e
          eval "$MOCHA_COMMAND $MOCHA_EXTRA_ARGS $PARALLEL_FLAG --reporter json" > results.json 2> test_output.txt
          MOCHA_EXIT=$?
          set -e

          echo "mocha_exit_code=$MOCHA_EXIT" >> "$GITHUB_OUTPUT"

          if [ "$MOCHA_EXIT" -eq 137 ]; then
            echo "::warning::Tests were killed (exit 137) - likely OOM. Partial results may be available."
          fi

          if [ -s results.json ]; then
            echo "Test execution completed (exit code: $MOCHA_EXIT)"
          else
            echo "No results.json - creating empty results file"
            echo '{"stats": {"tests": 0, "passes": 0}, "tests": []}' > results.json
          fi
        working-directory: ${{ inputs['working-directory'] }}

      - name: Extract test results
        id: extract-results
        run: |
          node <<'NODE'
          const fs = require('fs');
          const path = require('path');

          let total = 0;
          let passed = 0;
          let percentage = 0;
          const passingTests = [];
          const failingTests = [];
          const errorTests = [];
          const skippedTests = [];
          const xfailedTests = [];
          const xpassedTests = [];
          const allTests = [];
          const skippedWithReasons = {};
          const xfailedWithReasons = {};
          const warningsList = [];

          const safeRead = (filePath) => {
            try {
              return fs.readFileSync(filePath, 'utf-8');
            } catch (error) {
              return null;
            }
          };

          const rawResults = safeRead('results.json');

          if (rawResults) {
            try {
              const data = JSON.parse(rawResults);
              const stats = data?.stats ?? {};
              const tests = Array.isArray(data?.tests) ? data.tests : [];

              total = Number.isFinite(stats.tests) ? Number(stats.tests) : tests.length;
              passed = Number.isFinite(stats.passes)
                ? Number(stats.passes)
                : Array.isArray(data?.passes)
                ? data.passes.length
                : 0;

              for (const test of tests) {
                const suitePath = test.file ? path.relative(process.cwd(), test.file) || test.file : '';
                const fullTitle = test.fullTitle || test.title || 'Unnamed test';
                const identifier = suitePath ? `${suitePath}::${fullTitle}` : fullTitle;
                const state = test.state || (test.pending ? 'pending' : undefined);

                allTests.push(identifier);

                switch (state) {
                  case 'passed':
                    passingTests.push(identifier);
                    break;
                  case 'failed':
                    // Check if it's an error vs assertion failure
                    if (test.err && test.err.name && !test.err.name.includes('AssertionError')) {
                      errorTests.push(identifier);
                    } else {
                      failingTests.push(identifier);
                    }
                    break;
                  case 'pending':
                    skippedTests.push(identifier);
                    skippedWithReasons[identifier] =
                      (test.err && test.err.message) || 'Marked as pending in Mocha output';
                    break;
                  default: {
                    const matchBy = (collection) =>
                      Array.isArray(collection)
                        ? collection.find(
                            (item) =>
                              item &&
                              item.fullTitle === test.fullTitle &&
                              (item.file === test.file || (!item.file && !test.file)),
                          )
                        : undefined;

                    if (!state) {
                      if (matchBy(data?.failures)) {
                        failingTests.push(identifier);
                        break;
                      }
                      if (matchBy(data?.passes)) {
                        passingTests.push(identifier);
                        break;
                      }
                      const pendingMatch = matchBy(data?.pending);
                      if (pendingMatch) {
                        skippedTests.push(identifier);
                        skippedWithReasons[identifier] =
                          (pendingMatch.err && pendingMatch.err.message) || 'Marked as pending in Mocha output';
                        break;
                      }
                    }

                    skippedTests.push(identifier);
                    skippedWithReasons[identifier] = 'Test state unknown; treated as skipped';
                    break;
                  }
                }
              }

              if (!passed && passingTests.length) {
                passed = passingTests.length;
              }

              percentage = total > 0 ? (passed / total) * 100 : 0;

              console.log(`Parsed Mocha results: ${passed}/${total} passed (${percentage.toFixed(2)}%)`);
            } catch (error) {
              console.log(`Error parsing results.json: ${error.message}`);
            }
          } else {
            console.log('results.json not found. No test data to parse.');
          }

          // Extract warnings from test output
          const outputContent = safeRead('test_output.txt');
          if (outputContent) {
            const warnLines = outputContent
              .split('\n')
              .map((line) => line.trimEnd())
              .filter((line) => /\bWARN(?:ING)?\b/i.test(line));
            if (warnLines.length) {
              warningsList.push(...warnLines.slice(0, 200));
              console.log(`Collected ${warningsList.length} warning line(s) from Mocha output.`);
            }
          }

          // Save artifact data
          const testData = {
            passing_tests: passingTests,
            failing_tests: failingTests,
            error_tests: errorTests,
            skipped_tests: skippedTests,
            xfailed_tests: xfailedTests,
            xpassed_tests: xpassedTests,
            all_tests: allTests,
            skipped_tests_with_reasons: skippedWithReasons,
            xfailed_tests_with_reasons: xfailedWithReasons,
            warnings: warningsList,
          };

          fs.writeFileSync('test_data.json', JSON.stringify(testData, null, 2));

          console.log(`Results: ${passed}/${total} (${percentage.toFixed(2)}%)`);

          const outputLines = [
            `total=${total}`,
            `passed=${passed}`,
            `percentage=${percentage.toFixed(2)}`,
            `failing_count=${failingTests.length}`,
            `error_count=${errorTests.length}`,
            `skipped_count=${skippedTests.length}`,
            `xfailed_count=${xfailedTests.length}`,
          ];

          if (process.env.GITHUB_OUTPUT) {
            fs.appendFileSync(process.env.GITHUB_OUTPUT, `${outputLines.join('\n')}\n`);
          }
          NODE
        working-directory: ${{ inputs['working-directory'] }}

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact_name }}
          path: |
            ${{ inputs['working-directory'] }}/test_data.json
            ${{ inputs['working-directory'] }}/test_output.txt
            ${{ inputs['working-directory'] }}/results.json
          retention-days: 3
          if-no-files-found: ignore
