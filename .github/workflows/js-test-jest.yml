name: Reusable Compare Jest Results

on:
  workflow_call:
    inputs:
      target_branch_to_compare:
        description: "The target branch to compare against (e.g., main, refs/heads/main)."
        required: true
        type: string
      node-version:
        description: "Node.js version to use for testing."
        required: false
        type: string
        default: "18"
      install-command:
        description: "Optional command to install dependencies (defaults to npm/pnpm/yarn auto-detection)."
        required: false
        type: string
        default: ""
      jest-command:
        description: "Base command used to invoke Jest (e.g., npx jest, pnpm exec jest)."
        required: false
        type: string
        default: "npx jest"
      jest-extra-args:
        description: "Additional arguments to pass to the Jest command (applied before workflow-managed flags)."
        required: false
        type: string
        default: "--runInBand"
      working-directory:
        description: "Directory where install and test commands should be executed."
        required: false
        type: string
        default: "."
      ping_latest_committer:
        description: "If true, the latest committer on the PR will be added to the ping list."
        required: false
        type: boolean
        default: false
      runs_on:
        required: false
        type: string
        default: "self-hosted"
    secrets:
      DISCORD_WEBHOOK_URL:
        description: "Discord Webhook URL for failure notifications. If not provided, notifications are skipped."
        required: false
      DISCORD_USER_MAP:
        description: 'JSON string mapping GitHub usernames to Discord User IDs (e.g., {"user1":"id1"}). If not provided, users won''t be pinged.'
        required: false
    outputs:
      pr_total:
        description: "Total tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.total }}
      pr_passed:
        description: "Passed tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.passed }}
      pr_percentage:
        description: "Pass percentage in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.percentage }}
      pr_collection_errors:
        description: "PR branch has collection errors"
        value: ${{ jobs.test-source-branch.outputs.collection_errors }}
      pr_no_tests_found:
        description: "PR branch has no tests found"
        value: ${{ jobs.test-source-branch.outputs.no_tests_found }}
      target_total:
        description: "Total tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.total }}
      target_passed:
        description: "Passed tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.passed }}
      target_percentage:
        description: "Pass percentage in target branch"
        value: ${{ jobs.test-target-branch.outputs.percentage }}
      has_regressions:
        description: "Boolean indicating if regressions were found"
        value: ${{ jobs.compare-results.outputs.has_regressions }}
      regression_count:
        description: "Number of test regressions found"
        value: ${{ jobs.compare-results.outputs.regression_count }}

jobs:
  test-source-branch:
    runs-on: ${{ inputs.runs_on }}
    defaults:
      run:
        shell: bash
        working-directory: ${{ inputs.working-directory }}
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}
      failing_count: ${{ steps.extract-results.outputs.failing_count }}
      skipped_count: ${{ steps.extract-results.outputs.skipped_count }}

    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "${{ inputs.node-version }}"

      - name: Detect package manager and install dependencies
        id: install-deps
        run: |
          if [ -n "${{ inputs.install-command }}" ]; then
            echo "Using custom install command: ${{ inputs.install-command }}"
            ${{ inputs.install-command }}
          elif [ -f "pnpm-lock.yaml" ]; then
            echo "Detected pnpm"
            npm install -g pnpm
            pnpm install
          elif [ -f "yarn.lock" ]; then
            echo "Detected yarn"
            npm install -g yarn
            yarn install
          else
            echo "Using npm (default)"
            npm ci || npm install
          fi

      - name: Check for test discovery issues
        id: check-collection
        run: |
          echo "Running Jest --listTests to verify discovery..."
          set +e
          ${{ inputs.jest-command }} --listTests > collection_output.txt 2>&1
          LIST_EXIT_CODE=$?
          set -e

          # Set default values
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          if [ "$LIST_EXIT_CODE" -ne 0 ]; then
            echo "::error::Test discovery command failed in PR branch"
            HAS_COLLECTION_ERRORS="true"
            ERROR_TYPE="DiscoveryCommandFailed"
            ERROR_DETAILS=$(head -n 200 collection_output.txt | tr '\n' ' ' | sed 's/"/\\"/g')
          else
            echo "Jest --listTests executed successfully"

            TEST_COUNT=$(wc -l < collection_output.txt 2>/dev/null || echo "0")
            
            if [ "$TEST_COUNT" == "0" ] || grep -qi "No tests found" collection_output.txt; then
              echo "::warning::No tests were found in the PR branch"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              ERROR_DETAILS="No tests were discovered by Jest --listTests"
            else
              echo "Found $TEST_COUNT test files in PR branch"
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT

      - name: Run Jest tests
        id: run-tests
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Running Jest tests with JSON reporter..."
          set +e
          ${{ inputs.jest-command }} ${{ inputs.jest-extra-args }} --json --outputFile=pr_results.json > pr_test_output.txt 2>&1
          TEST_EXIT_CODE=$?
          set -e

          echo "Test exit code: $TEST_EXIT_CODE"
          echo "exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT

      - name: Extract results and prepare JSON
        id: extract-results
        if: always()
        run: |
          echo "Processing test results..."
          python3 - << 'PY'
          import json
          import os
          import sys

          # Default values
          pr_total = 0
          pr_passed = 0
          pr_percentage = 0
          failing_tests = []
          skipped_tests = []
          passing_tests = []
          all_tests = []
          warnings_list = []

          try:
              if os.path.exists('pr_results.json'):
                  print('Found pr_results.json')
                  with open('pr_results.json', 'r') as f:
                      results = json.load(f)
                  
                  # Jest JSON structure
                  if 'numTotalTests' in results:
                      pr_total = results.get('numTotalTests', 0)
                      pr_passed = results.get('numPassedTests', 0)
                      num_failed = results.get('numFailedTests', 0)
                      num_skipped = results.get('numPendingTests', 0) + results.get('numTodoTests', 0)
                      
                      # Process test results
                      for test_result in results.get('testResults', []):
                          test_file = test_result.get('name', '')
                          
                          for assertion in test_result.get('assertionResults', []):
                              ancestor_titles = assertion.get('ancestorTitles', [])
                              title = assertion.get('title', '')
                              full_name = '::'.join(ancestor_titles + [title]) if ancestor_titles else title
                              if test_file:
                                  full_name = f"{test_file}::{full_name}"
                              
                              status = assertion.get('status', '')
                              
                              all_tests.append(full_name)
                              
                              if status == 'passed':
                                  passing_tests.append(full_name)
                              elif status in ['failed', 'error']:
                                  failing_tests.append(full_name)
                              elif status in ['skipped', 'pending', 'todo', 'disabled']:
                                  skipped_tests.append(full_name)
                  
                  # Alternative structure for some Jest configs
                  elif 'testResults' in results:
                      for suite in results.get('testResults', []):
                          if 'tests' in suite:
                              for test in suite['tests']:
                                  test_name = test.get('title', test.get('fullName', ''))
                                  if test_name:
                                      all_tests.append(test_name)
                                      
                                      if test.get('status') == 'passed':
                                          passing_tests.append(test_name)
                                          pr_passed += 1
                                      elif test.get('status') in ['failed', 'error']:
                                          failing_tests.append(test_name)
                                      elif test.get('status') in ['skipped', 'pending']:
                                          skipped_tests.append(test_name)
                                      
                                      pr_total += 1
              else:
                  print('No pr_results.json file found')

          except Exception as e:
              print(f'Error processing Jest results: {e}')
              import traceback
              traceback.print_exc()

          # Calculate percentage
          if pr_total > 0:
              pr_percentage = (pr_passed / pr_total) * 100

          # Extract warnings from test output
          try:
              if os.path.exists('pr_test_output.txt'):
                  with open('pr_test_output.txt', 'r') as f:
                      output = f.read()
                      # Look for common Jest warning patterns
                      for line in output.split('\n'):
                          if any(pattern in line.lower() for pattern in ['warning:', 'deprecated:', 'console.warn']):
                              warnings_list.append(line.strip())
          except Exception as e:
              print(f'Could not extract warnings: {e}')

          # Save test data to JSON
          test_data = {
              'passing_tests': passing_tests,
              'failing_tests': failing_tests,
              'skipped_tests': skipped_tests,
              'xfailed_tests': [],  # Jest doesn't have xfail concept
              'all_tests': all_tests,
              'skipped_tests_with_reasons': {},  # Would need additional parsing
              'xfailed_tests_with_reasons': {},
              'warnings': warnings_list
          }

          with open('pr_test_data.json', 'w') as f:
              json.dump(test_data, f, indent=2)

          print('Test data saved to pr_test_data.json')
          print(f'Results - Total: {pr_total}, Passed: {pr_passed}, Percentage: {pr_percentage:.2f}%')

          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={pr_total}\n')
              f.write(f'passed={pr_passed}\n')
              f.write(f'percentage={pr_percentage:.2f}\n')
              f.write(f'failing_count={len(failing_tests)}\n')
              f.write(f'skipped_count={len(skipped_tests)}\n')
          PY

      - name: Upload PR branch artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: jest_pr_branch_data_${{ github.event.pull_request.number || github.run_id }}
          path: |
            pr_test_data.json
            pr_results.json
            pr_test_output.txt
          retention-days: 3
          if-no-files-found: ignore

  test-target-branch:
    runs-on: ${{ inputs.runs_on }}
    defaults:
      run:
        shell: bash
        working-directory: ${{ inputs.working-directory }}
    outputs:
      total: ${{ steps.extract-results.outputs.total || steps.set-error-outputs.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed || steps.set-error-outputs.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage || steps.set-error-outputs.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}

    steps:
      - name: Checkout Target Branch
        uses: actions/checkout@v4.2.2
        with:
          ref: ${{ inputs.target_branch_to_compare }}
          submodules: "recursive"

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "${{ inputs.node-version }}"

      - name: Detect package manager and install dependencies
        id: install-deps
        run: |
          if [ -n "${{ inputs.install-command }}" ]; then
            echo "Using custom install command: ${{ inputs.install-command }}"
            ${{ inputs.install-command }}
          elif [ -f "pnpm-lock.yaml" ]; then
            echo "Detected pnpm"
            npm install -g pnpm
            pnpm install
          elif [ -f "yarn.lock" ]; then
            echo "Detected yarn"
            npm install -g yarn
            yarn install
          else
            echo "Using npm (default)"
            npm ci || npm install
          fi

      - name: Check for test discovery issues
        id: check-collection
        run: |
          echo "Running Jest --listTests to verify discovery..."
          set +e
          ${{ inputs.jest-command }} --listTests > collection_output.txt 2>&1
          LIST_EXIT_CODE=$?
          set -e

          # Set default values
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          if [ "$LIST_EXIT_CODE" -ne 0 ]; then
            echo "::warning::Test discovery command failed in target branch"
            HAS_COLLECTION_ERRORS="true"
            ERROR_TYPE="DiscoveryCommandFailed"
            ERROR_DETAILS=$(head -n 200 collection_output.txt | tr '\n' ' ' | sed 's/"/\\"/g')
          else
            echo "Jest --listTests executed successfully"

            TEST_COUNT=$(wc -l < collection_output.txt 2>/dev/null || echo "0")
            
            if [ "$TEST_COUNT" == "0" ] || grep -qi "No tests found" collection_output.txt; then
              echo "::warning::No tests were found in the target branch"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              ERROR_DETAILS="No tests were discovered by Jest --listTests"
            else
              echo "Found $TEST_COUNT test files in target branch"
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT

      - name: Run Jest tests
        id: run-tests
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Running Jest tests with JSON reporter..."
          set +e
          ${{ inputs.jest-command }} ${{ inputs.jest-extra-args }} --json --outputFile=target_results.json > target_test_output.txt 2>&1
          TEST_EXIT_CODE=$?
          set -e

          echo "Test exit code: $TEST_EXIT_CODE"
          echo "exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT

      - name: Extract results and prepare JSON
        id: extract-results
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Processing test results..."
          python3 - << 'PY'
          import json
          import os
          import sys

          # Default values
          target_total = 0
          target_passed = 0
          target_percentage = 0
          failing_tests = []
          skipped_tests = []
          passing_tests = []
          all_tests = []
          warnings_list = []

          try:
              if os.path.exists('target_results.json'):
                  print('Found target_results.json')
                  with open('target_results.json', 'r') as f:
                      results = json.load(f)
                  
                  # Jest JSON structure
                  if 'numTotalTests' in results:
                      target_total = results.get('numTotalTests', 0)
                      target_passed = results.get('numPassedTests', 0)
                      
                      # Process test results
                      for test_result in results.get('testResults', []):
                          test_file = test_result.get('name', '')
                          
                          for assertion in test_result.get('assertionResults', []):
                              ancestor_titles = assertion.get('ancestorTitles', [])
                              title = assertion.get('title', '')
                              full_name = '::'.join(ancestor_titles + [title]) if ancestor_titles else title
                              if test_file:
                                  full_name = f"{test_file}::{full_name}"
                              
                              status = assertion.get('status', '')
                              
                              all_tests.append(full_name)
                              
                              if status == 'passed':
                                  passing_tests.append(full_name)
                              elif status in ['failed', 'error']:
                                  failing_tests.append(full_name)
                              elif status in ['skipped', 'pending', 'todo', 'disabled']:
                                  skipped_tests.append(full_name)
                  
                  # Alternative structure for some Jest configs
                  elif 'testResults' in results:
                      for suite in results.get('testResults', []):
                          if 'tests' in suite:
                              for test in suite['tests']:
                                  test_name = test.get('title', test.get('fullName', ''))
                                  if test_name:
                                      all_tests.append(test_name)
                                      
                                      if test.get('status') == 'passed':
                                          passing_tests.append(test_name)
                                          target_passed += 1
                                      elif test.get('status') in ['failed', 'error']:
                                          failing_tests.append(test_name)
                                      elif test.get('status') in ['skipped', 'pending']:
                                          skipped_tests.append(test_name)
                                      
                                      target_total += 1
              else:
                  print('No target_results.json file found')

          except Exception as e:
              print(f'Error processing Jest results: {e}')
              import traceback
              traceback.print_exc()

          # Calculate percentage
          if target_total > 0:
              target_percentage = (target_passed / target_total) * 100

          # Extract warnings from test output
          try:
              if os.path.exists('target_test_output.txt'):
                  with open('target_test_output.txt', 'r') as f:
                      output = f.read()
                      # Look for common Jest warning patterns
                      for line in output.split('\n'):
                          if any(pattern in line.lower() for pattern in ['warning:', 'deprecated:', 'console.warn']):
                              warnings_list.append(line.strip())
          except Exception as e:
              print(f'Could not extract warnings: {e}')

          # Save test data to JSON
          test_data = {
              'passing_tests': passing_tests,
              'failing_tests': failing_tests,
              'skipped_tests': skipped_tests,
              'xfailed_tests': [],  # Jest doesn't have xfail concept
              'all_tests': all_tests,
              'skipped_tests_with_reasons': {},
              'xfailed_tests_with_reasons': {},
              'warnings': warnings_list
          }

          with open('target_test_data.json', 'w') as f:
              json.dump(test_data, f, indent=2)

          print('Test data saved to target_test_data.json')
          print(f'Results - Total: {target_total}, Passed: {target_passed}, Percentage: {target_percentage:.2f}%')

          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={target_total}\n')
              f.write(f'passed={target_passed}\n')
              f.write(f'percentage={target_percentage:.2f}\n')
          PY

      - name: Upload target branch artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: jest_target_branch_data_${{ github.event.pull_request.number || github.run_id }}
          path: |
            target_test_data.json
            target_results.json
            target_test_output.txt
          retention-days: 3
          if-no-files-found: ignore

      - name: Set collection error outputs
        id: set-error-outputs
        if: steps.check-collection.outputs.has_collection_errors == 'true'
        run: |
          echo "::warning::Setting default outputs for target branch due to collection errors"
          echo "total=0" >> $GITHUB_OUTPUT
          echo "passed=0" >> $GITHUB_OUTPUT
          echo "percentage=0.00" >> $GITHUB_OUTPUT

  compare-results:
    needs: [test-source-branch, test-target-branch]
    uses: ./.github/workflows/test-regression-analysis.yml
    with:
      runs_on: ${{ inputs.runs_on }}
      baseline_label: ${{ inputs.target_branch_to_compare }}
      baseline_results_artifact: jest_target_branch_data_${{ github.event.pull_request.number || github.run_id }}
      baseline_results_filename: target_test_data.json
      current_label: ${{ github.head_ref || github.ref_name || 'source branch' }}
      current_results_artifact: jest_pr_branch_data_${{ github.event.pull_request.number || github.run_id }}
      current_results_filename: pr_test_data.json
      baseline_passed: ${{ needs.test-target-branch.outputs.passed }}
      baseline_total: ${{ needs.test-target-branch.outputs.total }}
      baseline_percentage: ${{ needs.test-target-branch.outputs.percentage }}
      current_passed: ${{ needs.test-source-branch.outputs.passed }}
      current_total: ${{ needs.test-source-branch.outputs.total }}
      current_percentage: ${{ needs.test-source-branch.outputs.percentage }}
      baseline_collection_errors: ${{ needs.test-target-branch.outputs.collection_errors }}
      baseline_no_tests_found: ${{ needs.test-target-branch.outputs.no_tests_found }}
      current_collection_errors: ${{ needs.test-source-branch.outputs.collection_errors }}
      current_no_tests_found: ${{ needs.test-source-branch.outputs.no_tests_found }}
      artifact_name: regression_details_pr_${{ github.event.pull_request.number || github.run_id }}_jest

  prepare-notification:
    name: Prepare Notification Data
    needs: [test-source-branch, test-target-branch, compare-results]
    if: |
      always() &&
      (
        needs.test-source-branch.outputs.collection_errors == 'true' ||
        needs.test-source-branch.outputs.no_tests_found == 'true' ||
        needs.compare-results.outputs.has_regressions == 'true'
      )
    runs-on: ${{ inputs.runs_on }}
    outputs:
      message_body: ${{ steps.construct_notification.outputs.message_body_out }}
      ping_user_ids: ${{ steps.construct_notification.outputs.ping_user_ids_out }}
      artifact_path: ${{ steps.construct_notification.outputs.artifact_path_out }}
      should_notify: "true"
      webhook_available_for_alert: ${{ steps.check_webhook_availability.outputs.webhook_available }}

    steps:
      - name: Check for Discord Webhook URL
        id: check_webhook_availability
        run: |
          if [ -z "${{ secrets.DISCORD_WEBHOOK_URL }}" ]; then
            echo "::notice::DISCORD_WEBHOOK_URL secret is not set. Discord notifications will be skipped."
            echo "webhook_available=false" >> $GITHUB_OUTPUT
          else
            echo "webhook_available=true" >> $GITHUB_OUTPUT
          fi

      - name: Download regression details (if any)
        id: download_regressions
        if: always()
        uses: actions/download-artifact@v4
        with:
          name: regression_details_pr_${{ github.event.pull_request.number || github.run_id }}_jest
          path: .
        continue-on-error: true

      - name: Construct notification message
        id: construct_notification
        run: |
          # Initialize message components
          MESSAGE_LINES=()
          MESSAGE_LINES+=("**Jest Test Results - PR #${{ github.event.pull_request.number }}**")
          MESSAGE_LINES+=("")

          # Add regression information if available
          if [[ "${{ needs.compare-results.outputs.has_regressions }}" == "true" ]]; then
            MESSAGE_LINES+=("**:red_circle: REGRESSIONS DETECTED**")
            MESSAGE_LINES+=("- ${{ needs.compare-results.outputs.regression_count }} test regressions found")
            
            # Add breakdown if available
            if [[ "${{ needs.compare-results.outputs.pass_to_fail_count }}" -gt 0 ]]; then
              MESSAGE_LINES+=("  - Pass→Fail: ${{ needs.compare-results.outputs.pass_to_fail_count }}")
            fi
            if [[ "${{ needs.compare-results.outputs.pass_to_skip_count }}" -gt 0 ]]; then
              MESSAGE_LINES+=("  - Pass→Skip: ${{ needs.compare-results.outputs.pass_to_skip_count }}")
            fi
            if [[ "${{ needs.compare-results.outputs.pass_to_gone_count }}" -gt 0 ]]; then
              MESSAGE_LINES+=("  - Pass→Gone: ${{ needs.compare-results.outputs.pass_to_gone_count }}")
            fi
          elif [[ "${{ needs.test-source-branch.outputs.collection_errors }}" == "true" ]]; then
            MESSAGE_LINES+=("**:red_circle: TEST DISCOVERY FAILED**")
            MESSAGE_LINES+=("- Error Type: ${{ needs.test-source-branch.outputs.error_type }}")
          elif [[ "${{ needs.test-source-branch.outputs.no_tests_found }}" == "true" ]]; then
            MESSAGE_LINES+=("**:warning: NO TESTS FOUND**")
          else
            MESSAGE_LINES+=("**:white_check_mark: NO REGRESSIONS DETECTED**")
          fi

          # Add test statistics
          MESSAGE_LINES+=("")
          MESSAGE_LINES+=("**Statistics:**")
          MESSAGE_LINES+=("- PR Branch: ${{ needs.test-source-branch.outputs.passed }}/${{ needs.test-source-branch.outputs.total }} passed (${{ needs.test-source-branch.outputs.percentage }}%)")
          MESSAGE_LINES+=("- Target Branch: ${{ needs.test-target-branch.outputs.passed }}/${{ needs.test-target-branch.outputs.total }} passed (${{ needs.test-target-branch.outputs.percentage }}%)")

          MESSAGE_LINES+=("---")
          MESSAGE_LINES+=("[View Workflow Run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})")

          # Set artifact path
          ARTIFACT_PATH_OUTPUT=""
          if [ -f "comprehensive_regression_report.txt" ]; then
            ARTIFACT_PATH_OUTPUT="comprehensive_regression_report.txt"
          elif [ -f "regression_details.txt" ]; then
            ARTIFACT_PATH_OUTPUT="regression_details.txt"
          fi

          # Construct final message
          FINAL_MESSAGE_BODY=$(printf "%s\\n" "${MESSAGE_LINES[@]}")
          FINAL_MESSAGE_BODY="${FINAL_MESSAGE_BODY%\\n}"

          echo "message_body_out<<EOF" >> $GITHUB_OUTPUT
          echo "$FINAL_MESSAGE_BODY" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          echo "artifact_path_out=$ARTIFACT_PATH_OUTPUT" >> $GITHUB_OUTPUT

          # Handle ping users (simplified)
          PING_USER_IDS=""
          if [[ "${{ inputs.ping_latest_committer }}" == "true" ]]; then
            # This would need actual implementation based on your needs
            PING_USER_IDS=""
          fi
          echo "ping_user_ids_out=$PING_USER_IDS" >> $GITHUB_OUTPUT

  notify-discord:
    name: Send Discord Notification
    needs: [prepare-notification]
    if: |
      always() &&
      needs.prepare-notification.outputs.should_notify == 'true' &&
      needs.prepare-notification.outputs.webhook_available_for_alert == 'true'
    uses: ./.github/workflows/discord-alert.yml
    with:
      message_body: ${{ needs.prepare-notification.outputs.message_body }}
      ping_user_ids: ${{ needs.prepare-notification.outputs.ping_user_ids }}
      artifact_paths: ${{ needs.prepare-notification.outputs.artifact_path }}
      should_notify: ${{ needs.prepare-notification.outputs.should_notify }}
      runs_on: ${{ inputs.runs_on }}
    secrets:
      DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      DISCORD_USER_MAP: ${{ secrets.DISCORD_USER_MAP }}
