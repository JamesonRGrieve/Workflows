name: Reusable Compare Pytest Results

on:
  workflow_call:
    inputs:
      target_branch_to_compare:
        description: "The target branch to compare against (e.g., main, refs/heads/main)."
        required: true
        type: string
      python-version:
        description: "Python version to use for testing."
        required: false
        type: string
        default: "3.10"
      working-directory:
        description: "Directory where pytest commands should be executed."
        required: false
        type: string
        default: "."
      ping_latest_committer:
        description: "If true, the latest committer on the PR will be added to the ping list."
        required: false
        type: boolean
        default: false
      runs_on:
        required: false
        type: string
        default: "self-hosted"
    secrets:
      DISCORD_WEBHOOK_URL:
        description: "Discord Webhook URL for failure notifications. If not provided, notifications are skipped."
        required: false
      DISCORD_USER_MAP:
        description: 'JSON string mapping GitHub usernames to Discord User IDs (e.g., {"user1":"id1"}). If not provided, users won''t be pinged.'
        required: false
    outputs:
      pr_total:
        description: "Total tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.total }}
      pr_passed:
        description: "Passed tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.passed }}
      pr_percentage:
        description: "Pass percentage in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.percentage }}
      pr_collection_errors:
        description: "PR branch has collection errors"
        value: ${{ jobs.test-source-branch.outputs.collection_errors }}
      pr_no_tests_found:
        description: "PR branch has no tests found"
        value: ${{ jobs.test-source-branch.outputs.no_tests_found }}
      target_total:
        description: "Total tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.total }}
      target_passed:
        description: "Passed tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.passed }}
      target_percentage:
        description: "Pass percentage in target branch"
        value: ${{ jobs.test-target-branch.outputs.percentage }}
      has_regressions:
        description: "Boolean indicating if regressions were found"
        value: ${{ jobs.compare-results.outputs.has_regressions }}
      regression_count:
        description: "Number of test regressions found"
        value: ${{ jobs.compare-results.outputs.regression_count }}

jobs:
  test-source-branch:
    runs-on: ${{ inputs.runs_on }}
    defaults:
      run:
        shell: bash
        working-directory: ${{ inputs.working-directory }}
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}
      failing_count: ${{ steps.extract-results.outputs.failing_count }}
      skipped_count: ${{ steps.extract-results.outputs.skipped_count }}
      xfailed_count: ${{ steps.extract-results.outputs.xfailed_count }}

    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-json-report pytest-asyncio
          if [ -f requirements.txt ]; then 
            pip install -r requirements.txt
          fi

      - name: Check for test collection errors
        id: check-collection
        run: |
          echo "Running pytest collection check..."
          python -m pytest --collect-only -v > collection_output.txt 2>&1 || true

          # Set default values
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          if grep -q "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt; then
            echo "::error::Test discovery errors detected in PR branch - Python modules could not be imported correctly"
            
            # Attempt to identify specific error type
            if grep -q "ImportError" collection_output.txt; then
              ERROR_TYPE="ImportError"
            elif grep -q "ModuleNotFoundError" collection_output.txt; then
              ERROR_TYPE="ModuleNotFoundError"
            elif grep -q "SyntaxError" collection_output.txt; then
              ERROR_TYPE="SyntaxError"
            elif grep -q "ERROR collecting" collection_output.txt; then
              ERROR_TYPE="CollectionError"
            elif grep -q "Interrupted:" collection_output.txt; then
              ERROR_TYPE="Interrupted"
            else 
              ERROR_TYPE="UnknownError"
            fi
            
            echo "PR branch discovery error type: $ERROR_TYPE"
            
            ERROR_FILE=$(grep -o "ERROR collecting.*\.py" collection_output.txt | grep -o "[a-zA-Z0-9_/]*\.py" || echo "Unknown file")
            
            if [[ "$ERROR_FILE" != "Unknown file" ]]; then
              echo "Error in file $ERROR_FILE"
              grep -A 15 "$ERROR_FILE" collection_output.txt > error_details.txt
              ERROR_DETAILS=$(cat error_details.txt | tr '\n' ' ' | sed 's/"/\\"/g')
            else
              grep -A 15 "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt | head -20 > error_details.txt
              ERROR_DETAILS=$(cat error_details.txt | tr '\n' ' ' | sed 's/"/\\"/g')
            fi
            
            echo "::error::PR branch discovery error details: ${ERROR_DETAILS:0:200}..."
            HAS_COLLECTION_ERRORS="true"
          else
            echo "No discovery errors detected in PR branch"
            
            TEST_COUNT=$(grep -o "collected [0-9]* item" collection_output.txt | grep -o "[0-9]*" || echo "0")
            
            if [[ "$TEST_COUNT" == "0" ]]; then
              echo "::warning::No tests were found in the PR branch"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              ERROR_DETAILS="No test files were discovered that match pytest's test discovery pattern"
            else  
              echo "Found $TEST_COUNT tests in PR branch"
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT

      - name: Run pytest
        id: run-tests
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Running pytest with JSON reporter..."
          set +e
          python -m pytest --json-report --json-report-file=pr_results.json > pr_test_output.txt 2>&1
          TEST_EXIT_CODE=$?
          set -e

          echo "Test exit code: $TEST_EXIT_CODE"
          echo "exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT

      - name: Extract results and prepare JSON
        id: extract-results
        if: always()
        run: |
          echo "Processing test results..."
          python3 - << 'PY'
          import json
          import os

          # Default values
          pr_total = 0
          pr_passed = 0
          pr_percentage = 0
          failing_tests = []
          skipped_tests = []
          xfailed_tests = []
          all_tests = []
          passing_tests = []
          skipped_tests_with_reasons = {}
          xfailed_tests_with_reasons = {}
          warnings_list = []

          try:
              print('Attempting to open pr_results.json')
              with open('pr_results.json') as f:
                  pr_results = json.load(f)
                  print(f'JSON loaded successfully, keys: {list(pr_results.keys())}')
              
              # Check for collection errors by looking at exitcode or error patterns
              if pr_results.get('exitcode', 0) > 1:
                  print('Detected non-zero exitcode, likely a collection error')
                  if 'collectors' in pr_results and pr_results['collectors']:
                      print(f'Collection errors found: {pr_results["collectors"]}')
                  pr_total = 0  # Explicitly set to 0 - no tests run when collection fails
                  pr_passed = 0
              elif 'summary' in pr_results and isinstance(pr_results['summary'], dict):
                  # Normal case - extract data from summary
                  summary = pr_results['summary']
                  pr_total = summary.get('total', 0)
                  pr_passed = summary.get('passed', 0)
                  print(f'Results extracted from summary - Total: {pr_total}, Passed: {pr_passed}')
                  
                  # Extract all tests by outcome and collect all test nodeids with reasons
                  if 'tests' in pr_results:
                      print('Extracting failing, skipped, xfailed, and all tests with reasons')
                      for test in pr_results['tests']:
                          outcome = test.get('outcome')
                          nodeid = test.get('nodeid', '')
                          if nodeid:
                              all_tests.append(nodeid)  # Track all tests regardless of outcome
                              if outcome == 'passed':
                                  passing_tests.append(nodeid)
                              elif outcome in ['failed', 'error']:
                                  failing_tests.append(nodeid)
                              elif outcome == 'skipped':
                                  skipped_tests.append(nodeid)
                                  # Extract skip reason
                                  skip_reason = 'No reason provided'
                                  if 'longrepr' in test and test['longrepr']:
                                      # longrepr can be a string or list, handle both
                                      longrepr = test['longrepr']
                                      if isinstance(longrepr, list) and longrepr:
                                          skip_reason = str(longrepr[0]) if longrepr[0] else 'No reason provided'
                                      elif isinstance(longrepr, str):
                                          skip_reason = longrepr
                                  elif 'call' in test and test['call'] and 'longrepr' in test['call']:
                                      skip_reason = str(test['call']['longrepr'])
                                  skipped_tests_with_reasons[nodeid] = skip_reason.strip()
                              elif outcome == 'xfailed':
                                  xfailed_tests.append(nodeid)
                                  # Extract xfail reason
                                  xfail_reason = 'No reason provided'
                                  if 'longrepr' in test and test['longrepr']:
                                      longrepr = test['longrepr']
                                      if isinstance(longrepr, list) and longrepr:
                                          xfail_reason = str(longrepr[0]) if longrepr[0] else 'No reason provided'
                                      elif isinstance(longrepr, str):
                                          xfail_reason = longrepr
                                  elif 'call' in test and test['call'] and 'longrepr' in test['call']:
                                      xfail_reason = str(test['call']['longrepr'])
                                  xfailed_tests_with_reasons[nodeid] = xfail_reason.strip()
                      
                      print(f'Found {len(passing_tests)} passing tests')
                      print(f'Found {len(failing_tests)} failing tests')
                      print(f'Found {len(skipped_tests)} skipped tests')
                      print(f'Found {len(xfailed_tests)} xfailed tests')
                      print(f'Found {len(all_tests)} total discovered tests')
              else:
                  print('Results JSON missing expected fields, using defaults')

          except FileNotFoundError:
              print('pr_results.json not found, using defaults')
          except Exception as e:
              print(f'Error processing results: {e}')
              import traceback
              traceback.print_exc()

          # Calculate percentage
          if pr_total > 0:
              pr_percentage = (pr_passed / pr_total) * 100

          # Extract warnings from test output
          try:
              if os.path.exists('pr_test_output.txt'):
                  with open('pr_test_output.txt', 'r') as f:
                      output = f.read()
                      # Look for warnings section
                      if 'warnings summary' in output.lower():
                          warnings_section = output[output.lower().find('warnings summary'):]
                          warnings_list = warnings_section.split('\n')[1:20]  # Get first 20 warning lines
          except Exception as e:
              print(f'Could not extract warnings: {e}')

          # Save test data to JSON
          test_data = {
              'passing_tests': passing_tests,
              'failing_tests': failing_tests,
              'skipped_tests': skipped_tests,
              'xfailed_tests': xfailed_tests,
              'all_tests': all_tests,
              'skipped_tests_with_reasons': skipped_tests_with_reasons,
              'xfailed_tests_with_reasons': xfailed_tests_with_reasons,
              'warnings': warnings_list
          }

          with open('pr_test_data.json', 'w') as f:
              json.dump(test_data, f, indent=2)

          print('Test data saved to pr_test_data.json')
          print(f'Results - Total: {pr_total}, Passed: {pr_passed}, Percentage: {pr_percentage:.2f}%')

          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={pr_total}\n')
              f.write(f'passed={pr_passed}\n')
              f.write(f'percentage={pr_percentage:.2f}\n')
              f.write(f'failing_count={len(failing_tests)}\n')
              f.write(f'skipped_count={len(skipped_tests)}\n')
              f.write(f'xfailed_count={len(xfailed_tests)}\n')
          PY

      - name: Upload PR branch artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pytest_pr_branch_data_${{ github.event.pull_request.number || github.run_id }}
          path: |
            pr_test_data.json
            pr_results.json
            pr_test_output.txt
          retention-days: 3
          if-no-files-found: ignore

  test-target-branch:
    runs-on: ${{ inputs.runs_on }}
    defaults:
      run:
        shell: bash
        working-directory: ${{ inputs.working-directory }}
    outputs:
      total: ${{ steps.extract-results.outputs.total || steps.set-error-outputs.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed || steps.set-error-outputs.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage || steps.set-error-outputs.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}

    steps:
      - name: Checkout Target Branch
        uses: actions/checkout@v4.2.2
        with:
          ref: ${{ inputs.target_branch_to_compare }}
          submodules: "recursive"

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-json-report pytest-asyncio
          if [ -f requirements.txt ]; then 
            pip install -r requirements.txt
          fi

      - name: Check for test collection errors
        id: check-collection
        run: |
          echo "Running pytest collection check..."
          python -m pytest --collect-only -v > collection_output.txt 2>&1 || true

          # Set default values
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          if grep -q "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt; then
            echo "::warning::Test discovery errors detected in target branch"
            HAS_COLLECTION_ERRORS="true"
            ERROR_TYPE=$(grep -o "ImportError\|ModuleNotFoundError\|SyntaxError\|CollectionError\|Interrupted" collection_output.txt | head -1)
            ERROR_DETAILS=$(head -n 200 collection_output.txt | tr '\n' ' ' | sed 's/"/\\"/g')
          else
            echo "No discovery errors detected in target branch"
            
            TEST_COUNT=$(grep -o "collected [0-9]* item" collection_output.txt | grep -o "[0-9]*" || echo "0")
            
            if [[ "$TEST_COUNT" == "0" ]]; then
              echo "::warning::No tests were found in the target branch"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              ERROR_DETAILS="No test files were discovered"
            else  
              echo "Found $TEST_COUNT tests in target branch"
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT

      - name: Run pytest
        id: run-tests
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Running pytest with JSON reporter..."
          set +e
          python -m pytest --json-report --json-report-file=target_results.json > target_test_output.txt 2>&1
          TEST_EXIT_CODE=$?
          set -e

          echo "Test exit code: $TEST_EXIT_CODE"
          echo "exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT

      - name: Extract results and prepare JSON
        id: extract-results
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Processing test results..."
          python3 - << 'PY'
          import json
          import os

          # Default values
          target_total = 0
          target_passed = 0
          target_percentage = 0
          failing_tests = []
          skipped_tests = []
          xfailed_tests = []
          all_tests = []
          passing_tests = []
          skipped_tests_with_reasons = {}
          xfailed_tests_with_reasons = {}
          warnings_list = []

          try:
              print('Attempting to open target_results.json')
              with open('target_results.json') as f:
                  target_results = json.load(f)
                  print(f'JSON loaded successfully')
              
              if 'summary' in target_results and isinstance(target_results['summary'], dict):
                  summary = target_results['summary']
                  target_total = summary.get('total', 0)
                  target_passed = summary.get('passed', 0)
                  
                  if 'tests' in target_results:
                      for test in target_results['tests']:
                          outcome = test.get('outcome')
                          nodeid = test.get('nodeid', '')
                          if nodeid:
                              all_tests.append(nodeid)
                              if outcome == 'passed':
                                  passing_tests.append(nodeid)
                              elif outcome in ['failed', 'error']:
                                  failing_tests.append(nodeid)
                              elif outcome == 'skipped':
                                  skipped_tests.append(nodeid)
                              elif outcome == 'xfailed':
                                  xfailed_tests.append(nodeid)

          except FileNotFoundError:
              print('target_results.json not found, using defaults')
          except Exception as e:
              print(f'Error processing results: {e}')

          # Calculate percentage
          if target_total > 0:
              target_percentage = (target_passed / target_total) * 100

          # Save test data to JSON
          test_data = {
              'passing_tests': passing_tests,
              'failing_tests': failing_tests,
              'skipped_tests': skipped_tests,
              'xfailed_tests': xfailed_tests,
              'all_tests': all_tests,
              'skipped_tests_with_reasons': skipped_tests_with_reasons,
              'xfailed_tests_with_reasons': xfailed_tests_with_reasons,
              'warnings': warnings_list
          }

          with open('target_test_data.json', 'w') as f:
              json.dump(test_data, f, indent=2)

          print(f'Results - Total: {target_total}, Passed: {target_passed}, Percentage: {target_percentage:.2f}%')

          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={target_total}\n')
              f.write(f'passed={target_passed}\n')
              f.write(f'percentage={target_percentage:.2f}\n')
          PY

      - name: Upload target branch artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pytest_target_branch_data_${{ github.event.pull_request.number || github.run_id }}
          path: |
            target_test_data.json
            target_results.json
            target_test_output.txt
          retention-days: 3
          if-no-files-found: ignore

      - name: Set collection error outputs
        id: set-error-outputs
        if: steps.check-collection.outputs.has_collection_errors == 'true'
        run: |
          echo "::warning::Setting default outputs for target branch due to collection errors"
          echo "total=0" >> $GITHUB_OUTPUT
          echo "passed=0" >> $GITHUB_OUTPUT
          echo "percentage=0.00" >> $GITHUB_OUTPUT

  compare-results:
    needs: [test-source-branch, test-target-branch]
    uses: ./.github/workflows/test-regression-analysis.yml
    with:
      runs_on: ${{ inputs.runs_on }}
      baseline_label: ${{ inputs.target_branch_to_compare }}
      baseline_results_artifact: pytest_target_branch_data_${{ github.event.pull_request.number || github.run_id }}
      baseline_results_filename: target_test_data.json
      current_label: ${{ github.head_ref || github.ref_name || 'source branch' }}
      current_results_artifact: pytest_pr_branch_data_${{ github.event.pull_request.number || github.run_id }}
      current_results_filename: pr_test_data.json
      baseline_passed: ${{ needs.test-target-branch.outputs.passed }}
      baseline_total: ${{ needs.test-target-branch.outputs.total }}
      baseline_percentage: ${{ needs.test-target-branch.outputs.percentage }}
      current_passed: ${{ needs.test-source-branch.outputs.passed }}
      current_total: ${{ needs.test-source-branch.outputs.total }}
      current_percentage: ${{ needs.test-source-branch.outputs.percentage }}
      baseline_collection_errors: ${{ needs.test-target-branch.outputs.collection_errors }}
      baseline_no_tests_found: ${{ needs.test-target-branch.outputs.no_tests_found }}
      current_collection_errors: ${{ needs.test-source-branch.outputs.collection_errors }}
      current_no_tests_found: ${{ needs.test-source-branch.outputs.no_tests_found }}
      artifact_name: regression_details_pytest_${{ github.event.pull_request.number || github.run_id }}
