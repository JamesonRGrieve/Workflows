name: Reusable Pytest Runner

on:
  workflow_call:
    inputs:
      ref:
        description: "Git ref to checkout and test. Leave empty for default checkout."
        required: false
        type: string
        default: ""
      python-version:
        description: "Python version to use for testing."
        required: false
        type: string
        default: "3.10"
      runs_on:
        description: "Runner label for the test job."
        required: false
        type: string
        default: "self-hosted"
      artifact_name:
        description: "Name for the test results artifact."
        required: true
        type: string
      parallel_workers:
        description: "Number of parallel workers for pytest-xdist. Use 'auto' for CPU count, or a number. Set to '1' to disable."
        required: false
        type: string
        default: "auto"
    outputs:
      total:
        description: "Total number of tests"
        value: ${{ jobs.test.outputs.total }}
      passed:
        description: "Number of passing tests"
        value: ${{ jobs.test.outputs.passed }}
      percentage:
        description: "Pass percentage"
        value: ${{ jobs.test.outputs.percentage }}
      collection_errors:
        description: "Whether collection errors occurred"
        value: ${{ jobs.test.outputs.collection_errors }}
      no_tests_found:
        description: "Whether no tests were found"
        value: ${{ jobs.test.outputs.no_tests_found }}
      has_errors:
        description: "Whether any errors occurred"
        value: ${{ jobs.test.outputs.has_errors }}
      error_type:
        description: "Type of error if any"
        value: ${{ jobs.test.outputs.error_type }}
      failing_count:
        description: "Number of failing tests"
        value: ${{ jobs.test.outputs.failing_count }}
      skipped_count:
        description: "Number of skipped tests"
        value: ${{ jobs.test.outputs.skipped_count }}
      xfailed_count:
        description: "Number of xfailed tests"
        value: ${{ jobs.test.outputs.xfailed_count }}

jobs:
  test:
    runs-on: ${{ inputs.runs_on }}
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      failing_count: ${{ steps.extract-results.outputs.failing_count }}
      skipped_count: ${{ steps.extract-results.outputs.skipped_count }}
      xfailed_count: ${{ steps.extract-results.outputs.xfailed_count }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"
          ref: ${{ inputs.ref || github.ref }}

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-json-report pytest-asyncio pytest-xdist
          PYPROJECT=$(find . -name "pyproject.toml" -type f | head -n 1)
          if [ -n "$PYPROJECT" ]; then
            pip install -e "$(dirname "$PYPROJECT")[dev]"
          elif [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

      - name: Check for test collection errors
        id: check-collection
        run: |
          echo "Running pytest collection check..."
          python -m pytest --collect-only -v > collection_output.txt 2>&1 || true

          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"

          if grep -q "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt; then
            echo "::error::Test discovery errors detected"
            if grep -q "ImportError" collection_output.txt; then
              ERROR_TYPE="ImportError"
            elif grep -q "ModuleNotFoundError" collection_output.txt; then
              ERROR_TYPE="ModuleNotFoundError"
            elif grep -q "SyntaxError" collection_output.txt; then
              ERROR_TYPE="SyntaxError"
            elif grep -q "ERROR collecting" collection_output.txt; then
              ERROR_TYPE="CollectionError"
            elif grep -q "Interrupted:" collection_output.txt; then
              ERROR_TYPE="Interrupted"
            else
              ERROR_TYPE="UnknownError"
            fi
            HAS_COLLECTION_ERRORS="true"
          else
            TEST_COUNT=$(grep -o "collected [0-9]* item" collection_output.txt | grep -o "[0-9]*" || echo "0")
            if [[ "$TEST_COUNT" == "0" ]]; then
              echo "::warning::No tests were found"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
            else
              echo "Found $TEST_COUNT tests"
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          if [[ "$HAS_COLLECTION_ERRORS" == "true" || "$NO_TESTS_FOUND" == "true" ]]; then
            echo "has_errors=true" >> $GITHUB_OUTPUT
          else
            echo "has_errors=false" >> $GITHUB_OUTPUT
          fi

      - name: Run tests
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Running tests with ${{ inputs.parallel_workers }} workers..."
          PARALLEL_FLAG=""
          if [ "${{ inputs.parallel_workers }}" != "1" ]; then
            PARALLEL_FLAG="-n ${{ inputs.parallel_workers }}"
          fi
          python -m pytest -vv $PARALLEL_FLAG --json-report --json-report-file=results.json --tb=short > test_output.txt 2>&1 || true
          if [ -f results.json ]; then
            echo "✅ Test execution completed"
          else
            echo "❌ Test execution failed"
          fi

      - name: Extract test results
        id: extract-results
        run: |
          python3 -c "
          import json
          import os

          total = passed = 0
          percentage = 0.0
          passing_tests = []
          failing_tests = []
          skipped_tests = []
          xfailed_tests = []
          all_tests = []
          skipped_with_reasons = {}
          xfailed_with_reasons = {}
          warnings_list = []

          try:
              with open('results.json') as f:
                  results = json.load(f)

              if results.get('exitcode', 0) <= 1 and 'summary' in results:
                  summary = results['summary']
                  total = summary.get('total', 0)
                  passed = summary.get('passed', 0)

                  for test in results.get('tests', []):
                      outcome = test.get('outcome')
                      nodeid = test.get('nodeid', '')
                      if not nodeid:
                          continue
                      all_tests.append(nodeid)
                      if outcome == 'passed':
                          passing_tests.append(nodeid)
                      elif outcome in ['failed', 'error']:
                          failing_tests.append(nodeid)
                      elif outcome == 'skipped':
                          skipped_tests.append(nodeid)
                          reason = test.get('longrepr', 'No reason')
                          if isinstance(reason, list):
                              reason = reason[0] if reason else 'No reason'
                          skipped_with_reasons[nodeid] = str(reason).strip()
                      elif outcome == 'xfailed':
                          xfailed_tests.append(nodeid)
                          reason = test.get('longrepr', 'No reason')
                          if isinstance(reason, list):
                              reason = reason[0] if reason else 'No reason'
                          xfailed_with_reasons[nodeid] = str(reason).strip()

              percentage = (passed / total * 100) if total > 0 else 0
          except FileNotFoundError:
              print('Results file not found')
          except Exception as e:
              print(f'Error: {e}')

          # Extract warnings
          try:
              with open('test_output.txt') as f:
                  content = f.read()
              if 'warnings summary' in content:
                  section = content.split('warnings summary')[1].split('-- Docs:')[0] if '-- Docs:' in content else content.split('warnings summary')[1]
                  current = []
                  for line in section.split('\n'):
                      line = line.rstrip()
                      if not line or line.startswith('='):
                          continue
                      if not line.startswith(' ') and ('.py:' in line or 'warning' in line.lower()):
                          if current:
                              warnings_list.append('\n'.join(current))
                          current = [line]
                      elif line.startswith(' ') and current:
                          current.append(line)
                  if current:
                      warnings_list.append('\n'.join(current))
          except:
              pass

          # Save artifact data
          with open('test_data.json', 'w') as f:
              json.dump({
                  'passing_tests': passing_tests,
                  'failing_tests': failing_tests,
                  'skipped_tests': skipped_tests,
                  'xfailed_tests': xfailed_tests,
                  'all_tests': all_tests,
                  'skipped_tests_with_reasons': skipped_with_reasons,
                  'xfailed_tests_with_reasons': xfailed_with_reasons,
                  'warnings': warnings_list
              }, f, indent=2)

          print(f'Results: {passed}/{total} ({percentage:.1f}%)')

          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={total}\n')
              f.write(f'passed={passed}\n')
              f.write(f'percentage={percentage:.2f}\n')
              f.write(f'failing_count={len(failing_tests)}\n')
              f.write(f'skipped_count={len(skipped_tests)}\n')
              f.write(f'xfailed_count={len(xfailed_tests)}\n')
          "

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact_name }}
          path: |
            test_data.json
            test_output.txt
            results.json
          retention-days: 3
          if-no-files-found: ignore
