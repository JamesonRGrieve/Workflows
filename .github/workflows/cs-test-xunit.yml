name: Reusable Compare xUnit Test Results

on:
  workflow_call:
    inputs:
      target_branch_to_compare:
        description: "The target branch to compare against (e.g., main, refs/heads/main)."
        required: true
        type: string
      dotnet-version:
        description: ".NET SDK version to use when running xUnit tests."
        required: false
        type: string
        default: "8.0.x"
      python-version:
        description: "Python version to use for result processing scripts."
        required: false
        type: string
        default: "3.10"
      working-directory:
        description: "Directory where xUnit (dotnet test) commands should be executed."
        required: false
        type: string
        default: "."
      ping_latest_committer:
        description: "If true, the latest committer on the PR will be added to the ping list."
        required: false
        type: boolean
        default: false
      runs_on:
        required: false
        type: string
        default: "self-hosted"
    secrets:
      DISCORD_WEBHOOK_URL:
        description: "Discord Webhook URL for failure notifications. If not provided, notifications are skipped."
        required: false
      DISCORD_USER_MAP:
        description: 'JSON string mapping GitHub usernames to Discord User IDs (e.g., {"user1":"id1"}). If not provided, users won''t be pinged.'
        required: false
    outputs:
      pr_total:
        description: "Total tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.total }}
      pr_passed:
        description: "Passed tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.passed }}
      pr_percentage:
        description: "Pass percentage in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.percentage }}
      pr_collection_errors:
        description: "PR branch has collection errors"
        value: ${{ jobs.test-source-branch.outputs.collection_errors }}
      pr_no_tests_found:
        description: "PR branch has no tests found"
        value: ${{ jobs.test-source-branch.outputs.no_tests_found }}
      target_total:
        description: "Total tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.total }}
      target_passed:
        description: "Passed tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.passed }}
      target_percentage:
        description: "Pass percentage in target branch"
        value: ${{ jobs.test-target-branch.outputs.percentage }}
      has_regressions:
        description: "Boolean indicating if regressions were found"
        value: ${{ jobs.compare-results.outputs.has_regressions }}
      regression_count:
        description: "Number of test regressions found"
        value: ${{ jobs.compare-results.outputs.regression_count }}

jobs:
  test-source-branch:
    runs-on: ${{ inputs.runs_on }}
    defaults:
      run:
        shell: bash
        working-directory: ${{ inputs.working-directory }}
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}
      failing_count: ${{ steps.extract-results.outputs.failing_count }}
      skipped_count: ${{ steps.extract-results.outputs.skipped_count }}

    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"

      - name: Set up .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: "${{ inputs.dotnet-version }}"

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Restore dependencies
        run: |
          echo "Restoring dotnet dependencies..."
          dotnet restore

      - name: Check for test discovery issues
        id: check-collection
        run: |
          echo "Running dotnet test --list-tests to verify discovery..."
          set +e
          dotnet test --list-tests > collection_output.txt 2>&1
          LIST_EXIT_CODE=$?
          set -e

          # Set default values
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          if [ "$LIST_EXIT_CODE" -ne 0 ]; then
            echo "::error::Test discovery command failed in PR branch"
            HAS_COLLECTION_ERRORS="true"
            ERROR_TYPE="DiscoveryCommandFailed"
            ERROR_DETAILS=$(head -n 200 collection_output.txt | tr '\n' ' ' | sed 's/"/\\"/g')
          else
            echo "dotnet test --list-tests executed successfully"

            if grep -qi "Test Run Aborted" collection_output.txt || grep -qi "Test host process exited unexpectedly" collection_output.txt; then
              echo "::error::Discovery encountered runtime errors"
              HAS_COLLECTION_ERRORS="true"
              ERROR_TYPE="RuntimeDiscoveryError"
              ERROR_DETAILS=$(head -n 200 collection_output.txt | tr '\n' ' ' | sed 's/"/\\"/g')
            else
              TEST_COUNT=$(grep -Eo "Total tests: [0-9]+" collection_output.txt | tail -1 | awk '{print $3}')

              if [[ -z "$TEST_COUNT" ]]; then
                # Attempt to count listed tests as fallback
                TEST_COUNT=$(grep -c "^    " collection_output.txt || echo "0")
              fi

              if grep -qi "No test is available" collection_output.txt || [[ -z "$TEST_COUNT" || "$TEST_COUNT" == "0" ]]; then
                echo "::warning::No tests were found in the PR branch"
                NO_TESTS_FOUND="true"
                ERROR_TYPE="NoTestsFound"
                ERROR_DETAILS="No tests were discovered by dotnet test --list-tests"
              else
                echo "Found $TEST_COUNT tests in PR branch"
              fi
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT

      - name: Run xUnit tests
        id: run-tests
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Running xUnit tests with TRX logger..."
          set +e
          dotnet test --logger:"trx;LogFileName=test_results.trx" --results-directory ./test_results > pr_test_output.txt 2>&1
          TEST_EXIT_CODE=$?
          set -e

          echo "Test exit code: $TEST_EXIT_CODE"
          echo "exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT

          # Save output for debugging
          cp pr_test_output.txt test_results/ || true

      - name: Convert TRX to JSON and extract results
        id: extract-results
        if: always()
        run: |
          echo "Processing test results..."
          python3 - << 'PY'
          import json
          import xml.etree.ElementTree as ET
          import os
          import sys
          from pathlib import Path

          # Default values
          pr_total = 0
          pr_passed = 0
          pr_percentage = 0
          failing_tests = []
          skipped_tests = []
          passing_tests = []
          all_tests = []
          warnings_list = []

          try:
              # Find TRX file
              trx_file = None
              if os.path.exists('test_results/test_results.trx'):
                  trx_file = 'test_results/test_results.trx'
              else:
                  # Search for any TRX file
                  for file in Path('test_results').glob('*.trx'):
                      trx_file = str(file)
                      break

              if trx_file and os.path.exists(trx_file):
                  print(f'Found TRX file: {trx_file}')
                  tree = ET.parse(trx_file)
                  root = tree.getroot()
                  
                  # Define namespaces
                  ns = {'tr': 'http://microsoft.com/schemas/VisualStudio/TeamTest/2010'}
                  
                  # Get test results
                  results = root.find('.//tr:Results', ns)
                  
                  if results is not None:
                      for result in results.findall('tr:UnitTestResult', ns):
                          test_name = result.get('testName', '')
                          outcome = result.get('outcome', '')
                          
                          # Get full test name including namespace
                          test_id = result.get('testId', '')
                          test_def = root.find(f'.//tr:UnitTest[@id="{test_id}"]', ns)
                          if test_def is not None:
                              test_method = test_def.find('.//tr:TestMethod', ns)
                              if test_method is not None:
                                  class_name = test_method.get('className', '')
                                  if class_name:
                                      test_name = f"{class_name}.{test_name}"
                          
                          if test_name:
                              all_tests.append(test_name)
                              
                              if outcome == 'Passed':
                                  passing_tests.append(test_name)
                                  pr_passed += 1
                              elif outcome in ['Failed', 'Error']:
                                  failing_tests.append(test_name)
                              elif outcome in ['NotExecuted', 'Ignored', 'Skipped']:
                                  skipped_tests.append(test_name)
                              
                              pr_total += 1
                  
                  # Get summary from counters element if available
                  counters = root.find('.//tr:Counters', ns)
                  if counters is not None:
                      pr_total = int(counters.get('total', pr_total))
                      pr_passed = int(counters.get('passed', pr_passed))
                      
              else:
                  print('No TRX file found, using defaults')

          except Exception as e:
              print(f'Error processing TRX: {e}')
              import traceback
              traceback.print_exc()

          # Calculate percentage
          if pr_total > 0:
              pr_percentage = (pr_passed / pr_total) * 100

          # Extract warnings from test output
          try:
              if os.path.exists('pr_test_output.txt'):
                  with open('pr_test_output.txt', 'r') as f:
                      output = f.read()
                      # Look for warning patterns in dotnet test output
                      if 'warning' in output.lower():
                          for line in output.split('\n'):
                              if 'warning' in line.lower():
                                  warnings_list.append(line.strip())
          except Exception as e:
              print(f'Could not extract warnings: {e}')

          # Save test data to JSON
          test_data = {
              'passing_tests': passing_tests,
              'failing_tests': failing_tests,
              'skipped_tests': skipped_tests,
              'xfailed_tests': [],  # xUnit doesn't have xfail concept like pytest
              'all_tests': all_tests,
              'skipped_tests_with_reasons': {},  # Would need additional parsing for reasons
              'xfailed_tests_with_reasons': {},
              'warnings': warnings_list
          }

          with open('pr_test_data.json', 'w') as f:
              json.dump(test_data, f, indent=2)

          print('Test data saved to pr_test_data.json')
          print(f'Results - Total: {pr_total}, Passed: {pr_passed}, Percentage: {pr_percentage:.2f}%')

          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={pr_total}\n')
              f.write(f'passed={pr_passed}\n')
              f.write(f'percentage={pr_percentage:.2f}\n')
              f.write(f'failing_count={len(failing_tests)}\n')
              f.write(f'skipped_count={len(skipped_tests)}\n')
          PY

      - name: Upload PR branch artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pr_branch_data_${{ github.event.pull_request.number || github.run_id }}
          path: |
            pr_test_data.json
            pr_test_output.txt
            test_results/
          retention-days: 3
          if-no-files-found: ignore

  test-target-branch:
    runs-on: ${{ inputs.runs_on }}
    defaults:
      run:
        shell: bash
        working-directory: ${{ inputs.working-directory }}
    outputs:
      total: ${{ steps.extract-results.outputs.total || steps.set-error-outputs.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed || steps.set-error-outputs.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage || steps.set-error-outputs.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}

    steps:
      - name: Checkout Target Branch
        uses: actions/checkout@v4.2.2
        with:
          ref: ${{ inputs.target_branch_to_compare }}
          submodules: "recursive"

      - name: Set up .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: "${{ inputs.dotnet-version }}"

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Restore dependencies
        run: |
          echo "Restoring dotnet dependencies..."
          dotnet restore

      - name: Check for test discovery issues
        id: check-collection
        run: |
          echo "Running dotnet test --list-tests to verify discovery..."
          set +e
          dotnet test --list-tests > collection_output.txt 2>&1
          LIST_EXIT_CODE=$?
          set -e

          # Set default values
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          if [ "$LIST_EXIT_CODE" -ne 0 ]; then
            echo "::warning::Test discovery command failed in target branch"
            HAS_COLLECTION_ERRORS="true"
            ERROR_TYPE="DiscoveryCommandFailed"
            ERROR_DETAILS=$(head -n 200 collection_output.txt | tr '\n' ' ' | sed 's/"/\\"/g')
          else
            echo "dotnet test --list-tests executed successfully"

            if grep -qi "Test Run Aborted" collection_output.txt || grep -qi "Test host process exited unexpectedly" collection_output.txt; then
              echo "::warning::Discovery encountered runtime errors in target branch"
              HAS_COLLECTION_ERRORS="true"
              ERROR_TYPE="RuntimeDiscoveryError"
              ERROR_DETAILS=$(head -n 200 collection_output.txt | tr '\n' ' ' | sed 's/"/\\"/g')
            else
              TEST_COUNT=$(grep -Eo "Total tests: [0-9]+" collection_output.txt | tail -1 | awk '{print $3}')

              if [[ -z "$TEST_COUNT" ]]; then
                TEST_COUNT=$(grep -c "^    " collection_output.txt || echo "0")
              fi

              if grep -qi "No test is available" collection_output.txt || [[ -z "$TEST_COUNT" || "$TEST_COUNT" == "0" ]]; then
                echo "::warning::No tests were found in the target branch"
                NO_TESTS_FOUND="true"
                ERROR_TYPE="NoTestsFound"
                ERROR_DETAILS="No tests were discovered by dotnet test --list-tests"
              else
                echo "Found $TEST_COUNT tests in target branch"
              fi
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT

      - name: Run xUnit tests
        id: run-tests
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Running xUnit tests with TRX logger..."
          set +e
          dotnet test --logger:"trx;LogFileName=test_results.trx" --results-directory ./test_results > target_test_output.txt 2>&1
          TEST_EXIT_CODE=$?
          set -e

          echo "Test exit code: $TEST_EXIT_CODE"
          echo "exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT

          # Save output for debugging
          cp target_test_output.txt test_results/ || true

      - name: Convert TRX to JSON and extract results
        id: extract-results
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Processing test results..."
          python3 - << 'PY'
          import json
          import xml.etree.ElementTree as ET
          import os
          import sys
          from pathlib import Path

          # Default values
          target_total = 0
          target_passed = 0
          target_percentage = 0
          failing_tests = []
          skipped_tests = []
          passing_tests = []
          all_tests = []
          warnings_list = []

          try:
              # Find TRX file
              trx_file = None
              if os.path.exists('test_results/test_results.trx'):
                  trx_file = 'test_results/test_results.trx'
              else:
                  # Search for any TRX file
                  for file in Path('test_results').glob('*.trx'):
                      trx_file = str(file)
                      break

              if trx_file and os.path.exists(trx_file):
                  print(f'Found TRX file: {trx_file}')
                  tree = ET.parse(trx_file)
                  root = tree.getroot()
                  
                  # Define namespaces
                  ns = {'tr': 'http://microsoft.com/schemas/VisualStudio/TeamTest/2010'}
                  
                  # Get test results
                  results = root.find('.//tr:Results', ns)
                  
                  if results is not None:
                      for result in results.findall('tr:UnitTestResult', ns):
                          test_name = result.get('testName', '')
                          outcome = result.get('outcome', '')
                          
                          # Get full test name including namespace
                          test_id = result.get('testId', '')
                          test_def = root.find(f'.//tr:UnitTest[@id="{test_id}"]', ns)
                          if test_def is not None:
                              test_method = test_def.find('.//tr:TestMethod', ns)
                              if test_method is not None:
                                  class_name = test_method.get('className', '')
                                  if class_name:
                                      test_name = f"{class_name}.{test_name}"
                          
                          if test_name:
                              all_tests.append(test_name)
                              
                              if outcome == 'Passed':
                                  passing_tests.append(test_name)
                                  target_passed += 1
                              elif outcome in ['Failed', 'Error']:
                                  failing_tests.append(test_name)
                              elif outcome in ['NotExecuted', 'Ignored', 'Skipped']:
                                  skipped_tests.append(test_name)
                              
                              target_total += 1
                  
                  # Get summary from counters element if available
                  counters = root.find('.//tr:Counters', ns)
                  if counters is not None:
                      target_total = int(counters.get('total', target_total))
                      target_passed = int(counters.get('passed', target_passed))
                      
              else:
                  print('No TRX file found, using defaults')

          except Exception as e:
              print(f'Error processing TRX: {e}')
              import traceback
              traceback.print_exc()

          # Calculate percentage
          if target_total > 0:
              target_percentage = (target_passed / target_total) * 100

          # Extract warnings from test output
          try:
              if os.path.exists('target_test_output.txt'):
                  with open('target_test_output.txt', 'r') as f:
                      output = f.read()
                      # Look for warning patterns in dotnet test output
                      if 'warning' in output.lower():
                          for line in output.split('\n'):
                              if 'warning' in line.lower():
                                  warnings_list.append(line.strip())
          except Exception as e:
              print(f'Could not extract warnings: {e}')

          # Save test data to JSON
          test_data = {
              'passing_tests': passing_tests,
              'failing_tests': failing_tests,
              'skipped_tests': skipped_tests,
              'xfailed_tests': [],  # xUnit doesn't have xfail concept like pytest
              'all_tests': all_tests,
              'skipped_tests_with_reasons': {},
              'xfailed_tests_with_reasons': {},
              'warnings': warnings_list
          }

          with open('target_test_data.json', 'w') as f:
              json.dump(test_data, f, indent=2)

          print('Test data saved to target_test_data.json')
          print(f'Results - Total: {target_total}, Passed: {target_passed}, Percentage: {target_percentage:.2f}%')

          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={target_total}\n')
              f.write(f'passed={target_passed}\n')
              f.write(f'percentage={target_percentage:.2f}\n')
          PY

      - name: Upload target branch artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: target_branch_data_${{ github.event.pull_request.number || github.run_id }}
          path: |
            target_test_data.json
            target_test_output.txt
            test_results/
          retention-days: 3
          if-no-files-found: ignore

      - name: Set collection error outputs
        id: set-error-outputs
        if: steps.check-collection.outputs.has_collection_errors == 'true'
        run: |
          echo "::warning::Setting default outputs for target branch due to collection errors"
          echo "total=0" >> $GITHUB_OUTPUT
          echo "passed=0" >> $GITHUB_OUTPUT
          echo "percentage=0.00" >> $GITHUB_OUTPUT

  compare-results:
    needs: [test-source-branch, test-target-branch]
    uses: ./.github/workflows/test-regression-analysis.yml
    with:
      runs_on: ${{ inputs.runs_on }}
      baseline_label: ${{ inputs.target_branch_to_compare }}
      baseline_results_artifact: target_branch_data_${{ github.event.pull_request.number || github.run_id }}
      baseline_results_filename: target_test_data.json
      current_label: ${{ github.head_ref || github.ref_name || 'source branch' }}
      current_results_artifact: pr_branch_data_${{ github.event.pull_request.number || github.run_id }}
      current_results_filename: pr_test_data.json
      baseline_passed: ${{ needs.test-target-branch.outputs.passed }}
      baseline_total: ${{ needs.test-target-branch.outputs.total }}
      baseline_percentage: ${{ needs.test-target-branch.outputs.percentage }}
      current_passed: ${{ needs.test-source-branch.outputs.passed }}
      current_total: ${{ needs.test-source-branch.outputs.total }}
      current_percentage: ${{ needs.test-source-branch.outputs.percentage }}
      baseline_collection_errors: ${{ needs.test-target-branch.outputs.collection_errors }}
      baseline_no_tests_found: ${{ needs.test-target-branch.outputs.no_tests_found }}
      current_collection_errors: ${{ needs.test-source-branch.outputs.collection_errors }}
      current_no_tests_found: ${{ needs.test-source-branch.outputs.no_tests_found }}
      artifact_name: regression_details_pr_${{ github.event.pull_request.number || github.run_id }}_xunit

  prepare-notification:
    name: Prepare Notification Data
    needs: [test-source-branch, test-target-branch, compare-results]
    if: |
      always() &&
      (
        needs.test-source-branch.outputs.collection_errors == 'true' ||
        needs.test-source-branch.outputs.no_tests_found == 'true' ||
        needs.compare-results.outputs.has_regressions == 'true'
      )
    runs-on: ${{ inputs.runs_on }}
    outputs:
      message_body: ${{ steps.construct_notification.outputs.message_body_out }}
      ping_user_ids: ${{ steps.construct_notification.outputs.ping_user_ids_out }}
      artifact_path: ${{ steps.construct_notification.outputs.artifact_path_out }}
      should_notify: "true"
      webhook_available_for_alert: ${{ steps.check_webhook_availability.outputs.webhook_available }}

    steps:
      - name: Check for Discord Webhook URL
        id: check_webhook_availability
        run: |
          if [ -z "${{ secrets.DISCORD_WEBHOOK_URL }}" ]; then
            echo "::notice::DISCORD_WEBHOOK_URL secret is not set. Discord notifications will be skipped."
            echo "webhook_available=false" >> $GITHUB_OUTPUT
          else
            echo "webhook_available=true" >> $GITHUB_OUTPUT
          fi

      - name: Download regression details (if any)
        id: download_regressions
        if: always()
        uses: actions/download-artifact@v4
        with:
          name: regression_details_pr_${{ github.event.pull_request.number || github.run_id }}_xunit
          path: .
        continue-on-error: true

      - name: Construct notification message
        id: construct_notification
        run: |
          # Initialize message components
          MESSAGE_LINES=()
          MESSAGE_LINES+=("**xUnit Test Results - PR #${{ github.event.pull_request.number }}**")
          MESSAGE_LINES+=("")

          # Add regression information if available
          if [[ "${{ needs.compare-results.outputs.has_regressions }}" == "true" ]]; then
            MESSAGE_LINES+=("**:red_circle: REGRESSIONS DETECTED**")
            MESSAGE_LINES+=("- ${{ needs.compare-results.outputs.regression_count }} test regressions found")
            
            # Add breakdown if available
            if [[ "${{ needs.compare-results.outputs.pass_to_fail_count }}" -gt 0 ]]; then
              MESSAGE_LINES+=("  - Pass→Fail: ${{ needs.compare-results.outputs.pass_to_fail_count }}")
            fi
            if [[ "${{ needs.compare-results.outputs.pass_to_skip_count }}" -gt 0 ]]; then
              MESSAGE_LINES+=("  - Pass→Skip: ${{ needs.compare-results.outputs.pass_to_skip_count }}")
            fi
            if [[ "${{ needs.compare-results.outputs.pass_to_gone_count }}" -gt 0 ]]; then
              MESSAGE_LINES+=("  - Pass→Gone: ${{ needs.compare-results.outputs.pass_to_gone_count }}")
            fi
          elif [[ "${{ needs.test-source-branch.outputs.collection_errors }}" == "true" ]]; then
            MESSAGE_LINES+=("**:red_circle: TEST DISCOVERY FAILED**")
            MESSAGE_LINES+=("- Error Type: ${{ needs.test-source-branch.outputs.error_type }}")
          elif [[ "${{ needs.test-source-branch.outputs.no_tests_found }}" == "true" ]]; then
            MESSAGE_LINES+=("**:warning: NO TESTS FOUND**")
          else
            MESSAGE_LINES+=("**:white_check_mark: NO REGRESSIONS DETECTED**")
          fi

          # Add test statistics
          MESSAGE_LINES+=("")
          MESSAGE_LINES+=("**Statistics:**")
          MESSAGE_LINES+=("- PR Branch: ${{ needs.test-source-branch.outputs.passed }}/${{ needs.test-source-branch.outputs.total }} passed (${{ needs.test-source-branch.outputs.percentage }}%)")
          MESSAGE_LINES+=("- Target Branch: ${{ needs.test-target-branch.outputs.passed }}/${{ needs.test-target-branch.outputs.total }} passed (${{ needs.test-target-branch.outputs.percentage }}%)")

          MESSAGE_LINES+=("---")
          MESSAGE_LINES+=("[View Workflow Run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})")

          # Set artifact path
          ARTIFACT_PATH_OUTPUT=""
          if [ -f "comprehensive_regression_report.txt" ]; then
            ARTIFACT_PATH_OUTPUT="comprehensive_regression_report.txt"
          elif [ -f "regression_details.txt" ]; then
            ARTIFACT_PATH_OUTPUT="regression_details.txt"
          fi

          # Construct final message
          FINAL_MESSAGE_BODY=$(printf "%s\\n" "${MESSAGE_LINES[@]}")
          FINAL_MESSAGE_BODY="${FINAL_MESSAGE_BODY%\\n}"

          echo "message_body_out<<EOF" >> $GITHUB_OUTPUT
          echo "$FINAL_MESSAGE_BODY" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          echo "artifact_path_out=$ARTIFACT_PATH_OUTPUT" >> $GITHUB_OUTPUT

          # Handle ping users (simplified)
          PING_USER_IDS=""
          if [[ "${{ inputs.ping_latest_committer }}" == "true" ]]; then
            # This would need actual implementation based on your needs
            PING_USER_IDS=""
          fi
          echo "ping_user_ids_out=$PING_USER_IDS" >> $GITHUB_OUTPUT

  notify-discord:
    name: Send Discord Notification
    needs: [prepare-notification]
    if: |
      always() &&
      needs.prepare-notification.outputs.should_notify == 'true' &&
      needs.prepare-notification.outputs.webhook_available_for_alert == 'true'
    uses: ./.github/workflows/discord-alert.yml
    with:
      message_body: ${{ needs.prepare-notification.outputs.message_body }}
      ping_user_ids: ${{ needs.prepare-notification.outputs.ping_user_ids }}
      artifact_paths: ${{ needs.prepare-notification.outputs.artifact_path }}
      should_notify: ${{ needs.prepare-notification.outputs.should_notify }}
      runs_on: ${{ inputs.runs_on }}
    secrets:
      DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      DISCORD_USER_MAP: ${{ secrets.DISCORD_USER_MAP }}
