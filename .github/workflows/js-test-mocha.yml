name: Reusable Compare Mocha Results

on:
  workflow_call:
    inputs:
      target_branch_to_compare:
        description: "The target branch to compare against (e.g., main, refs/heads/main)."
        required: true
        type: string
      node-version:
        description: "Node.js version to use for testing."
        required: false
        type: string
        default: "18"
      install-command:
        description: "Optional command to install dependencies (defaults to npm/pnpm/yarn auto-detection)."
        required: false
        type: string
        default: ""
      mocha-command:
        description: "Base command used to invoke Mocha (e.g., npx mocha, pnpm exec mocha)."
        required: false
        type: string
        default: "npx mocha"
      mocha-extra-args:
        description: "Additional arguments to pass to the Mocha command (applied before workflow-managed flags)."
        required: false
        type: string
        default: ""
      working-directory:
        description: "Directory where install and test commands should be executed."
        required: false
        type: string
        default: "."
      ping_latest_committer:
        description: "If true, the latest committer on the PR will be added to the ping list."
        required: false
        type: boolean
        default: false
      runs_on:
        required: false
        type: string
        default: "self-hosted"
    secrets:
      DISCORD_WEBHOOK_URL:
        description: "Discord Webhook URL for failure notifications. If not provided, notifications are skipped."
        required: false
      DISCORD_USER_MAP:
        description: 'JSON string mapping GitHub usernames to Discord User IDs (e.g., {"user1":"id1"}). If not provided, users won''t be pinged.'
        required: false
    outputs:
      pr_total:
        description: "Total tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.total }}
      pr_passed:
        description: "Passed tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.passed }}
      pr_percentage:
        description: "Pass percentage in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.percentage }}
      pr_collection_errors:
        description: "PR branch has collection errors"
        value: ${{ jobs.test-source-branch.outputs.collection_errors }}
      pr_no_tests_found:
        description: "PR branch has no tests found"
        value: ${{ jobs.test-source-branch.outputs.no_tests_found }}
      target_total:
        description: "Total tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.total }}
      target_passed:
        description: "Passed tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.passed }}
      target_percentage:
        description: "Pass percentage in target branch"
        value: ${{ jobs.test-target-branch.outputs.percentage }}
      has_regressions:
        description: "Boolean indicating if regressions were found"
        value: ${{ jobs.compare-results.outputs.has_regressions }}
      regression_count:
        description: "Number of test regressions found"
        value: ${{ jobs.compare-results.outputs.regression_count }}

jobs:
  test-source-branch:
    runs-on: ${{ inputs.runs_on }}
    defaults:
      run:
        shell: bash
        working-directory: ${{ inputs.working-directory }}
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}
      failing_count: ${{ steps.extract-results.outputs.failing_count }}
      skipped_count: ${{ steps.extract-results.outputs.skipped_count }}

    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "${{ inputs.node-version }}"

      - name: Detect package manager and install dependencies
        id: install-deps
        run: |
          if [ -n "${{ inputs.install-command }}" ]; then
            echo "Using custom install command: ${{ inputs.install-command }}"
            ${{ inputs.install-command }}
          elif [ -f "pnpm-lock.yaml" ]; then
            echo "Detected pnpm"
            npm install -g pnpm
            pnpm install
          elif [ -f "yarn.lock" ]; then
            echo "Detected yarn"
            npm install -g yarn
            yarn install
          else
            echo "Using npm (default)"
            npm ci || npm install
          fi

      - name: Check for test discovery issues
        id: check-collection
        run: |
          echo "Running Mocha --dry-run to verify discovery..."
          set +e
          ${{ inputs.mocha-command }} --dry-run > collection_output.txt 2>&1
          LIST_EXIT_CODE=$?
          set -e

          # Set default values
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          if [ "$LIST_EXIT_CODE" -ne 0 ] && grep -q "Error\|Cannot find module\|SyntaxError" collection_output.txt; then
            echo "::error::Test discovery command failed in PR branch"
            HAS_COLLECTION_ERRORS="true"
            ERROR_TYPE="DiscoveryCommandFailed"
            ERROR_DETAILS=$(head -n 200 collection_output.txt | tr '\n' ' ' | sed 's/"/\\"/g')
          else
            echo "Mocha --dry-run executed successfully"

            TEST_COUNT=$(grep -c "✓\|√\|- " collection_output.txt 2>/dev/null || echo "0")
            
            if [ "$TEST_COUNT" == "0" ] || grep -qi "0 passing" collection_output.txt; then
              echo "::warning::No tests were found in the PR branch"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              ERROR_DETAILS="No tests were discovered by Mocha --dry-run"
            else
              echo "Found $TEST_COUNT test entries in PR branch"
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT

      - name: Run Mocha tests
        id: run-tests
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Running Mocha tests with JSON reporter..."
          set +e
          ${{ inputs.mocha-command }} ${{ inputs.mocha-extra-args }} --reporter json > pr_results.json 2> pr_test_output.txt
          TEST_EXIT_CODE=$?
          set -e

          echo "Test exit code: $TEST_EXIT_CODE"
          echo "exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT

      - name: Extract results and prepare JSON
        id: extract-results
        if: always()
        run: |
          echo "Processing test results..."
          python3 - << 'PY'
          import json
          import os
          import sys

          # Default values
          pr_total = 0
          pr_passed = 0
          pr_percentage = 0
          failing_tests = []
          skipped_tests = []
          passing_tests = []
          all_tests = []
          warnings_list = []

          try:
              if os.path.exists('pr_results.json'):
                  print('Found pr_results.json')
                  with open('pr_results.json', 'r') as f:
                      results = json.load(f)
                  
                  # Mocha JSON structure
                  if 'stats' in results:
                      stats = results['stats']
                      pr_total = stats.get('tests', 0)
                      pr_passed = stats.get('passes', 0)
                      
                      # Process test results
                      if 'tests' in results:
                          for test in results['tests']:
                              full_title = test.get('fullTitle', test.get('title', ''))
                              if full_title:
                                  all_tests.append(full_title)
                                  if test.get('pass'):
                                      passing_tests.append(full_title)
                                  elif test.get('pending'):
                                      skipped_tests.append(full_title)
                                  else:
                                      failing_tests.append(full_title)
                      
                      # Also process failures array if present
                      if 'failures' in results:
                          for failure in results['failures']:
                              full_title = failure.get('fullTitle', failure.get('title', ''))
                              if full_title and full_title not in failing_tests:
                                  failing_tests.append(full_title)
                                  if full_title not in all_tests:
                                      all_tests.append(full_title)
                      
                      # Process pending/skipped tests
                      if 'pending' in results:
                          for pending in results['pending']:
                              full_title = pending.get('fullTitle', pending.get('title', ''))
                              if full_title and full_title not in skipped_tests:
                                  skipped_tests.append(full_title)
                                  if full_title not in all_tests:
                                      all_tests.append(full_title)
              else:
                  print('No pr_results.json file found')

          except Exception as e:
              print(f'Error processing Mocha results: {e}')
              import traceback
              traceback.print_exc()

          # Calculate percentage
          if pr_total > 0:
              pr_percentage = (pr_passed / pr_total) * 100

          # Extract warnings from test output
          try:
              if os.path.exists('pr_test_output.txt'):
                  with open('pr_test_output.txt', 'r') as f:
                      output = f.read()
                      # Look for common warning patterns
                      for line in output.split('\n'):
                          if any(pattern in line.lower() for pattern in ['warning:', 'deprecated:', 'console.warn']):
                              warnings_list.append(line.strip())
          except Exception as e:
              print(f'Could not extract warnings: {e}')

          # Save test data to JSON
          test_data = {
              'passing_tests': passing_tests,
              'failing_tests': failing_tests,
              'skipped_tests': skipped_tests,
              'xfailed_tests': [],  # Mocha doesn't have xfail concept
              'all_tests': all_tests,
              'skipped_tests_with_reasons': {},  # Would need additional parsing
              'xfailed_tests_with_reasons': {},
              'warnings': warnings_list
          }

          with open('pr_test_data.json', 'w') as f:
              json.dump(test_data, f, indent=2)

          print('Test data saved to pr_test_data.json')
          print(f'Results - Total: {pr_total}, Passed: {pr_passed}, Percentage: {pr_percentage:.2f}%')

          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={pr_total}\n')
              f.write(f'passed={pr_passed}\n')
              f.write(f'percentage={pr_percentage:.2f}\n')
              f.write(f'failing_count={len(failing_tests)}\n')
              f.write(f'skipped_count={len(skipped_tests)}\n')
          PY

      - name: Upload PR branch artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mocha_pr_branch_data_${{ github.event.pull_request.number || github.run_id }}
          path: |
            pr_test_data.json
            pr_results.json
            pr_test_output.txt
          retention-days: 3
          if-no-files-found: ignore

  test-target-branch:
    runs-on: ${{ inputs.runs_on }}
    defaults:
      run:
        shell: bash
        working-directory: ${{ inputs.working-directory }}
    outputs:
      total: ${{ steps.extract-results.outputs.total || steps.set-error-outputs.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed || steps.set-error-outputs.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage || steps.set-error-outputs.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}

    steps:
      - name: Checkout Target Branch
        uses: actions/checkout@v4.2.2
        with:
          ref: ${{ inputs.target_branch_to_compare }}
          submodules: "recursive"

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "${{ inputs.node-version }}"

      - name: Detect package manager and install dependencies
        id: install-deps
        run: |
          if [ -n "${{ inputs.install-command }}" ]; then
            echo "Using custom install command: ${{ inputs.install-command }}"
            ${{ inputs.install-command }}
          elif [ -f "pnpm-lock.yaml" ]; then
            echo "Detected pnpm"
            npm install -g pnpm
            pnpm install
          elif [ -f "yarn.lock" ]; then
            echo "Detected yarn"
            npm install -g yarn
            yarn install
          else
            echo "Using npm (default)"
            npm ci || npm install
          fi

      - name: Check for test discovery issues
        id: check-collection
        run: |
          echo "Running Mocha --dry-run to verify discovery..."
          set +e
          ${{ inputs.mocha-command }} --dry-run > collection_output.txt 2>&1
          LIST_EXIT_CODE=$?
          set -e

          # Set default values
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          if [ "$LIST_EXIT_CODE" -ne 0 ] && grep -q "Error\|Cannot find module\|SyntaxError" collection_output.txt; then
            echo "::warning::Test discovery command failed in target branch"
            HAS_COLLECTION_ERRORS="true"
            ERROR_TYPE="DiscoveryCommandFailed"
            ERROR_DETAILS=$(head -n 200 collection_output.txt | tr '\n' ' ' | sed 's/"/\\"/g')
          else
            echo "Mocha --dry-run executed successfully"

            TEST_COUNT=$(grep -c "✓\|√\|- " collection_output.txt 2>/dev/null || echo "0")
            
            if [ "$TEST_COUNT" == "0" ] || grep -qi "0 passing" collection_output.txt; then
              echo "::warning::No tests were found in the target branch"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              ERROR_DETAILS="No tests were discovered by Mocha --dry-run"
            else
              echo "Found $TEST_COUNT test entries in target branch"
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT

      - name: Run Mocha tests
        id: run-tests
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Running Mocha tests with JSON reporter..."
          set +e
          ${{ inputs.mocha-command }} ${{ inputs.mocha-extra-args }} --reporter json > target_results.json 2> target_test_output.txt
          TEST_EXIT_CODE=$?
          set -e

          echo "Test exit code: $TEST_EXIT_CODE"
          echo "exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT

      - name: Extract results and prepare JSON
        id: extract-results
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "Processing test results..."
          python3 - << 'PY'
          import json
          import os
          import sys

          # Default values
          target_total = 0
          target_passed = 0
          target_percentage = 0
          failing_tests = []
          skipped_tests = []
          passing_tests = []
          all_tests = []
          warnings_list = []

          try:
              if os.path.exists('target_results.json'):
                  print('Found target_results.json')
                  with open('target_results.json', 'r') as f:
                      results = json.load(f)
                  
                  # Mocha JSON structure
                  if 'stats' in results:
                      stats = results['stats']
                      target_total = stats.get('tests', 0)
                      target_passed = stats.get('passes', 0)
                      
                      # Process test results
                      if 'tests' in results:
                          for test in results['tests']:
                              full_title = test.get('fullTitle', test.get('title', ''))
                              if full_title:
                                  all_tests.append(full_title)
                                  if test.get('pass'):
                                      passing_tests.append(full_title)
                                  elif test.get('pending'):
                                      skipped_tests.append(full_title)
                                  else:
                                      failing_tests.append(full_title)
              else:
                  print('No target_results.json file found')

          except Exception as e:
              print(f'Error processing Mocha results: {e}')
              import traceback
              traceback.print_exc()

          # Calculate percentage
          if target_total > 0:
              target_percentage = (target_passed / target_total) * 100

          # Save test data to JSON
          test_data = {
              'passing_tests': passing_tests,
              'failing_tests': failing_tests,
              'skipped_tests': skipped_tests,
              'xfailed_tests': [],  # Mocha doesn't have xfail concept
              'all_tests': all_tests,
              'skipped_tests_with_reasons': {},
              'xfailed_tests_with_reasons': {},
              'warnings': warnings_list
          }

          with open('target_test_data.json', 'w') as f:
              json.dump(test_data, f, indent=2)

          print('Test data saved to target_test_data.json')
          print(f'Results - Total: {target_total}, Passed: {target_passed}, Percentage: {target_percentage:.2f}%')

          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={target_total}\n')
              f.write(f'passed={target_passed}\n')
              f.write(f'percentage={target_percentage:.2f}\n')
          PY

      - name: Upload target branch artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: mocha_target_branch_data_${{ github.event.pull_request.number || github.run_id }}
          path: |
            target_test_data.json
            target_results.json
            target_test_output.txt
          retention-days: 3
          if-no-files-found: ignore

      - name: Set collection error outputs
        id: set-error-outputs
        if: steps.check-collection.outputs.has_collection_errors == 'true'
        run: |
          echo "::warning::Setting default outputs for target branch due to collection errors"
          echo "total=0" >> $GITHUB_OUTPUT
          echo "passed=0" >> $GITHUB_OUTPUT
          echo "percentage=0.00" >> $GITHUB_OUTPUT

  compare-results:
    needs: [test-source-branch, test-target-branch]
    uses: ./.github/workflows/test-regression-analysis.yml
    with:
      runs_on: ${{ inputs.runs_on }}
      baseline_label: ${{ inputs.target_branch_to_compare }}
      baseline_results_artifact: mocha_target_branch_data_${{ github.event.pull_request.number || github.run_id }}
      baseline_results_filename: target_test_data.json
      current_label: ${{ github.head_ref || github.ref_name || 'source branch' }}
      current_results_artifact: mocha_pr_branch_data_${{ github.event.pull_request.number || github.run_id }}
      current_results_filename: pr_test_data.json
      baseline_passed: ${{ needs.test-target-branch.outputs.passed }}
      baseline_total: ${{ needs.test-target-branch.outputs.total }}
      baseline_percentage: ${{ needs.test-target-branch.outputs.percentage }}
      current_passed: ${{ needs.test-source-branch.outputs.passed }}
      current_total: ${{ needs.test-source-branch.outputs.total }}
      current_percentage: ${{ needs.test-source-branch.outputs.percentage }}
      baseline_collection_errors: ${{ needs.test-target-branch.outputs.collection_errors }}
      baseline_no_tests_found: ${{ needs.test-target-branch.outputs.no_tests_found }}
      current_collection_errors: ${{ needs.test-source-branch.outputs.collection_errors }}
      current_no_tests_found: ${{ needs.test-source-branch.outputs.no_tests_found }}
      artifact_name: regression_details_mocha_${{ github.event.pull_request.number || github.run_id }}
