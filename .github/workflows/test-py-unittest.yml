name: Reusable Unittest Runner

on:
  workflow_call:
    inputs:
      ref:
        description: "Git ref to checkout and test. Leave empty for default checkout."
        required: false
        type: string
        default: ""
      python-version:
        description: "Python version to use for testing."
        required: false
        type: string
        default: "3.10"
      runs_on:
        description: "Runner label for the test job."
        required: false
        type: string
        default: '["self-hosted", "multithreaded"]'
      artifact_name:
        description: "Name for the test results artifact."
        required: true
        type: string
      start-directory:
        description: "Directory passed to unittest discovery."
        required: false
        type: string
        default: "."
      test-pattern:
        description: "Pattern used by unittest discovery."
        required: false
        type: string
        default: "test*.py"
      top-level-directory:
        description: "Optional top level directory for unittest discovery (empty string disables)."
        required: false
        type: string
        default: ""
      working-directory:
        description: "Directory where unittest commands should be executed."
        required: false
        type: string
        default: "."
      parallel_workers:
        description: "Number of parallel workers. Leave empty for runner default (6 for multithreaded, 1 for singlethreaded). Use 'auto' for cgroup-aware CPU count, or a number. Note: unittest has limited parallel support."
        required: false
        type: string
        default: ""
    outputs:
      total:
        description: "Total number of tests"
        value: ${{ jobs.test.outputs.total }}
      passed:
        description: "Number of passing tests"
        value: ${{ jobs.test.outputs.passed }}
      percentage:
        description: "Pass percentage"
        value: ${{ jobs.test.outputs.percentage }}
      collection_errors:
        description: "Whether collection errors occurred"
        value: ${{ jobs.test.outputs.collection_errors }}
      no_tests_found:
        description: "Whether no tests were found"
        value: ${{ jobs.test.outputs.no_tests_found }}
      has_errors:
        description: "Whether any errors occurred"
        value: ${{ jobs.test.outputs.has_errors }}
      error_type:
        description: "Type of error if any"
        value: ${{ jobs.test.outputs.error_type }}
      failing_count:
        description: "Number of failing tests"
        value: ${{ jobs.test.outputs.failing_count }}
      error_count:
        description: "Number of errored tests"
        value: ${{ jobs.test.outputs.error_count }}
      skipped_count:
        description: "Number of skipped tests"
        value: ${{ jobs.test.outputs.skipped_count }}
      xfailed_count:
        description: "Number of xfailed tests"
        value: ${{ jobs.test.outputs.xfailed_count }}

jobs:
  test:
    runs-on: ${{ fromJSON(inputs.runs_on) }}
    defaults:
      run:
        shell: bash
        working-directory: ${{ inputs['working-directory'] }}
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      failing_count: ${{ steps.extract-results.outputs.failing_count }}
      error_count: ${{ steps.extract-results.outputs.error_count }}
      skipped_count: ${{ steps.extract-results.outputs.skipped_count }}
      xfailed_count: ${{ steps.extract-results.outputs.xfailed_count }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"
          ref: ${{ inputs.ref || github.ref }}

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          PYPROJECT=$(find . -name "pyproject.toml" -type f | head -n 1)
          if [ -n "$PYPROJECT" ]; then
            pip install -e "$(dirname "$PYPROJECT")[dev]" 2>/dev/null || pip install -e "$(dirname "$PYPROJECT")" 2>/dev/null || true
          fi

      - name: Prepare unittest JSON helper
        run: |
          cat <<'PY' > "$RUNNER_TEMP/unittest_to_json.py"
          import argparse
          import datetime as dt
          import inspect
          import json
          import os
          import sys
          import time
          import traceback
          import unittest
          from typing import Any, Dict, Iterable, List, Optional


          def iter_tests(suite: unittest.TestSuite) -> Iterable[unittest.TestCase]:
              for item in suite:
                  if isinstance(item, unittest.TestSuite):
                      yield from iter_tests(item)
                  else:
                      yield item


          def default_nodeid(test: unittest.TestCase) -> str:
              module = test.__class__.__module__
              class_name = test.__class__.__name__
              method = getattr(test, "_testMethodName", str(test))
              return f"{module}::{class_name}::{method}"


          def test_source_file(test: unittest.TestCase) -> Optional[str]:
              try:
                  file_path = inspect.getsourcefile(test.__class__)
              except TypeError:
                  file_path = None
              if file_path:
                  return os.path.relpath(file_path, os.getcwd())
              return None


          class RecordingResult(unittest.TestResult):
              def __init__(self) -> None:
                  super().__init__()
                  self.test_records: List[Dict[str, Any]] = []
                  self._start_times: Dict[unittest.TestCase, float] = {}
                  self.summary: Dict[str, int] = {
                      "passed": 0,
                      "failed": 0,
                      "errors": 0,
                      "skipped": 0,
                      "xfailed": 0,
                      "xpassed": 0,
                  }

              def startTest(self, test: unittest.TestCase) -> None:
                  self._start_times[test] = time.perf_counter()
                  super().startTest(test)

              def _finalize_record(
                  self,
                  test: unittest.TestCase,
                  outcome: str,
                  message: Optional[str] = None,
              ) -> None:
                  start = self._start_times.pop(test, None)
                  duration = time.perf_counter() - start if start is not None else 0.0
                  nodeid = default_nodeid(test)
                  record = {
                      "nodeid": nodeid,
                      "outcome": outcome,
                      "duration": duration,
                      "file": test_source_file(test),
                  }
                  if message:
                      record["longrepr"] = message
                  self.test_records.append(record)

              def addSuccess(self, test: unittest.TestCase) -> None:
                  super().addSuccess(test)
                  self.summary["passed"] += 1
                  self._finalize_record(test, "passed")

              def addFailure(self, test: unittest.TestCase, err: BaseException) -> None:
                  super().addFailure(test, err)
                  self.summary["failed"] += 1
                  message = self._exc_info_to_string(err, test)
                  self._finalize_record(test, "failed", message)

              def addError(self, test: unittest.TestCase, err: BaseException) -> None:
                  super().addError(test, err)
                  self.summary["errors"] += 1
                  message = self._exc_info_to_string(err, test)
                  self._finalize_record(test, "error", message)

              def addSkip(self, test: unittest.TestCase, reason: str) -> None:
                  super().addSkip(test, reason)
                  self.summary["skipped"] += 1
                  self._finalize_record(test, "skipped", reason)

              def addExpectedFailure(self, test: unittest.TestCase, err: BaseException) -> None:
                  super().addExpectedFailure(test, err)
                  self.summary["xfailed"] += 1
                  message = self._exc_info_to_string(err, test)
                  self._finalize_record(test, "xfailed", message)

              def addUnexpectedSuccess(self, test: unittest.TestCase) -> None:
                  super().addUnexpectedSuccess(test)
                  self.summary["xpassed"] += 1
                  self._finalize_record(test, "failed", "Unexpected success")


          def iter_suite(start_dir: str, pattern: str, top_level_dir: Optional[str]) -> Iterable[unittest.TestCase]:
              loader = unittest.TestLoader()
              suite = loader.discover(start_dir=start_dir, pattern=pattern, top_level_dir=top_level_dir)
              return iter_tests(suite)


          def run_tests(
              start_dir: str,
              pattern: str,
              top_level_dir: Optional[str],
              *,
              dry_run: bool,
              output: str,
          ) -> int:
              top_level_dir = top_level_dir or None
              try:
                  tests = list(iter_suite(start_dir, pattern, top_level_dir))
              except Exception:
                  error_payload = {
                      "created": dt.datetime.utcnow().isoformat() + "Z",
                      "exitcode": 2,
                      "errors": [traceback.format_exc()],
                      "summary": {"total": 0, "passed": 0},
                      "tests": [],
                  }
                  with open(output, "w", encoding="utf-8") as fh:
                      json.dump(error_payload, fh, indent=2)
                  print("ERROR: failed to discover tests", file=sys.stderr)
                  return 2

              if dry_run:
                  payload = {
                      "created": dt.datetime.utcnow().isoformat() + "Z",
                      "collected": len(tests),
                      "tests": [
                          {
                              "nodeid": default_nodeid(test),
                              "file": test_source_file(test),
                              "doc": test.shortDescription(),
                          }
                          for test in tests
                      ],
                  }
                  with open(output, "w", encoding="utf-8") as fh:
                      json.dump(payload, fh, indent=2)
                  print(f"Discovered {len(tests)} unittest cases")
                  return 0

              result = RecordingResult()
              runner = unittest.TextTestRunner(verbosity=2, resultclass=lambda *_, **__: result)
              loader = unittest.TestLoader()
              suite = loader.discover(start_dir=start_dir, pattern=pattern, top_level_dir=top_level_dir)
              runner.run(suite)

              total = len(result.test_records)
              passed = result.summary["passed"]
              summary = {
                  "total": total,
                  "passed": passed,
                  "failed": result.summary["failed"],
                  "errors": result.summary["errors"],
                  "skipped": result.summary["skipped"],
                  "xfailed": result.summary["xfailed"],
                  "xpassed": result.summary["xpassed"],
              }

              payload = {
                  "created": dt.datetime.utcnow().isoformat() + "Z",
                  "exitcode": 0,
                  "summary": summary,
                  "tests": result.test_records,
              }

              with open(output, "w", encoding="utf-8") as fh:
                  json.dump(payload, fh, indent=2)

              print(
                  "Test run complete: total={total} passed={passed} failed={failed} errors={errors} skipped={skipped}".format(
                      total=summary["total"],
                      passed=summary["passed"],
                      failed=summary["failed"],
                      errors=summary["errors"],
                      skipped=summary["skipped"],
                  )
              )
              return 0 if result.wasSuccessful() else 1


          def parse_args(argv: Optional[List[str]] = None) -> argparse.Namespace:
              parser = argparse.ArgumentParser(
                  description="Run unittest discovery with JSON output"
              )
              parser.add_argument("--start-directory", default=".")
              parser.add_argument("--pattern", default="test*.py")
              parser.add_argument("--top-level-directory", default=None)
              parser.add_argument("--output", required=True)
              parser.add_argument("--collect-only", action="store_true")
              return parser.parse_args(argv)


          def main(argv: Optional[List[str]] = None) -> int:
              args = parse_args(argv)
              top_level = args.top_level_directory or None
              return run_tests(
                  start_dir=args.start_directory,
                  pattern=args.pattern,
                  top_level_dir=top_level,
                  dry_run=args.collect_only,
                  output=args.output,
              )


          if __name__ == "__main__":
              sys.exit(main())
          PY
          echo "UNITTEST_JSON_HELPER=$RUNNER_TEMP/unittest_to_json.py" >> $GITHUB_ENV

      - name: Check for test collection errors
        id: check-collection
        run: |
          echo "Running unittest collection check..."

          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"

          set +e
          python "$UNITTEST_JSON_HELPER" \
            --collect-only \
            --start-directory "${{ inputs['start-directory'] }}" \
            --pattern "${{ inputs['test-pattern'] }}" \
            --top-level-directory "${{ inputs['top-level-directory'] }}" \
            --output unittest_collection.json > collection_output.txt 2>&1
          EXIT_CODE=$?
          set -e

          if [ "$EXIT_CODE" -ne 0 ]; then
            HAS_COLLECTION_ERRORS="true"
            ERROR_TYPE="CollectionError"
            echo "::error::Unittest discovery failed"
          else
            if [ -f unittest_collection.json ]; then
              TEST_COUNT=$(python3 -c "
          import json
          with open('unittest_collection.json', 'r', encoding='utf-8') as handle:
              payload = json.load(handle)
          print(len(payload.get('tests', [])))
          ")
            else
              TEST_COUNT=0
            fi

            if [ -z "$TEST_COUNT" ] || [ "$TEST_COUNT" = "0" ]; then
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              echo "::warning::No unittest tests were found"
            else
              echo "Found $TEST_COUNT unittest test(s)"
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> "$GITHUB_OUTPUT"
          echo "no_tests_found=$NO_TESTS_FOUND" >> "$GITHUB_OUTPUT"
          echo "error_type=$ERROR_TYPE" >> "$GITHUB_OUTPUT"
          if [[ "$HAS_COLLECTION_ERRORS" == "true" || "$NO_TESTS_FOUND" == "true" ]]; then
            echo "has_errors=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_errors=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Run tests
        id: run-tests
        continue-on-error: true
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          set -euo pipefail

          echo "Running unittest suite..."

          set +e
          python "$UNITTEST_JSON_HELPER" \
            --start-directory "${{ inputs['start-directory'] }}" \
            --pattern "${{ inputs['test-pattern'] }}" \
            --top-level-directory "${{ inputs['top-level-directory'] }}" \
            --output results.json 2>&1 | tee test_output.txt
          EXIT_CODE=$?
          set -e

          echo "unittest_exit_code=$EXIT_CODE" >> "$GITHUB_OUTPUT"

          if [ -s results.json ]; then
            echo "✅ Test execution completed (exit code: $EXIT_CODE)"
          else
            echo "❌ Test execution failed (exit code: $EXIT_CODE)"
            echo '{"exitcode": 1, "summary": {"total": 0, "passed": 0}, "tests": []}' > results.json
          fi

      - name: Extract test results
        id: extract-results
        run: |
          python3 -c "
          import json
          import os

          total = passed = 0
          percentage = 0.0
          passing_tests = []
          failing_tests = []
          error_tests = []
          skipped_tests = []
          xfailed_tests = []
          xpassed_tests = []
          all_tests = []
          skipped_with_reasons = {}
          xfailed_with_reasons = {}
          warnings_list = []

          try:
              with open('results.json') as f:
                  results = json.load(f)

              if 'summary' in results:
                  summary = results['summary']
                  total = summary.get('total', 0)
                  passed = summary.get('passed', 0)

              for test in results.get('tests', []):
                  outcome = test.get('outcome')
                  nodeid = test.get('nodeid', '')
                  if not nodeid:
                      continue
                  all_tests.append(nodeid)
                  if outcome == 'passed':
                      passing_tests.append(nodeid)
                  elif outcome == 'failed':
                      failing_tests.append(nodeid)
                  elif outcome == 'error':
                      error_tests.append(nodeid)
                  elif outcome == 'skipped':
                      skipped_tests.append(nodeid)
                      reason = test.get('longrepr', 'No reason')
                      if isinstance(reason, list):
                          reason = reason[0] if reason else 'No reason'
                      skipped_with_reasons[nodeid] = str(reason).strip()
                  elif outcome == 'xfailed':
                      xfailed_tests.append(nodeid)
                      reason = test.get('longrepr', 'No reason')
                      if isinstance(reason, list):
                          reason = reason[0] if reason else 'No reason'
                      xfailed_with_reasons[nodeid] = str(reason).strip()
                  elif outcome == 'xpassed':
                      xpassed_tests.append(nodeid)

              percentage = (passed / total * 100) if total > 0 else 0
          except FileNotFoundError:
              print('Results file not found')
          except Exception as e:
              print(f'Error: {e}')

          # Save artifact data
          with open('test_data.json', 'w') as f:
              json.dump({
                  'passing_tests': passing_tests,
                  'failing_tests': failing_tests,
                  'error_tests': error_tests,
                  'skipped_tests': skipped_tests,
                  'xfailed_tests': xfailed_tests,
                  'xpassed_tests': xpassed_tests,
                  'all_tests': all_tests,
                  'skipped_tests_with_reasons': skipped_with_reasons,
                  'xfailed_tests_with_reasons': xfailed_with_reasons,
                  'warnings': warnings_list
              }, f, indent=2)

          print(f'Results: {passed}/{total} ({percentage:.1f}%)')

          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={total}\n')
              f.write(f'passed={passed}\n')
              f.write(f'percentage={percentage:.2f}\n')
              f.write(f'failing_count={len(failing_tests)}\n')
              f.write(f'error_count={len(error_tests)}\n')
              f.write(f'skipped_count={len(skipped_tests)}\n')
              f.write(f'xfailed_count={len(xfailed_tests)}\n')
          "

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact_name }}
          path: |
            ${{ inputs['working-directory'] }}/test_data.json
            ${{ inputs['working-directory'] }}/test_output.txt
            ${{ inputs['working-directory'] }}/results.json
          retention-days: 3
          if-no-files-found: ignore
