name: Cache Main Branch Tests

# Runs on push to main/master to pre-populate pytest cache for PRs
# Uses same concurrency group as PR test-target job, so PRs targeting
# this SHA will wait for tests to complete, then use cached results
#
# PRs targeting other branches (not main/master) won't find a cache
# and will run their own target branch tests - this is expected behavior

on:
  push:
    branches:
      - main
      - master
  workflow_dispatch:
    inputs:
      branch:
        description: "Branch to cache tests for"
        required: false
        default: "main"

jobs:
  cache-pytest:
    runs-on: self-hosted
    # Same concurrency group as test-target in run-branch-test.yml
    # PRs targeting this SHA will wait here, then find our cache
    concurrency:
      group: pytest-target-${{ github.ref_name }}-${{ github.sha }}
      cancel-in-progress: false

    steps:
      - name: Check existing cache
        id: cache-check
        uses: actions/cache/restore@v4
        with:
          path: cached_target
          key: pytest-${{ github.ref_name }}-${{ github.sha }}
          lookup-only: true

      - name: Skip if cached
        if: steps.cache-check.outputs.cache-hit == 'true'
        run: |
          echo "âœ… Cache already exists for ${{ github.ref_name }}@${{ github.sha }}"
          echo "Skipping test run"

      - name: Checkout
        if: steps.cache-check.outputs.cache-hit != 'true'
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"

      - name: Set up Python
        if: steps.cache-check.outputs.cache-hit != 'true'
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "3.10"

      - name: Install dependencies
        if: steps.cache-check.outputs.cache-hit != 'true'
        run: |
          echo "ðŸ“¦ Installing dependencies..."
          python -m pip install --upgrade pip
          pip install pytest pytest-json-report pytest-asyncio pytest-xdist
          PYPROJECT=$(find . -name "pyproject.toml" -type f | head -n 1)
          if [ -n "$PYPROJECT" ]; then
            pip install -e "$(dirname "$PYPROJECT")[dev]" || pip install -e "$(dirname "$PYPROJECT")"
          elif [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi

      - name: Check for test collection errors
        id: check-collection
        if: steps.cache-check.outputs.cache-hit != 'true'
        run: |
          echo "ðŸ” Running pytest collection check..."
          python -m pytest --collect-only -q > collection_output.txt 2>&1 || true

          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"

          if grep -q "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt; then
            echo "::error::Test discovery errors detected"
            HAS_COLLECTION_ERRORS="true"
            ERROR_TYPE="CollectionError"
          else
            TEST_COUNT=$(grep -o "collected [0-9]* item" collection_output.txt | grep -o "[0-9]*" || echo "0")
            if [[ "$TEST_COUNT" == "0" ]]; then
              echo "::warning::No tests were found"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
            else
              echo "âœ… Found $TEST_COUNT tests"
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT

      - name: Run tests
        id: run-tests
        continue-on-error: true
        if: |
          steps.cache-check.outputs.cache-hit != 'true' &&
          steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          echo "ðŸ§ª Running tests for ${{ github.ref_name }}@${{ github.sha }}..."

          set +e
          python -m pytest -q -n auto --json-report --json-report-file=results.json --tb=line 2>&1 | tee test_output.txt
          PYTEST_EXIT=$?
          set -e

          echo "pytest_exit_code=$PYTEST_EXIT" >> $GITHUB_OUTPUT

          if [ $PYTEST_EXIT -eq 137 ]; then
            echo "::warning::Tests were killed (exit 137) - likely OOM. Partial results may be available."
          fi

          if [ -f results.json ]; then
            echo "âœ… Test execution completed (exit code: $PYTEST_EXIT)"
          else
            echo "âŒ No results.json - creating empty results file"
            echo '{"exitcode": '$PYTEST_EXIT', "summary": {"total": 0, "passed": 0}, "tests": []}' > results.json
          fi

      - name: Extract test results
        id: extract-results
        if: steps.cache-check.outputs.cache-hit != 'true'
        run: |
          python3 -c "
          import json
          import os

          total = passed = 0
          passing_tests = []
          failing_tests = []
          skipped_tests = []
          xfailed_tests = []
          all_tests = []
          skipped_with_reasons = {}
          xfailed_with_reasons = {}

          try:
              with open('results.json') as f:
                  results = json.load(f)

              if results.get('exitcode', 0) <= 1 and 'summary' in results:
                  summary = results['summary']
                  total = summary.get('total', 0)
                  passed = summary.get('passed', 0)

                  for test in results.get('tests', []):
                      outcome = test.get('outcome')
                      nodeid = test.get('nodeid', '')
                      if not nodeid:
                          continue
                      all_tests.append(nodeid)
                      if outcome == 'passed':
                          passing_tests.append(nodeid)
                      elif outcome in ['failed', 'error']:
                          failing_tests.append(nodeid)
                      elif outcome == 'skipped':
                          skipped_tests.append(nodeid)
                          reason = test.get('longrepr', 'No reason')
                          if isinstance(reason, list):
                              reason = reason[0] if reason else 'No reason'
                          skipped_with_reasons[nodeid] = str(reason).strip()
                      elif outcome == 'xfailed':
                          xfailed_tests.append(nodeid)
                          reason = test.get('longrepr', 'No reason')
                          if isinstance(reason, list):
                              reason = reason[0] if reason else 'No reason'
                          xfailed_with_reasons[nodeid] = str(reason).strip()

          except Exception as e:
              print(f'Error: {e}')

          percentage = (passed / total * 100) if total > 0 else 0

          with open('test_data.json', 'w') as f:
              json.dump({
                  'passing_tests': passing_tests,
                  'failing_tests': failing_tests,
                  'skipped_tests': skipped_tests,
                  'xfailed_tests': xfailed_tests,
                  'all_tests': all_tests,
                  'skipped_tests_with_reasons': skipped_with_reasons,
                  'xfailed_tests_with_reasons': xfailed_with_reasons,
              }, f, indent=2)

          print(f'Results: {passed}/{total} ({percentage:.1f}%)')

          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={total}\n')
              f.write(f'passed={passed}\n')
              f.write(f'percentage={percentage:.2f}\n')
              f.write(f'failing_count={len(failing_tests)}\n')
              f.write(f'skipped_count={len(skipped_tests)}\n')
              f.write(f'xfailed_count={len(xfailed_tests)}\n')
          "

      - name: Save results to cache
        if: steps.cache-check.outputs.cache-hit != 'true'
        run: |
          echo "ðŸ’¾ Saving results to cache for PRs to use..."
          mkdir -p cached_target

          [ -f test_data.json ] && cp test_data.json cached_target/
          [ -f test_output.txt ] && cp test_output.txt cached_target/
          [ -f results.json ] && cp results.json cached_target/

          # Save outputs for PR workflows to load
          cat > cached_target/outputs.env << ENVEOF
          total=${{ steps.extract-results.outputs.total || '0' }}
          passed=${{ steps.extract-results.outputs.passed || '0' }}
          percentage=${{ steps.extract-results.outputs.percentage || '0.00' }}
          collection_errors=${{ steps.check-collection.outputs.has_collection_errors || 'false' }}
          no_tests_found=${{ steps.check-collection.outputs.no_tests_found || 'false' }}
          has_errors=false
          error_type=${{ steps.check-collection.outputs.error_type || 'none' }}
          failing_count=${{ steps.extract-results.outputs.failing_count || '0' }}
          skipped_count=${{ steps.extract-results.outputs.skipped_count || '0' }}
          xfailed_count=${{ steps.extract-results.outputs.xfailed_count || '0' }}
          ENVEOF

      # Explicitly save cache (not relying on post-hook) so it's available
      # before concurrency group releases waiting jobs
      - name: Save cache
        if: steps.cache-check.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: cached_target
          key: pytest-${{ github.ref_name }}-${{ github.sha }}

      - name: Confirm cache saved
        if: steps.cache-check.outputs.cache-hit != 'true'
        run: |
          echo "âœ… Cache saved with key: pytest-${{ github.ref_name }}-${{ github.sha }}"
          echo "PRs targeting this SHA will find and use these results"
