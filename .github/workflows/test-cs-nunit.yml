name: Reusable NUnit Runner

on:
  workflow_call:
    inputs:
      ref:
        description: "Git ref to checkout and test. Leave empty for default checkout."
        required: false
        type: string
        default: ""
      dotnet-version:
        description: ".NET SDK version to use for testing."
        required: false
        type: string
        default: "8.0.x"
      runs_on:
        description: "Runner label for the test job."
        required: false
        type: string
        default: '["self-hosted", "multithreaded"]'
      artifact_name:
        description: "Name for the test results artifact."
        required: true
        type: string
      working_directory:
        description: "Directory where dotnet test should be executed."
        required: false
        type: string
        default: "."
      parallel_workers:
        description: "Number of parallel test workers. Leave empty for runner default (6 for multithreaded, 1 for singlethreaded). Use 'auto' for cgroup-aware CPU count, or a number."
        required: false
        type: string
        default: ""
    outputs:
      total:
        description: "Total number of tests"
        value: ${{ jobs.test.outputs.total }}
      passed:
        description: "Number of passing tests"
        value: ${{ jobs.test.outputs.passed }}
      percentage:
        description: "Pass percentage"
        value: ${{ jobs.test.outputs.percentage }}
      collection_errors:
        description: "Whether collection errors occurred"
        value: ${{ jobs.test.outputs.collection_errors }}
      no_tests_found:
        description: "Whether no tests were found"
        value: ${{ jobs.test.outputs.no_tests_found }}
      has_errors:
        description: "Whether any errors occurred"
        value: ${{ jobs.test.outputs.has_errors }}
      error_type:
        description: "Type of error if any"
        value: ${{ jobs.test.outputs.error_type }}
      failing_count:
        description: "Number of failing tests"
        value: ${{ jobs.test.outputs.failing_count }}
      error_count:
        description: "Number of errored tests"
        value: ${{ jobs.test.outputs.error_count }}
      skipped_count:
        description: "Number of skipped tests"
        value: ${{ jobs.test.outputs.skipped_count }}
      xfailed_count:
        description: "Number of xfailed tests"
        value: ${{ jobs.test.outputs.xfailed_count }}

jobs:
  test:
    runs-on: ${{ fromJSON(inputs.runs_on) }}
    defaults:
      run:
        shell: bash
        working-directory: ${{ inputs.working_directory }}
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      failing_count: ${{ steps.extract-results.outputs.failing_count }}
      error_count: ${{ steps.extract-results.outputs.error_count }}
      skipped_count: ${{ steps.extract-results.outputs.skipped_count }}
      xfailed_count: ${{ steps.extract-results.outputs.xfailed_count }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"
          ref: ${{ inputs.ref || github.ref }}

      - name: Set up .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: "${{ inputs.dotnet-version }}"

      - name: Restore dependencies
        run: |
          echo "Restoring dotnet dependencies..."
          dotnet restore

      - name: Check for test discovery issues
        id: check-collection
        run: |
          echo "Running dotnet test --list-tests to verify discovery..."
          set +e
          dotnet test --list-tests > collection_output.txt 2>&1
          LIST_EXIT_CODE=$?
          set -e

          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"

          if [ "$LIST_EXIT_CODE" -ne 0 ]; then
            echo "::error::Test discovery command failed"
            HAS_COLLECTION_ERRORS="true"
            ERROR_TYPE="DiscoveryCommandFailed"
          else
            echo "dotnet test --list-tests executed successfully"

            if grep -qi "Test Run Aborted" collection_output.txt || grep -qi "Test host process exited unexpectedly" collection_output.txt; then
              echo "::error::Discovery encountered runtime errors"
              HAS_COLLECTION_ERRORS="true"
              ERROR_TYPE="RuntimeDiscoveryError"
            else
              TEST_COUNT=$(grep -Eo "Total tests: [0-9]+" collection_output.txt | tail -1 | awk '{print $3}')

              if [[ -z "$TEST_COUNT" ]]; then
                TEST_COUNT=$(grep -c "^    " collection_output.txt || echo "0")
              fi

              if grep -qi "No test is available" collection_output.txt || [[ -z "$TEST_COUNT" || "$TEST_COUNT" == "0" ]]; then
                echo "::warning::No tests were found"
                NO_TESTS_FOUND="true"
                ERROR_TYPE="NoTestsFound"
              else
                echo "Found $TEST_COUNT tests"
              fi
            fi
          fi

          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> "$GITHUB_OUTPUT"
          echo "no_tests_found=$NO_TESTS_FOUND" >> "$GITHUB_OUTPUT"
          echo "error_type=$ERROR_TYPE" >> "$GITHUB_OUTPUT"
          if [[ "$HAS_COLLECTION_ERRORS" == "true" || "$NO_TESTS_FOUND" == "true" ]]; then
            echo "has_errors=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_errors=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Run tests
        id: run-tests
        continue-on-error: true
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          set -euo pipefail

          # Source shared worker calculation script
          source "$GITHUB_WORKSPACE/.github/scripts/cgroup_workers.sh"
          WORKERS=$(determine_workers "${{ inputs.parallel_workers }}" '${{ inputs.runs_on }}')

          echo "Running NUnit tests with $WORKERS parallel workers..."

          PARALLEL_FLAG=""
          if [ "$WORKERS" != "1" ]; then
            PARALLEL_FLAG="-- NUnit.NumberOfTestWorkers=$WORKERS"
          fi

          mkdir -p TestResults

          set +e
          dotnet test --logger "trx;LogFileName=results.trx" --results-directory TestResults --verbosity normal $PARALLEL_FLAG 2>&1 | tee test_output.txt
          DOTNET_EXIT=$?
          set -e

          echo "dotnet_exit_code=$DOTNET_EXIT" >> "$GITHUB_OUTPUT"

          if [ "$DOTNET_EXIT" -eq 137 ]; then
            echo "::warning::Tests were killed (exit 137) - likely OOM. Partial results may be available."
          fi

          if [ -f TestResults/results.trx ]; then
            echo "✅ Test execution completed (exit code: $DOTNET_EXIT)"
          else
            echo "❌ No results.trx - test execution may have failed"
          fi

      - name: Convert TRX results to JSON
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          python3 "$GITHUB_WORKSPACE/.github/scripts/trx_to_pytest_json.py" TestResults/results.trx results.json

      - name: Extract test results
        id: extract-results
        run: |
          python3 -c "
          import json
          import os

          total = passed = 0
          percentage = 0.0
          passing_tests = []
          failing_tests = []
          error_tests = []
          skipped_tests = []
          xfailed_tests = []
          xpassed_tests = []
          all_tests = []
          skipped_with_reasons = {}
          xfailed_with_reasons = {}
          warnings_list = []

          try:
              with open('results.json') as f:
                  results = json.load(f)

              if 'summary' in results:
                  summary = results['summary']
                  total = summary.get('total', 0)
                  passed = summary.get('passed', 0)

              for test in results.get('tests', []):
                  outcome = test.get('outcome')
                  nodeid = test.get('nodeid', '')
                  if not nodeid:
                      continue
                  all_tests.append(nodeid)
                  if outcome == 'passed':
                      passing_tests.append(nodeid)
                  elif outcome == 'failed':
                      failing_tests.append(nodeid)
                  elif outcome == 'error':
                      error_tests.append(nodeid)
                  elif outcome == 'skipped':
                      skipped_tests.append(nodeid)
                      reason = test.get('longrepr', 'No reason')
                      if isinstance(reason, list):
                          reason = reason[0] if reason else 'No reason'
                      skipped_with_reasons[nodeid] = str(reason).strip()
                  elif outcome == 'xfailed':
                      xfailed_tests.append(nodeid)
                      reason = test.get('longrepr', 'No reason')
                      if isinstance(reason, list):
                          reason = reason[0] if reason else 'No reason'
                      xfailed_with_reasons[nodeid] = str(reason).strip()
                  elif outcome == 'xpassed':
                      xpassed_tests.append(nodeid)

              percentage = (passed / total * 100) if total > 0 else 0
          except FileNotFoundError:
              print('Results file not found')
          except Exception as e:
              print(f'Error: {e}')

          # Save artifact data
          with open('test_data.json', 'w') as f:
              json.dump({
                  'passing_tests': passing_tests,
                  'failing_tests': failing_tests,
                  'error_tests': error_tests,
                  'skipped_tests': skipped_tests,
                  'xfailed_tests': xfailed_tests,
                  'xpassed_tests': xpassed_tests,
                  'all_tests': all_tests,
                  'skipped_tests_with_reasons': skipped_with_reasons,
                  'xfailed_tests_with_reasons': xfailed_with_reasons,
                  'warnings': warnings_list
              }, f, indent=2)

          print(f'Results: {passed}/{total} ({percentage:.1f}%)')

          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={total}\n')
              f.write(f'passed={passed}\n')
              f.write(f'percentage={percentage:.2f}\n')
              f.write(f'failing_count={len(failing_tests)}\n')
              f.write(f'error_count={len(error_tests)}\n')
              f.write(f'skipped_count={len(skipped_tests)}\n')
              f.write(f'xfailed_count={len(xfailed_tests)}\n')
          "

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ inputs.artifact_name }}
          path: |
            ${{ inputs.working_directory }}/test_data.json
            ${{ inputs.working_directory }}/test_output.txt
            ${{ inputs.working_directory }}/results.json
          retention-days: 3
          if-no-files-found: ignore
